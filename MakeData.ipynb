{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_dir='/home/dc/cs230_project/dataset')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--data_dir\", type=str, default='/home/dc/cs230_project/dataset', help=\"store duplicate questions\")\n",
    "params, _ = parser.parse_known_args()\n",
    "\n",
    "print(params)\n",
    "\n",
    "class MakeData():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        nothing here\n",
    "        '''\n",
    "        self.W2V_PATH = \"/home/dc/cs230_project/dataset/GloVe/glove.840B.300d.txt\"\n",
    "    \n",
    "    def save(self,X_train,X_valid,X_test,y_train,y_valid,y_test,\n",
    "             X_train_fn=\"X_train\",X_valid_fn=\"X_valid\",X_test_fn=\"X_test\", \n",
    "             y_train_fn=\"y_train\",y_valid_fn=\"y_valid\",y_test_fn=\"y_test\",small=False,big=False,clean=False):\n",
    "    \n",
    "        checkList=[]\n",
    "        checkList.append(big), checkList.append(small), checkList.append(clean)\n",
    "        assert(checkList.count(True)==1)\n",
    "    \n",
    "        self.save_single_file(X_train_fn,X_train,big=big,small=small,clean=clean)\n",
    "        self.save_single_file(X_valid_fn,X_valid,big=big,small=small,clean=clean)\n",
    "        self.save_single_file(X_test_fn,X_test,big=big,small=small,clean=clean)\n",
    "        self.save_single_file(y_train_fn,y_train,big=big,small=small,clean=clean)\n",
    "        self.save_single_file(y_valid_fn,y_valid,big=big,small=small,clean=clean)\n",
    "        self.save_single_file(y_test_fn,y_test,big=big,small=small,clean=clean)\n",
    "     \n",
    "\n",
    "    def save_single_file(self,filename,data,big=False,small=False,clean=False):\n",
    "        '''\n",
    "        input: filename to be saved, 3 boolean values \n",
    "        if big: save data to filename.pkl\n",
    "        if small: save data to filename_small.pkl\n",
    "        if clean: save data to filename_clean.pkl\n",
    "        '''\n",
    "        #check arguments only 1 can be true\n",
    "        checkList=[]\n",
    "        checkList.append(big), checkList.append(small), checkList.append(clean)\n",
    "        assert(checkList.count(True)==1)\n",
    "    \n",
    "        if (big==True):\n",
    "            fh = open(os.path.join(params.data_dir,filename+'.pkl'), 'wb+')\n",
    "            pickle.dump(data, fh)\n",
    "            fh.close()\n",
    "        elif(small==True):\n",
    "            fh = open(os.path.join(params.data_dir,filename+'_small'+'.pkl'), 'wb+')\n",
    "            pickle.dump(data, fh)\n",
    "            fh.close()\n",
    "        elif(clean==True):\n",
    "            fh = open(os.path.join(params.data_dir,filename+'_clean'+'.pkl'), 'wb+')\n",
    "            pickle.dump(data, fh)\n",
    "            fh.close()\n",
    "\n",
    "\n",
    "    def load_single_file(self,filename):\n",
    "        fh = open(os.path.join(params.data_dir,filename+'.pkl'),'rb')\n",
    "        data = pickle.load(fh)\n",
    "        fh.close()\n",
    "        return data\n",
    "\n",
    "\n",
    "    def load_data(self,small=False,big=False, clean=False):\n",
    "        '''\n",
    "        load data from pkl files\n",
    "        '''\n",
    "        checkList=[]\n",
    "        checkList.append(big), checkList.append(small), checkList.append(clean)\n",
    "        assert(checkList.count(True)==1)\n",
    "        \n",
    "        if small==True:\n",
    "            print(\"loading small\")\n",
    "            X_train = self.load_single_file(\"X_train_small\")\n",
    "            X_valid = self.load_single_file(\"X_valid_small\")\n",
    "            X_test = self.load_single_file(\"X_test_small\")\n",
    "            y_train = self.load_single_file(\"y_train_small\")\n",
    "            y_valid = self.load_single_file(\"y_valid_small\")\n",
    "            y_test = self.load_single_file(\"y_test_small\")\n",
    "        elif(big==True):\n",
    "            print(\"loading big\")\n",
    "            X_train = self.load_single_file(\"X_train\")\n",
    "            X_valid = self.load_single_file(\"X_valid\")\n",
    "            X_test = self.load_single_file(\"X_test\")\n",
    "            y_train = self.load_single_file(\"y_train\")\n",
    "            y_valid = self.load_single_file(\"y_valid\")\n",
    "            y_test = self.load_single_file(\"y_test\")\n",
    "        elif(clean==True):\n",
    "            print(\"loading clean\")\n",
    "            X_train = self.load_single_file(\"X_train_clean\")\n",
    "            X_valid = self.load_single_file(\"X_valid_clean\")\n",
    "            X_test = self.load_single_file(\"X_test_clean\")\n",
    "            y_train = self.load_single_file(\"y_train_clean\")\n",
    "            y_valid = self.load_single_file(\"y_valid_clean\")\n",
    "            y_test = self.load_single_file(\"y_test_clean\")\n",
    "    \n",
    "        return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "\n",
    "\n",
    "    def num_sent(self,text):\n",
    "        '''\n",
    "        this is a gross approximation because we dont tokenize and clearly ellipsis are used in the text\n",
    "        and they increase the sentence count\n",
    "        '''\n",
    "        return text.count('.')\n",
    "\n",
    "    def num_words(self,text):\n",
    "        return len(text.split())\n",
    "\n",
    "\n",
    "    def clean_quora(self,quora_path,big=False,small=False,clean=False):\n",
    "        '''\n",
    "        input: path of quora tsv file downloaded from kaggle\n",
    "        output: df with questions <10 chars removed\n",
    "        orig: miminmal cleaning just enough to get training to pass. Remove blank lines or questions with no words.\n",
    "        clean_ten: clean char less than 10 long bc you get things like What??? which clearly are junk and not duplicates\n",
    "        clean_most: clean long questions which are clearly not duplicates\n",
    "        '''\n",
    "        checkList=[]\n",
    "        checkList.append(big), checkList.append(small), checkList.append(clean)\n",
    "        assert(checkList.count(True)==1)\n",
    "    \n",
    "        df = pd.read_csv(os.path.join(params.data_dir,\"quora_duplicate_questions.tsv\"),sep=\"\\t\")\n",
    "        #print(df.head())\n",
    "        df = df.drop([\"id\",\"qid1\",\"qid2\"],axis=1)\n",
    "        print(f\"before cleaning:{df.count()}\")\n",
    "        #blanks in question1 or question2 columns cause training loop error\n",
    "        df=df.dropna()\n",
    "        print(df.count())\n",
    "        #404287\n",
    "        #this is num chars!!! wrong!! \n",
    "        df['q1_chars'] = df['question1'].apply(len)\n",
    "        df['q2_chars'] = df['question2'].apply(len)\n",
    "        df['len_q1'] = df['question1'].apply(self.num_words)\n",
    "        df['len_q2'] = df['question2'].apply(self.num_words)\n",
    "        df['diff'] = df['question1'].apply(self.num_words) - df['question2'].apply(self.num_words)\n",
    "        df['num_q1_sent'] = df['question1'].apply(self.num_sent)\n",
    "        df['num_q2_sent'] = df['question2'].apply(self.num_sent)\n",
    "   \n",
    "        #print(df.head())\n",
    "        if big==True or small==True:\n",
    "            df = df.loc[ (df['q1_chars'] > 10) & (df['q2_chars'] > 10)]\n",
    "        if clean==True:\n",
    "            df = df.loc[ (df['q1_chars'] > 10) & (df['q2_chars'] > 10)]\n",
    "            #not correct\n",
    "            print(f\"before clean True:{df.count()}\")\n",
    "            df = df.loc[ (df['diff'] < 10) & (df['diff'] > -10)]\n",
    "            print(f\"after clean True:{df.count()}\")\n",
    "            #add more\n",
    "        print(f\" after clean:{df.count()}\")\n",
    "    \n",
    "        return df\n",
    "\n",
    "    def make_dataset(self,path,small=False,clean=False,big=False,small_percent=0.10):\n",
    "        '''\n",
    "        input: path: path where quora_duplicate.tsv\n",
    "        output: train, dev, valid tsv datasets\n",
    "        '''\n",
    "        checkList=[]\n",
    "        checkList.append(big), checkList.append(small), checkList.append(clean)\n",
    "        assert(checkList.count(True)==1)\n",
    "        \n",
    "        \n",
    "        df = self.clean_quora(params.data_dir,big=big,small=small,clean=clean)\n",
    "        print(df.values.shape)#(404290, 3)\n",
    "        if small==True:\n",
    "            num_rows = df.count()\n",
    "            print(\"total num_rows\",num_rows.values)\n",
    "            num_rows = (int)(num_rows.values[0]*.25)\n",
    "            print(\"small num_rows\",num_rows)\n",
    "            df = df[:num_rows]\n",
    "            print(f\"after small processing shape:{df.values.shape}\") #(404290, 3)\n",
    "        if (big==True or clean==True):\n",
    "            print(f\"after processing shape:{df.values.shape}\") #(404290, 3)\n",
    "        \n",
    "        print(f\"after processing, sre you small or large? shape:{df.values.shape}\") #(404290, 3)\n",
    "        #drop rows which have 0 in either column. Must be populated wq1 and q2\n",
    "        #lowercase and split dataframe\n",
    "        #df.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x)\n",
    "        #keep capital letters,\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[['question1','question2']].values, df[['is_duplicate']].values, test_size=0.40, random_state=42)\n",
    "        X_test,X_valid,y_test,y_valid = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n",
    "        print(f\"X_train.shape:{X_train.shape} X_train.shape:{y_train.shape}\")\n",
    "        print(f\"X_test.shape:{X_test.shape} y_test.shape:{y_test.shape}\")\n",
    "        print(f\"X_valid.shape:{X_valid.shape} y_valid.shape:{y_valid.shape}\")\n",
    "    \n",
    "        #print(X_train[:6],y_train[:6])\n",
    "        #print('---------------------')\n",
    "        #print(X_test[:6],y_test[:6])\n",
    "        #print('---------------------')\n",
    "        #print(X_valid[:6],y_valid[:6])\n",
    "    \n",
    "        return X_train,X_valid,X_test,y_train,y_valid,y_test\n",
    "\n",
    "    \n",
    "    def format_data(self, X_train, X_valid,X_test, y_train,y_valid,y_test):\n",
    "        s1 = {}\n",
    "        s2 = {}\n",
    "        target = {}\n",
    "        s1['train'],s1['dev'],s1['test'],s2['train'],s2['dev'],s2['test'] = {},{},{},{},{},{}\n",
    "        target['train'],target['dev'],target['test']={},{},{}\n",
    "        \n",
    "        s1['train']['sent'] = [x for x in X_train[:,0]]\n",
    "        s2['train']['sent'] = [x for x in X_train[:,1]]\n",
    "        s1['dev']['sent'] = [x for x in X_valid[:,0]]\n",
    "        s2['dev']['sent'] = [x for x in X_valid[:,1]]\n",
    "        s1['test']['sent'] = [x for x in X_test[:,0]]\n",
    "        s2['test']['sent'] = [x for x in X_test[:,1]]\n",
    "        target['train']['data'] = np.array([x[0] for x in y_train])\n",
    "        target['dev']['data'] = np.array([x[0] for x in y_valid])\n",
    "        target['test']['data'] = np.array([x[0] for x in y_test.tolist()])\n",
    "        \n",
    "        train = {'s1': s1['train']['sent'], 's2': s2['train']['sent'],\n",
    "                 'label': target['train']['data']}\n",
    "        dev = {'s1': s1['dev']['sent'], 's2': s2['dev']['sent'],\n",
    "               'label': target['dev']['data']}\n",
    "        test = {'s1': s1['test']['sent'], 's2': s2['test']['sent'],\n",
    "                'label': target['test']['data']}\n",
    "        return train,dev,test\n",
    "        \n",
    "    def get_word_dict(self,sentences):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        for sent in sentences:\n",
    "            for word in sent.split():\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict['<s>'] = ''\n",
    "        word_dict['</s>'] = ''\n",
    "        word_dict['<p>'] = ''\n",
    "        return word_dict\n",
    "    \n",
    "    def get_glove(self,word_dict, glove_path):\n",
    "        # create word_vec with glove vectors\n",
    "        word_vec = {}\n",
    "        with open(glove_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "        print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "            len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "    \n",
    "    \n",
    "    def build_vocab(self,sentences, glove_path):\n",
    "        word_dict = self.get_word_dict(sentences)\n",
    "        word_vec = self.get_glove(word_dict, glove_path)\n",
    "        print('Vocab size : {0}'.format(len(word_vec)))\n",
    "        return word_vec\n",
    "\n",
    "    def quora(self,big=False,small=False,clean=False):\n",
    "        X_train,X_valid,X_test,y_train,y_valid,y_test = self.load_data(big=big,small=small,clean=clean)\n",
    "        train,valid,test = self.format_data(X_train, X_valid,X_test, y_train,y_valid,y_test)\n",
    "        print(f\"quora checkpoint len(train[s1]):{len(train['s1'])},len(train[s2]):{len(train['s2'])},\\\n",
    "          len(train[label]):{len(train['label'])}\")\n",
    "        print('============')\n",
    "        print(f\"len(valid['s1']):{len(valid['s1'])}, len(valid[s2]):{len(valid['s2'])}, \\\n",
    "          len(valid['label']):{len(valid['label'])}\")\n",
    "        print('============')\n",
    "        print(f\"len(test['s1']):{len(test['s1'])},len(test['s2']):{len(test['s2'])}, \\\n",
    "          len(test['label']):{len(test['label'])}\")\n",
    "          \n",
    "        word_vec = self.build_vocab(train['s1'] + train['s2'] +\n",
    "                       valid['s1'] + valid['s2'] +\n",
    "                       test['s1'] + test['s2'], self.W2V_PATH)\n",
    "        for split in ['s1', 's2']:\n",
    "            for data_type in ['train', 'valid', 'test']:\n",
    "                eval(data_type)[split] = np.array([['<s>'] +\n",
    "                [word for word in sent.split() if word in word_vec] +\n",
    "                ['</s>'] for sent in eval(data_type)[split]])\n",
    "    \n",
    "        return train,valid,test,word_vec\n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        '''\n",
    "        create 3 datasets, big, small and clean. Print out stats to verify\n",
    "        '''\n",
    "        # test create large dataset\n",
    "        X_train,X_valid,X_test,y_train,y_valid,y_test = self.make_dataset(params.data_dir, small=False,big=True,clean=False)\n",
    "        self.save(X_train,X_valid,X_test,y_train,y_valid,y_test,big=True,small=False,clean=False)\n",
    "\n",
    "        #test loading large dataset\n",
    "        X_train,X_valid,X_test,y_train,y_valid,y_test = self.load_data(big=True,small=False,clean=False)\n",
    "        print(\"should see large dataset: 404158,242494,80832\")\n",
    "        print(type(X_train),X_train.shape,type(y_train),y_train.shape)\n",
    "        print(type(X_valid),X_valid.shape,type(y_valid),y_valid.shape)\n",
    "        print(type(X_test),X_test.shape,type(y_test),y_test.shape)\n",
    "\n",
    "        #test creating small dataset\n",
    "        X_train_small,X_valid_small,X_test_small,y_train_small,y_valid_small,y_test_small \\\n",
    "        = self.make_dataset(params.data_dir,big=False,small=True,clean=False)\n",
    "        self.save(X_train_small,X_valid_small,X_test_small,y_train_small,y_valid_small,y_test_small,\n",
    "             big=False,small=True,clean=False)\n",
    "\n",
    "        #test loading small dataset\n",
    "        X_train,X_valid,X_test,y_train,y_valid,y_test = self.load_data(small=True,big=False,clean=False)\n",
    "        print(\"should see small dataset: 60623,20208\")\n",
    "        print(type(X_train),X_train.shape,type(y_train),y_train.shape)\n",
    "        print(type(X_valid),X_valid.shape,type(y_valid),y_valid.shape)\n",
    "        print(type(X_test),X_test.shape,type(y_test),y_test.shape)\n",
    "\n",
    "        #test creating clean dataset\n",
    "        X_train_clean,X_valid_clean,X_test_clean,y_train_clean,y_valid_clean,y_test_clean = \\\n",
    "        self.make_dataset(params.data_dir, small=False,big=False,clean=True)\n",
    "        self.save(X_train_clean,X_valid_clean,X_test_clean,y_train_clean,y_valid_clean,y_test_clean, \n",
    "             big=False,small=False,clean=True)\n",
    "        #test loading clean dataset\n",
    "        X_train,X_valid,X_test,y_train,y_valid,y_test = self.load_data(small=False,big=False,clean=True)\n",
    "        print(\"should see clean dataset: 404158,80832\")\n",
    "        print(type(X_train),X_train.shape,type(y_train),y_train.shape)\n",
    "        print(type(X_valid),X_valid.shape,type(y_valid),y_valid.shape)\n",
    "        print(type(X_test),X_test.shape,type(y_test),y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning:question1       404289\n",
      "question2       404288\n",
      "is_duplicate    404290\n",
      "dtype: int64\n",
      "question1       404287\n",
      "question2       404287\n",
      "is_duplicate    404287\n",
      "dtype: int64\n",
      " after clean:question1       404158\n",
      "question2       404158\n",
      "is_duplicate    404158\n",
      "q1_chars        404158\n",
      "q2_chars        404158\n",
      "len_q1          404158\n",
      "len_q2          404158\n",
      "diff            404158\n",
      "num_q1_sent     404158\n",
      "num_q2_sent     404158\n",
      "dtype: int64\n",
      "(404158, 10)\n",
      "after processing shape:(404158, 10)\n",
      "after processing, sre you small or large? shape:(404158, 10)\n",
      "X_train.shape:(242494, 2) X_train.shape:(242494, 1)\n",
      "X_test.shape:(80832, 2) y_test.shape:(80832, 1)\n",
      "X_valid.shape:(80832, 2) y_valid.shape:(80832, 1)\n",
      "loading big\n",
      "should see large dataset: 404158,242494,80832\n",
      "<class 'numpy.ndarray'> (242494, 2) <class 'numpy.ndarray'> (242494, 1)\n",
      "<class 'numpy.ndarray'> (80832, 2) <class 'numpy.ndarray'> (80832, 1)\n",
      "<class 'numpy.ndarray'> (80832, 2) <class 'numpy.ndarray'> (80832, 1)\n",
      "before cleaning:question1       404289\n",
      "question2       404288\n",
      "is_duplicate    404290\n",
      "dtype: int64\n",
      "question1       404287\n",
      "question2       404287\n",
      "is_duplicate    404287\n",
      "dtype: int64\n",
      " after clean:question1       404158\n",
      "question2       404158\n",
      "is_duplicate    404158\n",
      "q1_chars        404158\n",
      "q2_chars        404158\n",
      "len_q1          404158\n",
      "len_q2          404158\n",
      "diff            404158\n",
      "num_q1_sent     404158\n",
      "num_q2_sent     404158\n",
      "dtype: int64\n",
      "(404158, 10)\n",
      "total num_rows [404158 404158 404158 404158 404158 404158 404158 404158 404158 404158]\n",
      "small num_rows 101039\n",
      "after small processing shape:(101039, 10)\n",
      "after processing, sre you small or large? shape:(101039, 10)\n",
      "X_train.shape:(60623, 2) X_train.shape:(60623, 1)\n",
      "X_test.shape:(20208, 2) y_test.shape:(20208, 1)\n",
      "X_valid.shape:(20208, 2) y_valid.shape:(20208, 1)\n",
      "loading small\n",
      "should see small dataset: 60623,20208\n",
      "<class 'numpy.ndarray'> (60623, 2) <class 'numpy.ndarray'> (60623, 1)\n",
      "<class 'numpy.ndarray'> (20208, 2) <class 'numpy.ndarray'> (20208, 1)\n",
      "<class 'numpy.ndarray'> (20208, 2) <class 'numpy.ndarray'> (20208, 1)\n",
      "before cleaning:question1       404289\n",
      "question2       404288\n",
      "is_duplicate    404290\n",
      "dtype: int64\n",
      "question1       404287\n",
      "question2       404287\n",
      "is_duplicate    404287\n",
      "dtype: int64\n",
      "before clean True:question1       404158\n",
      "question2       404158\n",
      "is_duplicate    404158\n",
      "q1_chars        404158\n",
      "q2_chars        404158\n",
      "len_q1          404158\n",
      "len_q2          404158\n",
      "diff            404158\n",
      "num_q1_sent     404158\n",
      "num_q2_sent     404158\n",
      "dtype: int64\n",
      "after clean True:question1       366111\n",
      "question2       366111\n",
      "is_duplicate    366111\n",
      "q1_chars        366111\n",
      "q2_chars        366111\n",
      "len_q1          366111\n",
      "len_q2          366111\n",
      "diff            366111\n",
      "num_q1_sent     366111\n",
      "num_q2_sent     366111\n",
      "dtype: int64\n",
      " after clean:question1       366111\n",
      "question2       366111\n",
      "is_duplicate    366111\n",
      "q1_chars        366111\n",
      "q2_chars        366111\n",
      "len_q1          366111\n",
      "len_q2          366111\n",
      "diff            366111\n",
      "num_q1_sent     366111\n",
      "num_q2_sent     366111\n",
      "dtype: int64\n",
      "(366111, 10)\n",
      "after processing shape:(366111, 10)\n",
      "after processing, sre you small or large? shape:(366111, 10)\n",
      "X_train.shape:(219666, 2) X_train.shape:(219666, 1)\n",
      "X_test.shape:(73222, 2) y_test.shape:(73222, 1)\n",
      "X_valid.shape:(73223, 2) y_valid.shape:(73223, 1)\n",
      "loading clean\n",
      "should see clean dataset: 404158,80832\n",
      "<class 'numpy.ndarray'> (219666, 2) <class 'numpy.ndarray'> (219666, 1)\n",
      "<class 'numpy.ndarray'> (73223, 2) <class 'numpy.ndarray'> (73223, 1)\n",
      "<class 'numpy.ndarray'> (73222, 2) <class 'numpy.ndarray'> (73222, 1)\n"
     ]
    }
   ],
   "source": [
    "#make_data = MakeData()\n",
    "#make_data.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
