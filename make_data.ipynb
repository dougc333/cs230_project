{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--data_dir\", type=str, default='/home/dc/cs230_project/dataset', help=\"store duplicate questions\")\n",
    "params, _ = parser.parse_known_args()\n",
    "\n",
    "print(params)\n",
    "\n",
    "def save(X_train,X_valid,X_test,y_train,y_valid,y_test):\n",
    "    save_single_file(\"X_train\",X_train)\n",
    "    save_single_file(\"X_valid\",X_valid)\n",
    "    save_single_file(\"X_test\",X_test)\n",
    "    save_single_file(\"y_train\",y_train)\n",
    "    save_single_file(\"y_valid\",y_valid)\n",
    "    save_single_file(\"y_test\",y_test)\n",
    "    \n",
    "    \n",
    "def save_single_file(filename,data):\n",
    "    fh = open(filename+'.pkl', 'wb+')\n",
    "    pickle.dump(data, fh)\n",
    "    fh.close()\n",
    "    \n",
    "def load_single_file(filename):\n",
    "    fh = open(os.path.join(params.data_dir,filename+'.pkl'),'rb')\n",
    "    data = pickle.load(fh)\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    #df = df.loc[ (df['q1_len'] > 10) & (df['q2_len'] > 10)]\n",
    "    '''\n",
    "    X_train = load_single_file(\"X_train\")\n",
    "    X_valid = load_single_file(\"X_valid\")\n",
    "    X_test = load_single_file(\"X_test\")\n",
    "    y_train = load_single_file(\"y_train\")\n",
    "    y_valid = load_single_file(\"y_valid\")\n",
    "    y_test = load_single_file(\"y_test\")\n",
    "    return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "    \n",
    "def make_dataset(path):\n",
    "    '''\n",
    "    input: path: path where quora_duplicate.tsv\n",
    "    output: train, dev, valid tsv datasets\n",
    "    '''\n",
    "    input_file = 'quora_duplicate_questions.tsv'\n",
    "    df = pd.read_csv(os.path.join(path,input_file),sep='\\t')\n",
    "    df = df.drop([\"id\",\"qid1\",\"qid2\"],axis=1)\n",
    "    print(df.head())\n",
    "    print(f\"num rows dataframe:{len(df)}\")\n",
    "    print(df.values.shape)\n",
    "    #drop rows which have 0 in either column. Must be populated wq1 and q2\n",
    "    #lowercase and split dataframe\n",
    "    #df.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x)\n",
    "    #keep capital letters,\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[['question1','question2']].values, df[['is_duplicate']].values, test_size=0.40, random_state=42)\n",
    "    X_test,X_valid,y_test,y_valid = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n",
    "    print(f\"X_train.shape:{X_train.shape} X_train.shape:{y_train.shape}\")\n",
    "    print(f\"X_test.shape:{X_test.shape} y_test.shape:{y_test.shape}\")\n",
    "    print(f\"X_valid.shape:{X_valid.shape} y_valid.shape:{y_valid.shape}\")\n",
    "    \n",
    "    print(X_train[:6],y_train[:6])\n",
    "    print('---------------------')\n",
    "    print(X_test[:6],y_test[:6])\n",
    "    print('---------------------')\n",
    "    print(X_valid[:6],y_valid[:6])\n",
    "    \n",
    "    return X_train,X_valid,X_test,y_train,y_valid,y_test\n",
    "    \n",
    "#X_train,X_valid,X_test,y_train,y_valid,y_test = make_dataset('/home/dc/cs230_project')\n",
    "#save(X_train,X_valid,X_test,y_train,y_valid,y_test)\n",
    "X_train,X_valid,X_test,y_train,y_valid,y_test = load_data()\n",
    "print(type(X_train),X_train.shape,type(y_train),y_train.shape)\n",
    "print(type(X_valid),X_valid.shape,type(y_valid),y_valid.shape)\n",
    "print(type(X_test),X_test.shape,type(y_test),y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reforamt into q1 and q2 because the questions have different lengt\n",
    "\n",
    "def format_data(X_train, X_valid,X_test, y_train,y_valid,y_test):\n",
    "    s1 = {}\n",
    "    s2 = {}\n",
    "    target = {}\n",
    "    s1['train'],s1['dev'],s1['test'],s2['train'],s2['dev'],s2['test'] = {},{},{},{},{},{}\n",
    "    target['train'],target['dev'],target['test']={},{},{}\n",
    "\n",
    "    s1['train']['sent'] = [x for x in X_train[:,0]]\n",
    "    s2['train']['sent'] = [x for x in X_train[:,1]]\n",
    "    s1['dev']['sent'] = [x for x in X_valid[:,0]]\n",
    "    s2['dev']['sent'] = [x for x in X_valid[:,1]]\n",
    "    s1['test']['sent'] = [x for x in X_test[:,0]]\n",
    "    s2['test']['sent'] = [x for x in X_test[:,1]]\n",
    "    target['train']['data'] = np.array([x[0] for x in y_train])\n",
    "    target['dev']['data'] = np.array([x[0] for x in y_valid])\n",
    "    target['test']['data'] = np.array([x[0] for x in y_test.tolist()])\n",
    "\n",
    "    train = {'s1': s1['train']['sent'], 's2': s2['train']['sent'],\n",
    "             'label': target['train']['data']}\n",
    "    dev = {'s1': s1['dev']['sent'], 's2': s2['dev']['sent'],\n",
    "           'label': target['dev']['data']}\n",
    "    test = {'s1': s1['test']['sent'], 's2': s2['test']['sent'],\n",
    "            'label': target['test']['data']}\n",
    "    return train,dev,test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid,X_test,y_train,y_valid,y_test = load_data()\n",
    "train,valid,test = format_data(X_train, X_valid,X_test, y_train,y_valid,y_test)\n",
    "print(type(train),type(valid),type(test))\n",
    "print(type(train['s1']),type(train['s2']),type(train['label']))\n",
    "print(type(valid['s1']),type(valid['s2']),type(valid['label']))\n",
    "print(type(test['s1']),type(test['s2']),type(test['label']))\n",
    "print(type(test['label']))\n",
    "print(test['label'])\n",
    "print(test['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[0], [1], [2]]])\n",
    "np.squeeze(x).shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def get_nli(data_path):\n",
    "    s1 = {}\n",
    "    s2 = {}\n",
    "    target = {}\n",
    "\n",
    "    dico_label = {'entailment': 0,  'neutral': 1, 'contradiction': 2}\n",
    "\n",
    "    for data_type in ['train', 'dev', 'test']:\n",
    "        s1[data_type], s2[data_type], target[data_type] = {}, {}, {}\n",
    "        s1[data_type]['path'] = os.path.join(data_path, 's1.' + data_type)\n",
    "        s2[data_type]['path'] = os.path.join(data_path, 's2.' + data_type)\n",
    "        target[data_type]['path'] = os.path.join(data_path,\n",
    "                                                 'labels.' + data_type)\n",
    "\n",
    "        s1[data_type]['sent'] = [line.rstrip() for line in\n",
    "                                 open(s1[data_type]['path'], 'r')]\n",
    "        s2[data_type]['sent'] = [line.rstrip() for line in\n",
    "                                 open(s2[data_type]['path'], 'r')]\n",
    "        target[data_type]['data'] = np.array([dico_label[line.rstrip('\\n')]\n",
    "                for line in open(target[data_type]['path'], 'r')])\n",
    "\n",
    "        assert len(s1[data_type]['sent']) == len(s2[data_type]['sent']) == \\\n",
    "            len(target[data_type]['data'])\n",
    "\n",
    "        print('** {0} DATA : Found {1} pairs of {2} sentences.'.format(\n",
    "                data_type.upper(), len(s1[data_type]['sent']), data_type))\n",
    "\n",
    "    train = {'s1': s1['train']['sent'], 's2': s2['train']['sent'],\n",
    "             'label': target['train']['data']}\n",
    "    dev = {'s1': s1['dev']['sent'], 's2': s2['dev']['sent'],\n",
    "           'label': target['dev']['data']}\n",
    "    test = {'s1': s1['test']['sent'], 's2': s2['test']['sent'],\n",
    "            'label': target['test']['data']}\n",
    "    return train, dev, test\n",
    "\n",
    "train, valid, test = get_nli(\"/home/dc/cs230_project/dataset/SNLI\")\n",
    "print(type(train),type(valid),type(test))\n",
    "print(type(train['s1']),type(train['s2']),type(train['label']))\n",
    "print(type(valid['s1']),type(valid['s2']),type(valid['label']))\n",
    "print(type(test['s1']),type(test['s2']),type(test['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#snli stats\n",
    "import os\n",
    "\n",
    "def num_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "snli_dir = 'dataset/SNLI'\n",
    "\n",
    "file_name=\"s1.train\"\n",
    "with open(os.path.join(snli_dir,file_name),\"r\") as fh:    \n",
    "    num_word = [num_words(line) for line in fh]\n",
    "print(len(num_word))\n",
    "print(type(num_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 60 8.251292851591012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.min(num_word),np.max(num_word),np.mean(num_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "<class 'list'>\n",
      "0 6\n",
      "Counter({1: 522769, 0: 26038, 2: 339, 3: 134, 4: 48, 5: 36, 6: 3})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def num_periods(text):\n",
    "    return text.count(\".\")\n",
    "\n",
    "file_name=\"s1.train\"\n",
    "with open(os.path.join(snli_dir,file_name),\"r\") as fh:    \n",
    "    num_sent = [num_periods(line) for line in fh]\n",
    "print(len(num_sent))\n",
    "print(type(num_sent))\n",
    "print(np.min(num_sent),np.max(num_sent))\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "print(collections.Counter(num_sent))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(num_sent,bins=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "<class 'list'>\n",
      "1 60 8.251292851591012\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def num_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "snli_dir = 'dataset/SNLI'\n",
    "\n",
    "file_name=\"s2.train\"\n",
    "with open(os.path.join(snli_dir,file_name),\"r\") as fh:    \n",
    "    num_word = [num_words(line) for line in fh]\n",
    "print(len(num_word))\n",
    "print(type(num_word))\n",
    "print(np.min(num_word),np.max(num_word),np.mean(num_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "<class 'list'>\n",
      "0 7\n"
     ]
    }
   ],
   "source": [
    "def num_periods(text):\n",
    "    return text.count(\".\")\n",
    "\n",
    "file_name=\"s2.train\"\n",
    "with open(os.path.join(snli_dir,file_name),\"r\") as fh:    \n",
    "    num_sent = [num_periods(line) for line in fh]\n",
    "print(len(num_sent))\n",
    "print(type(num_sent))\n",
    "print(np.min(num_sent),np.max(num_sent),np.mean(num_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
