{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_dir='/home/dc/cs230_project/dataset')\n",
      "\n",
      "togrep : ['-f', '/run/user/1000/jupyter/kernel-84c6d9f5-f2b2-4c60-97a8-bf9b044efa8d.json']\n",
      "\n",
      "Namespace(LSTM_num_layers=2, batch_size=128, data_dir='/home/dc/cs230_project/dataset', decay=0.99, dpout_fc=0.2, dpout_model=0.2, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=256, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=2, n_enc_layers=1, n_epochs=25, nonlinear_fc=1, optimizer='adam', outputdir='savedir/', outputmodelname='3layernonlinear_2features_adam_small.pickle', pool_type='max', seed=4, weight_decay=0.0005, word_emb_dim=300)\n",
      "loading small\n",
      "quora checkpoint len(train[s1]):60623,len(train[s2]):60623,          len(train[label]):60623\n",
      "============\n",
      "len(valid['s1']):20208, len(valid[s2]):20208,           len(valid['label']):20208\n",
      "============\n",
      "len(test['s1']):20208,len(test['s2']):20208,           len(test['label']):20208\n",
      "Found 47877(/106290) words with glove vectors\n",
      "Vocab size : 47877\n",
      "checkpoint after formatting: len(train[s1]):60623 ,len(train[s2]):60623       ,len(train[label]):60623, len(valid[s2]):20208 ,len(valid[s2]):20208,       len(valid[label]):20208,len(test[s2]):20208, len(test[s2]):20208       ,len(valid[label]):20208,len(word_vec):47877\n",
      "NLINet(\n",
      "  (encoder): InferSent(\n",
      "    (enc_lstm): LSTM(300, 2048, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2)\n",
      "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2)\n",
      "    (12): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:451: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num epochs:25\n",
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.001\n",
      "<class 'torch.Tensor'> tensor(8290) 8290\n",
      "12672 ; loss 0.62 ; sentence/s 177 ; words/s 13200 ; accuracy train : 64.77\n",
      "<class 'torch.Tensor'> tensor(17106) 17106\n",
      "25472 ; loss 0.59 ; sentence/s 173 ; words/s 13353 ; accuracy train : 66.82\n",
      "<class 'torch.Tensor'> tensor(25990) 25990\n",
      "38272 ; loss 0.58 ; sentence/s 176 ; words/s 13156 ; accuracy train : 67.68\n",
      "<class 'torch.Tensor'> tensor(34914) 34914\n",
      "51072 ; loss 0.58 ; sentence/s 176 ; words/s 13107 ; accuracy train : 68.19\n",
      "results : epoch 1 ; mean accuracy train : 68.56\n",
      "\n",
      "VALIDATION : Epoch 1\n",
      "list saved to file!\n",
      "list saved to file!\n",
      "togrep : results : epoch 1 ; mean accuracy valid :              70.95\n",
      "saving model at epoch 1\n",
      "\n",
      "TRAINING : Epoch 2\n",
      "Learning rate : 0.001\n",
      "<class 'torch.Tensor'> tensor(9147) 9147\n",
      "12672 ; loss 0.55 ; sentence/s 176 ; words/s 12968 ; accuracy train : 71.46\n",
      "<class 'torch.Tensor'> tensor(18243) 18243\n",
      "25472 ; loss 0.56 ; sentence/s 177 ; words/s 13065 ; accuracy train : 71.26\n",
      "<class 'torch.Tensor'> tensor(27468) 27468\n",
      "38272 ; loss 0.54 ; sentence/s 176 ; words/s 13095 ; accuracy train : 71.53\n",
      "<class 'torch.Tensor'> tensor(36732) 36732\n",
      "51072 ; loss 0.54 ; sentence/s 177 ; words/s 13130 ; accuracy train : 71.74\n",
      "results : epoch 2 ; mean accuracy train : 71.83\n",
      "\n",
      "VALIDATION : Epoch 2\n",
      "list saved to file!\n",
      "list saved to file!\n",
      "togrep : results : epoch 2 ; mean accuracy valid :              69.44\n",
      "\n",
      "TRAINING : Epoch 3\n",
      "Learning rate : 0.001\n",
      "<class 'torch.Tensor'> tensor(9317) 9317\n",
      "12672 ; loss 0.53 ; sentence/s 176 ; words/s 13125 ; accuracy train : 72.79\n",
      "<class 'torch.Tensor'> tensor(18682) 18682\n",
      "25472 ; loss 0.52 ; sentence/s 181 ; words/s 12576 ; accuracy train : 72.98\n",
      "<class 'torch.Tensor'> tensor(28091) 28091\n",
      "38272 ; loss 0.52 ; sentence/s 178 ; words/s 12994 ; accuracy train : 73.15\n",
      "<class 'torch.Tensor'> tensor(37521) 37521\n",
      "51072 ; loss 0.52 ; sentence/s 173 ; words/s 13410 ; accuracy train : 73.28\n",
      "results : epoch 3 ; mean accuracy train : 73.46\n",
      "\n",
      "VALIDATION : Epoch 3\n",
      "list saved to file!\n",
      "list saved to file!\n",
      "togrep : results : epoch 3 ; mean accuracy valid :              73.58\n",
      "saving model at epoch 3\n",
      "\n",
      "TRAINING : Epoch 4\n",
      "Learning rate : 0.001\n",
      "<class 'torch.Tensor'> tensor(9615) 9615\n",
      "12672 ; loss 0.49 ; sentence/s 173 ; words/s 13444 ; accuracy train : 75.12\n",
      "<class 'torch.Tensor'> tensor(19336) 19336\n",
      "25472 ; loss 0.49 ; sentence/s 175 ; words/s 13094 ; accuracy train : 75.53\n",
      "<class 'torch.Tensor'> tensor(28974) 28974\n",
      "38272 ; loss 0.49 ; sentence/s 177 ; words/s 13044 ; accuracy train : 75.45\n",
      "<class 'torch.Tensor'> tensor(38561) 38561\n",
      "51072 ; loss 0.49 ; sentence/s 179 ; words/s 12776 ; accuracy train : 75.31\n",
      "results : epoch 4 ; mean accuracy train : 75.36\n",
      "\n",
      "VALIDATION : Epoch 4\n",
      "list saved to file!\n",
      "list saved to file!\n",
      "togrep : results : epoch 4 ; mean accuracy valid :              73.29\n",
      "saving state dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type NLINet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type InferSent. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done saving state dict\n",
      "\n",
      "TEST : Epoch 5\n",
      "calculating validation error\n",
      "\n",
      "VALIDATION : Epoch 1000000.0\n",
      "list saved to file!\n",
      "list saved to file!\n",
      "finalgrep : accuracy valid : 73.29\n",
      "calculating test error\n",
      "list saved to file!\n",
      "list saved to file!\n",
      "finalgrep : accuracy test : 73.16\n",
      "fin 1604.5672342777252\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import inspect\n",
    "import time \n",
    "import torch\n",
    "from torchqrnn import QRNN\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from MakeData import MakeData\n",
    "\n",
    "#from data import get_nli, get_batch, build_vocab\n",
    "#from mutils import get_optimizer\n",
    "#from models import NLINet\n",
    "\n",
    "start_time = time.time()\n",
    "W2V_PATH = \"/home/dc/cs230_project/dataset/GloVe/glove.840B.300d.txt\"\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training')\n",
    "# paths\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='3layernonlinear_2features_adam_small.pickle')\n",
    "parser.add_argument(\"--modeldir\", type=str, default='rocandmodel/', help=\"roc and model directory\")\n",
    "parser.add_argument(\"--rocdir\", type=str, default='rocandmodel/', help=\"roc and model directory\")\n",
    "\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=25)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "#this only works if num_layers>1\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=.2, help=\"encoder dropout\")\n",
    "#this is only for the dropout after batchnorm in nonlinear\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0.2, help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"adam\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"weight decay for sgd\")\n",
    "\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSent', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=256, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=2, help=\"duplicate/not duplicate\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default='300', help=\"embedding dim\")\n",
    "parser.add_argument(\"--LSTM_num_layers\", type=int, default='2', help=\"LSTM num layers\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default='/home/dc/cs230_project/dataset', help=\"store duplicate questions\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=4, help=\"seed\")\n",
    "\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "\n",
    "# set gpu device\n",
    "torch.cuda.set_device(params.gpu_id)\n",
    "\n",
    "# print parameters passed, and all parameters\n",
    "print('\\ntogrep : {0}\\n'.format(sys.argv[1:]))\n",
    "print(params)\n",
    "\n",
    "\"\"\"\n",
    "SEED\n",
    "\"\"\"\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "\n",
    "make_data = MakeData()\n",
    "train, valid, test,word_vec = make_data.quora(big=False,small=True,clean=False)\n",
    "\n",
    "print(f\"checkpoint after formatting: len(train[s1]):{len(train['s1'])} ,len(train[s2]):{len(train['s2'])} \\\n",
    "      ,len(train[label]):{len(train['label'])}, len(valid[s2]):{len(valid['s1'])} ,len(valid[s2]):{len(valid['s2'])}, \\\n",
    "      len(valid[label]):{len(valid['label'])},len(test[s2]):{len(test['s1'])}, len(test[s2]):{len(test['s2'])} \\\n",
    "      ,len(valid[label]):{len(valid['label'])},len(word_vec):{len(word_vec)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODEL\n",
    "\"\"\"\n",
    "# model config\n",
    "config_nli_model = {\n",
    "    'n_words'        :  300          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, params.LSTM_num_layers,\n",
    "                                bidirectional=True, dropout=self.dpout_model)\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # either all weights are on cpu or they are on gpu\n",
    "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len: [max_len, ..., min_len] (bsize)\n",
    "        # sent: Variable(seqlen x bsize x worddim)\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # Sort by length (keep idx) https://github.com/pytorch/pytorch/issues/3584\n",
    "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() else torch.from_numpy(idx_sort)\n",
    "        sent = sent.index_select(1, Variable(idx_sort))\n",
    "\n",
    "        # Padding perf increase\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
    "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
    "\n",
    "        # Un-sort by length\n",
    "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_unsort)\n",
    "        sent_output = sent_output.index_select(1, Variable(idx_unsort))\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            sent_len = Variable(torch.FloatTensor(sent_len.copy())).unsqueeze(1).cuda()\n",
    "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
    "            emb = emb / sent_len.expand_as(emb)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb = torch.max(sent_output, 0)[0]\n",
    "            if emb.ndimension() == 3:\n",
    "                emb = emb.squeeze(0)\n",
    "                assert emb.ndimension() == 2\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with w2v vectors\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with k first w2v vectors\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if k <= K:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "                    k += 1\n",
    "                if k > K:\n",
    "                    if word in [self.bos, self.eos]:\n",
    "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    # build w2v vocab with k most frequent words\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (K))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # keep only new words\n",
    "        for word in self.word_vec:\n",
    "            if word in word_dict:\n",
    "                del word_dict[word]\n",
    "\n",
    "        # udpate vocabulary\n",
    "        if word_dict:\n",
    "            new_word_vec = self.get_w2v(word_dict)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "        else:\n",
    "            new_word_vec = []\n",
    "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # sent in batch in decreasing order of lengths\n",
    "        # batch: (bsize, max_len, word_dim)\n",
    "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "        return torch.FloatTensor(embed)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        if self.moses_tok:\n",
    "            s = ' '.join(word_tokenize(s))\n",
    "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
    "            return s.split()\n",
    "        else:\n",
    "            return word_tokenize(s)\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
    "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # filters words without w2v vectors\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
    "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
    "                s_f = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
    "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
    "\n",
    "        # sort by decreasing length\n",
    "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "        sentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "        return sentences, lengths, idx_sort\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "        sentences, lengths, idx_sort = self.prepare_samples(\n",
    "                        sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences), bsize):\n",
    "            batch = Variable(self.get_batch(\n",
    "                        sentences[stidx:stidx + bsize]), volatile=True)\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            batch = self.forward(\n",
    "                (batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "            embeddings.append(batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # unsort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
    "\n",
    "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
    "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        batch = Variable(self.get_batch(sent), volatile=True)\n",
    "\n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.enc_lstm(batch)[0]\n",
    "        output, idxs = torch.max(output, 0)\n",
    "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
    "        idxs = idxs.data.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "        # visualize model\n",
    "        import matplotlib.pyplot as plt\n",
    "        x = range(len(sent[0]))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent[0], rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs\n",
    "\n",
    "\n",
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = eval(self.encoder_type)(config)\n",
    "        self.inputdim = 2*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = ((int)(self.inputdim/2)) if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                \n",
    "                )\n",
    "        else:\n",
    "            print(f\"self.inputdim:{self.inputdim}, self.fc_dim:{self.fc_dim}\")\n",
    "            print(type(self.inputdim),type(self.fc_dim))\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def get_optimizer(s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    assert expected_args[:2] == ['self', 'params']\n",
    "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "            str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn, optim_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "encoder_types = ['InferSent', 'BLSTMprojEncoder', 'BGRUlastEncoder',\n",
    "                 'InnerAttentionMILAEncoder', 'InnerAttentionYANGEncoder',\n",
    "                 'InnerAttentionNAACLEncoder', 'ConvNetEncoder', 'LSTMEncoder']\n",
    "assert params.encoder_type in encoder_types, \"encoder_type must be in \" + \\\n",
    "                                             str(encoder_types)\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)\n",
    "\n",
    "\n",
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "#BCE next w2 categories\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = 10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None\n",
    "\n",
    "\n",
    "def get_batch(batch, word_vec):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), 300))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "\n",
    "\n",
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['s1']))\n",
    "    #print(f\"type(permutation):{type(permutation)}\")\n",
    "    #print(f\"type(train['s1']):{type(train['s1'])}\")\n",
    "    \n",
    "    s1 = train['s1'][permutation]\n",
    "    s2 = train['s2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "    \n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        #print(type(s1_batch),type(s2_batch)) #should be list\n",
    "        #print(f\"s1_len:{s1_len},s2_len:{s2_len}\")\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        target_batch=target[stidx:stidx + params.batch_size]\n",
    "        #print(f\"target_batch.shape:{target_batch.shape}\")\n",
    "        #print(f\"target_batch:{target_batch}\")\n",
    "        #print(f\"target shape:{target.shape}\")\n",
    "        #print(f\"target:{target[stidx:stidx + params.batch_size]}\")\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "        #print(f\"tgt_batch:{tgt_batch}\")\n",
    "        #print(f\"k:{k}\")\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        #print(f\"type(tgt_batch):{type(tgt_batch)}\")\n",
    "        #print(f\"type(output):{type(output)}\")\n",
    "        #print(f\"output size:{output.size()}\")\n",
    "        \n",
    "        #print(f\"output:{output}\")\n",
    "        #\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        all_costs.append(loss.item())\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "        \n",
    "        if len(all_costs) == 100:\n",
    "            print(type(correct),correct,correct.item())\n",
    "            #logs.append('{0} ; loss {1} accuracy:{2} ;'.format(stidx,round(np.mean(all_costs), 2),round(100.*correct.item()/(stidx+k), 2)))\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)), \n",
    "                            round(100.*correct.item()/(stidx+k), 2)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = round(100 * correct.item()/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "\n",
    "def save_list(my_list,filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(my_list,f)\n",
    "        f.close()\n",
    "    print(\"list saved to file!\")\n",
    "\n",
    "def read_list(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        my_list = pickle.load(f)\n",
    "    return my_list\n",
    "\n",
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['s1'] if eval_type == 'valid' else test['s1']\n",
    "    s2 = valid['s2'] if eval_type == 'valid' else test['s2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "    \n",
    "    predictions=[]\n",
    "    targets=[]\n",
    "    \n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        predictions.append(pred.cpu().data.numpy().tolist())\n",
    "        targets.append(tgt_batch.cpu().data.numpy().tolist())\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        \n",
    "        \n",
    "        \n",
    "    # save model\n",
    "    if eval_type == 'valid':\n",
    "        save_list(predictions,\"valid_\"+params.outputmodelname+\"_predict.pkl\")\n",
    "        save_list(targets,\"valid_\"+params.outputmodelname+\"_targets.pkl\")\n",
    "    else:\n",
    "        save_list(predictions,\"test_\"+params.outputmodelname+\"_predict.pkl\")\n",
    "        save_list(targets,\"test_\"+params.outputmodelname+\"_targets.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    eval_acc = round(100 * correct.item() / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train model on Natural Language Inference task\n",
    "\"\"\"\n",
    "epoch = 1\n",
    "print(f\"total num epochs:{params.n_epochs}\")\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1\n",
    "    \n",
    "#nli_net.save_state_dict(os.path.join(params.outputdir, params.outputmodelname))\n",
    "# Run best model on test set.\n",
    "#nli_net.load_state_dict(os.path.join(params.outputdir, params.outputmodelname))\n",
    "print(\"saving state dict\")\n",
    "torch.save(nli_net.state_dict,os.path.join(params.outputdir, params.outputmodelname + \"_statedict.pt\"))\n",
    "print(\"done saving state dict\")\n",
    "\n",
    "\n",
    "print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "print('calculating validation error')\n",
    "evaluate(1e6, 'valid', True)\n",
    "print('calculating test error')\n",
    "evaluate(0, 'test', True)\n",
    "\n",
    "# Save full model\n",
    "torch.save(nli_net, os.path.join(params.outputdir, params.outputmodelname + '_fullmodel.pt'))\n",
    "#save encoder\n",
    "torch.save(nli_net.encoder.state_dict(), os.path.join(params.outputdir, params.outputmodelname + '.encoder.pt'))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"fin\",elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Namespace(data_dir='/home/dc/cs230_project/dataset')\n",
    "\n",
    "togrep : ['-f', '/run/user/1000/jupyter/kernel-84c6d9f5-f2b2-4c60-97a8-bf9b044efa8d.json']\n",
    "\n",
    "Namespace(LSTM_num_layers=1, batch_size=128, data_dir='/home/dc/cs230_project/dataset', decay=0.99, dpout_fc=0.0, dpout_model=0.0, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=512, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=2, n_enc_layers=1, n_epochs=25, nonlinear_fc=1, optimizer='sgd,lr=0.1', outputdir='savedir/', outputmodelname='3layernonlinear_small.pickle', pool_type='max', seed=4, weight_decay=0.0005, word_emb_dim=300)\n",
    "loading small\n",
    "quora checkpoint len(train[s1]):60623,len(train[s2]):60623,          len(train[label]):60623\n",
    "============\n",
    "len(valid['s1']):20208, len(valid[s2]):20208,           len(valid['label']):20208\n",
    "============\n",
    "len(test['s1']):20208,len(test['s2']):20208,           len(test['label']):20208\n",
    "Found 47877(/106290) words with glove vectors\n",
    "Vocab size : 47877\n",
    "checkpoint after formatting: len(train[s1]):60623 ,len(train[s2]):60623       ,len(train[label]):60623, len(valid[s2]):20208 ,len(valid[s2]):20208,       len(valid[label]):20208,len(test[s2]):20208, len(test[s2]):20208       ,len(valid[label]):20208,len(word_vec):47877\n",
    "NLINet(\n",
    "  (encoder): InferSent(\n",
    "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=8192, out_features=512, bias=True)\n",
    "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU()\n",
    "    (3): Dropout(p=0.0)\n",
    "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (6): ReLU()\n",
    "    (7): Dropout(p=0.0)\n",
    "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (10): ReLU()\n",
    "    (11): Dropout(p=0.0)\n",
    "    (12): Linear(in_features=512, out_features=2, bias=True)\n",
    "  )\n",
    ")\n",
    "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:451: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
    "total num epochs:25\n",
    "\n",
    "TRAINING : Epoch 1\n",
    "Learning rate : 0.1\n",
    "<class 'torch.Tensor'> tensor(7675) 7675\n",
    "12672 ; loss 0.66 ; sentence/s 489 ; words/s 36377 ; accuracy train : 59.96\n",
    "<class 'torch.Tensor'> tensor(16018) 16018\n",
    "25472 ; loss 0.63 ; sentence/s 478 ; words/s 36814 ; accuracy train : 62.57\n",
    "<class 'torch.Tensor'> tensor(24485) 24485\n",
    "38272 ; loss 0.61 ; sentence/s 476 ; words/s 35583 ; accuracy train : 63.76\n",
    "<class 'torch.Tensor'> tensor(33070) 33070\n",
    "51072 ; loss 0.6 ; sentence/s 482 ; words/s 35757 ; accuracy train : 64.59\n",
    "results : epoch 1 ; mean accuracy train : 65.17\n",
    "\n",
    "VALIDATION : Epoch 1\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 1 ; mean accuracy valid :              69.05\n",
    "saving model at epoch 1\n",
    "\n",
    "TRAINING : Epoch 2\n",
    "Learning rate : 0.099\n",
    "<class 'torch.Tensor'> tensor(8939) 8939\n",
    "12672 ; loss 0.58 ; sentence/s 481 ; words/s 35322 ; accuracy train : 69.84\n",
    "<class 'torch.Tensor'> tensor(17858) 17858\n",
    "25472 ; loss 0.58 ; sentence/s 484 ; words/s 35645 ; accuracy train : 69.76\n",
    "<class 'torch.Tensor'> tensor(26873) 26873\n",
    "38272 ; loss 0.57 ; sentence/s 482 ; words/s 35780 ; accuracy train : 69.98\n",
    "<class 'torch.Tensor'> tensor(36008) 36008\n",
    "51072 ; loss 0.56 ; sentence/s 484 ; words/s 35955 ; accuracy train : 70.33\n",
    "results : epoch 2 ; mean accuracy train : 70.46\n",
    "\n",
    "VALIDATION : Epoch 2\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 2 ; mean accuracy valid :              70.65\n",
    "saving model at epoch 2\n",
    "\n",
    "TRAINING : Epoch 3\n",
    "Learning rate : 0.09801\n",
    "<class 'torch.Tensor'> tensor(9261) 9261\n",
    "12672 ; loss 0.55 ; sentence/s 480 ; words/s 35803 ; accuracy train : 72.35\n",
    "<class 'torch.Tensor'> tensor(18541) 18541\n",
    "25472 ; loss 0.55 ; sentence/s 497 ; words/s 34506 ; accuracy train : 72.43\n",
    "<class 'torch.Tensor'> tensor(27806) 27806\n",
    "38272 ; loss 0.54 ; sentence/s 487 ; words/s 35496 ; accuracy train : 72.41\n",
    "<class 'torch.Tensor'> tensor(37223) 37223\n",
    "51072 ; loss 0.54 ; sentence/s 471 ; words/s 36489 ; accuracy train : 72.7\n",
    "results : epoch 3 ; mean accuracy train : 72.78\n",
    "\n",
    "VALIDATION : Epoch 3\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 3 ; mean accuracy valid :              72.23\n",
    "saving model at epoch 3\n",
    "\n",
    "TRAINING : Epoch 4\n",
    "Learning rate : 0.0970299\n",
    "<class 'torch.Tensor'> tensor(9467) 9467\n",
    "12672 ; loss 0.52 ; sentence/s 473 ; words/s 36619 ; accuracy train : 73.96\n",
    "<class 'torch.Tensor'> tensor(19088) 19088\n",
    "25472 ; loss 0.51 ; sentence/s 475 ; words/s 35497 ; accuracy train : 74.56\n",
    "<class 'torch.Tensor'> tensor(28571) 28571\n",
    "38272 ; loss 0.52 ; sentence/s 484 ; words/s 35587 ; accuracy train : 74.4\n",
    "<class 'torch.Tensor'> tensor(38079) 38079\n",
    "51072 ; loss 0.51 ; sentence/s 488 ; words/s 34877 ; accuracy train : 74.37\n",
    "results : epoch 4 ; mean accuracy train : 74.39\n",
    "\n",
    "VALIDATION : Epoch 4\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 4 ; mean accuracy valid :              72.59\n",
    "saving model at epoch 4\n",
    "\n",
    "TRAINING : Epoch 5\n",
    "Learning rate : 0.096059601\n",
    "<class 'torch.Tensor'> tensor(9709) 9709\n",
    "12672 ; loss 0.5 ; sentence/s 484 ; words/s 35618 ; accuracy train : 75.85\n",
    "<class 'torch.Tensor'> tensor(19515) 19515\n",
    "25472 ; loss 0.49 ; sentence/s 481 ; words/s 35667 ; accuracy train : 76.23\n",
    "<class 'torch.Tensor'> tensor(29224) 29224\n",
    "38272 ; loss 0.5 ; sentence/s 482 ; words/s 35829 ; accuracy train : 76.1\n",
    "<class 'torch.Tensor'> tensor(39029) 39029\n",
    "51072 ; loss 0.49 ; sentence/s 482 ; words/s 35157 ; accuracy train : 76.23\n",
    "results : epoch 5 ; mean accuracy train : 76.28\n",
    "\n",
    "VALIDATION : Epoch 5\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 5 ; mean accuracy valid :              73.88\n",
    "saving model at epoch 5\n",
    "\n",
    "TRAINING : Epoch 6\n",
    "Learning rate : 0.09509900499\n",
    "<class 'torch.Tensor'> tensor(9864) 9864\n",
    "12672 ; loss 0.48 ; sentence/s 479 ; words/s 35937 ; accuracy train : 77.06\n",
    "<class 'torch.Tensor'> tensor(19741) 19741\n",
    "25472 ; loss 0.48 ; sentence/s 481 ; words/s 35389 ; accuracy train : 77.11\n",
    "<class 'torch.Tensor'> tensor(29756) 29756\n",
    "38272 ; loss 0.46 ; sentence/s 481 ; words/s 35559 ; accuracy train : 77.49\n",
    "<class 'torch.Tensor'> tensor(39700) 39700\n",
    "51072 ; loss 0.47 ; sentence/s 481 ; words/s 35704 ; accuracy train : 77.54\n",
    "results : epoch 6 ; mean accuracy train : 77.58\n",
    "\n",
    "VALIDATION : Epoch 6\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 6 ; mean accuracy valid :              74.18\n",
    "saving model at epoch 6\n",
    "\n",
    "TRAINING : Epoch 7\n",
    "Learning rate : 0.0941480149401\n",
    "<class 'torch.Tensor'> tensor(10180) 10180\n",
    "12672 ; loss 0.44 ; sentence/s 480 ; words/s 36437 ; accuracy train : 79.53\n",
    "<class 'torch.Tensor'> tensor(20326) 20326\n",
    "25472 ; loss 0.45 ; sentence/s 477 ; words/s 35162 ; accuracy train : 79.4\n",
    "<class 'torch.Tensor'> tensor(30446) 30446\n",
    "38272 ; loss 0.45 ; sentence/s 484 ; words/s 35598 ; accuracy train : 79.29\n",
    "<class 'torch.Tensor'> tensor(40536) 40536\n",
    "51072 ; loss 0.45 ; sentence/s 485 ; words/s 35429 ; accuracy train : 79.17\n",
    "results : epoch 7 ; mean accuracy train : 79.25\n",
    "\n",
    "VALIDATION : Epoch 7\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 7 ; mean accuracy valid :              74.98\n",
    "saving model at epoch 7\n",
    "\n",
    "TRAINING : Epoch 8\n",
    "Learning rate : 0.093206534790699\n",
    "<class 'torch.Tensor'> tensor(10340) 10340\n",
    "12672 ; loss 0.42 ; sentence/s 486 ; words/s 35450 ; accuracy train : 80.78\n",
    "<class 'torch.Tensor'> tensor(20750) 20750\n",
    "25472 ; loss 0.42 ; sentence/s 475 ; words/s 35651 ; accuracy train : 81.05\n",
    "<class 'torch.Tensor'> tensor(31102) 31102\n",
    "38272 ; loss 0.42 ; sentence/s 489 ; words/s 35027 ; accuracy train : 80.99\n",
    "<class 'torch.Tensor'> tensor(41372) 41372\n",
    "51072 ; loss 0.43 ; sentence/s 483 ; words/s 35428 ; accuracy train : 80.8\n",
    "results : epoch 8 ; mean accuracy train : 80.74\n",
    "\n",
    "VALIDATION : Epoch 8\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 8 ; mean accuracy valid :              75.1\n",
    "saving model at epoch 8\n",
    "\n",
    "TRAINING : Epoch 9\n",
    "Learning rate : 0.09227446944279201\n",
    "<class 'torch.Tensor'> tensor(10552) 10552\n",
    "12672 ; loss 0.4 ; sentence/s 483 ; words/s 35580 ; accuracy train : 82.44\n",
    "<class 'torch.Tensor'> tensor(21099) 21099\n",
    "25472 ; loss 0.4 ; sentence/s 474 ; words/s 35980 ; accuracy train : 82.42\n",
    "<class 'torch.Tensor'> tensor(31640) 31640\n",
    "38272 ; loss 0.4 ; sentence/s 482 ; words/s 35343 ; accuracy train : 82.4\n",
    "<class 'torch.Tensor'> tensor(42162) 42162\n",
    "51072 ; loss 0.4 ; sentence/s 489 ; words/s 35740 ; accuracy train : 82.35\n",
    "results : epoch 9 ; mean accuracy train : 82.36\n",
    "\n",
    "VALIDATION : Epoch 9\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 9 ; mean accuracy valid :              74.53\n",
    "Shrinking lr by : 5. New lr = 0.0184548938885584\n",
    "\n",
    "TRAINING : Epoch 10\n",
    "Learning rate : 0.018270344949672817\n",
    "<class 'torch.Tensor'> tensor(10821) 10821\n",
    "12672 ; loss 0.37 ; sentence/s 479 ; words/s 35889 ; accuracy train : 84.54\n",
    "<class 'torch.Tensor'> tensor(21626) 21626\n",
    "25472 ; loss 0.37 ; sentence/s 488 ; words/s 35286 ; accuracy train : 84.48\n",
    "<class 'torch.Tensor'> tensor(32452) 32452\n",
    "38272 ; loss 0.37 ; sentence/s 473 ; words/s 35806 ; accuracy train : 84.51\n",
    "<class 'torch.Tensor'> tensor(43231) 43231\n",
    "51072 ; loss 0.37 ; sentence/s 485 ; words/s 35219 ; accuracy train : 84.44\n",
    "results : epoch 10 ; mean accuracy train : 84.42\n",
    "\n",
    "VALIDATION : Epoch 10\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 10 ; mean accuracy valid :              75.76\n",
    "saving model at epoch 10\n",
    "\n",
    "TRAINING : Epoch 11\n",
    "Learning rate : 0.01808764150017609\n",
    "<class 'torch.Tensor'> tensor(10892) 10892\n",
    "12672 ; loss 0.36 ; sentence/s 475 ; words/s 36440 ; accuracy train : 85.09\n",
    "<class 'torch.Tensor'> tensor(21746) 21746\n",
    "25472 ; loss 0.36 ; sentence/s 487 ; words/s 35107 ; accuracy train : 84.95\n",
    "<class 'torch.Tensor'> tensor(32658) 32658\n",
    "38272 ; loss 0.36 ; sentence/s 486 ; words/s 35541 ; accuracy train : 85.05\n",
    "<class 'torch.Tensor'> tensor(43529) 43529\n",
    "51072 ; loss 0.36 ; sentence/s 481 ; words/s 35812 ; accuracy train : 85.02\n",
    "results : epoch 11 ; mean accuracy train : 84.94\n",
    "\n",
    "VALIDATION : Epoch 11\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 11 ; mean accuracy valid :              75.67\n",
    "Shrinking lr by : 5. New lr = 0.003617528300035218\n",
    "\n",
    "TRAINING : Epoch 12\n",
    "Learning rate : 0.003581353017034866\n",
    "<class 'torch.Tensor'> tensor(10935) 10935\n",
    "12672 ; loss 0.35 ; sentence/s 480 ; words/s 35747 ; accuracy train : 85.43\n",
    "<class 'torch.Tensor'> tensor(21850) 21850\n",
    "25472 ; loss 0.36 ; sentence/s 486 ; words/s 35195 ; accuracy train : 85.35\n",
    "<class 'torch.Tensor'> tensor(32808) 32808\n",
    "38272 ; loss 0.36 ; sentence/s 482 ; words/s 35719 ; accuracy train : 85.44\n",
    "<class 'torch.Tensor'> tensor(43794) 43794\n",
    "51072 ; loss 0.35 ; sentence/s 474 ; words/s 36091 ; accuracy train : 85.54\n",
    "results : epoch 12 ; mean accuracy train : 85.52\n",
    "\n",
    "VALIDATION : Epoch 12\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 12 ; mean accuracy valid :              75.78\n",
    "saving model at epoch 12\n",
    "\n",
    "TRAINING : Epoch 13\n",
    "Learning rate : 0.0035455394868645173\n",
    "<class 'torch.Tensor'> tensor(10940) 10940\n",
    "12672 ; loss 0.35 ; sentence/s 487 ; words/s 35684 ; accuracy train : 85.47\n",
    "<class 'torch.Tensor'> tensor(21907) 21907\n",
    "25472 ; loss 0.35 ; sentence/s 474 ; words/s 35940 ; accuracy train : 85.57\n",
    "<class 'torch.Tensor'> tensor(32776) 32776\n",
    "38272 ; loss 0.36 ; sentence/s 479 ; words/s 35793 ; accuracy train : 85.35\n",
    "<class 'torch.Tensor'> tensor(43692) 43692\n",
    "51072 ; loss 0.35 ; sentence/s 481 ; words/s 35401 ; accuracy train : 85.34\n",
    "results : epoch 13 ; mean accuracy train : 85.44\n",
    "\n",
    "VALIDATION : Epoch 13\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 13 ; mean accuracy valid :              75.76\n",
    "Shrinking lr by : 5. New lr = 0.0007091078973729035\n",
    "\n",
    "TRAINING : Epoch 14\n",
    "Learning rate : 0.0007020168183991745\n",
    "<class 'torch.Tensor'> tensor(10923) 10923\n",
    "12672 ; loss 0.35 ; sentence/s 480 ; words/s 36041 ; accuracy train : 85.34\n",
    "<class 'torch.Tensor'> tensor(21887) 21887\n",
    "25472 ; loss 0.35 ; sentence/s 472 ; words/s 36071 ; accuracy train : 85.5\n",
    "<class 'torch.Tensor'> tensor(32837) 32837\n",
    "38272 ; loss 0.35 ; sentence/s 484 ; words/s 35422 ; accuracy train : 85.51\n",
    "<class 'torch.Tensor'> tensor(43839) 43839\n",
    "51072 ; loss 0.35 ; sentence/s 486 ; words/s 35501 ; accuracy train : 85.62\n",
    "results : epoch 14 ; mean accuracy train : 85.61\n",
    "\n",
    "VALIDATION : Epoch 14\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 14 ; mean accuracy valid :              75.74\n",
    "Shrinking lr by : 5. New lr = 0.00014040336367983488\n",
    "\n",
    "TRAINING : Epoch 15\n",
    "Learning rate : 0.00013899933004303652\n",
    "<class 'torch.Tensor'> tensor(11011) 11011\n",
    "12672 ; loss 0.35 ; sentence/s 480 ; words/s 35403 ; accuracy train : 86.02\n",
    "<class 'torch.Tensor'> tensor(21945) 21945\n",
    "25472 ; loss 0.35 ; sentence/s 470 ; words/s 36422 ; accuracy train : 85.72\n",
    "<class 'torch.Tensor'> tensor(32890) 32890\n",
    "38272 ; loss 0.35 ; sentence/s 477 ; words/s 35892 ; accuracy train : 85.65\n",
    "<class 'torch.Tensor'> tensor(43856) 43856\n",
    "51072 ; loss 0.35 ; sentence/s 484 ; words/s 35567 ; accuracy train : 85.66\n",
    "results : epoch 15 ; mean accuracy train : 85.59\n",
    "\n",
    "VALIDATION : Epoch 15\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 15 ; mean accuracy valid :              75.84\n",
    "saving model at epoch 15\n",
    "\n",
    "TRAINING : Epoch 16\n",
    "Learning rate : 0.00013760933674260615\n",
    "<class 'torch.Tensor'> tensor(11014) 11014\n",
    "12672 ; loss 0.35 ; sentence/s 482 ; words/s 35737 ; accuracy train : 86.05\n",
    "<class 'torch.Tensor'> tensor(21987) 21987\n",
    "25472 ; loss 0.35 ; sentence/s 478 ; words/s 36395 ; accuracy train : 85.89\n",
    "<class 'torch.Tensor'> tensor(32961) 32961\n",
    "38272 ; loss 0.35 ; sentence/s 476 ; words/s 35849 ; accuracy train : 85.84\n",
    "<class 'torch.Tensor'> tensor(43896) 43896\n",
    "51072 ; loss 0.36 ; sentence/s 485 ; words/s 35720 ; accuracy train : 85.73\n",
    "results : epoch 16 ; mean accuracy train : 85.77\n",
    "\n",
    "VALIDATION : Epoch 16\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 16 ; mean accuracy valid :              75.73\n",
    "Shrinking lr by : 5. New lr = 2.7521867348521228e-05\n",
    "\n",
    "TRAINING : Epoch 17\n",
    "Learning rate : 2.7246648675036015e-05\n",
    "<class 'torch.Tensor'> tensor(10983) 10983\n",
    "12672 ; loss 0.35 ; sentence/s 479 ; words/s 35459 ; accuracy train : 85.8\n",
    "<class 'torch.Tensor'> tensor(21887) 21887\n",
    "25472 ; loss 0.35 ; sentence/s 478 ; words/s 35508 ; accuracy train : 85.5\n",
    "<class 'torch.Tensor'> tensor(32873) 32873\n",
    "38272 ; loss 0.35 ; sentence/s 485 ; words/s 35416 ; accuracy train : 85.61\n",
    "<class 'torch.Tensor'> tensor(43913) 43913\n",
    "51072 ; loss 0.35 ; sentence/s 477 ; words/s 35435 ; accuracy train : 85.77\n",
    "results : epoch 17 ; mean accuracy train : 85.75\n",
    "\n",
    "VALIDATION : Epoch 17\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 17 ; mean accuracy valid :              75.72\n",
    "Shrinking lr by : 5. New lr = 5.449329735007203e-06\n",
    "saving state dict\n",
    "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type NLINet. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type InferSent. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "done saving state dict\n",
    "\n",
    "TEST : Epoch 18\n",
    "calculating validation error\n",
    "\n",
    "VALIDATION : Epoch 1000000.0\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "finalgrep : accuracy valid : 75.72\n",
    "calculating test error\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "finalgrep : accuracy test : 75.16\n",
    "fin 2433.8374314308167\n",
    "\n",
    "1\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make learning rate bigger. improves training error faster but it also makes difference between train and validation error\n",
    "bigger faster also. try clean dataset. should see less overfitting with clean dataset. stop when difference between\n",
    "validation error and train error is 10. \n",
    "\n",
    "THIS IS WORSE!!!!\n",
    "\n",
    "Stuck in local minima. VE doesnt move past 69.xx over epochs. CHANGE TO ADAM FROM SGD\n",
    "\n",
    "Namespace(data_dir='/home/dc/cs230_project/dataset')\n",
    "\n",
    "togrep : ['-f', '/run/user/1000/jupyter/kernel-84c6d9f5-f2b2-4c60-97a8-bf9b044efa8d.json']\n",
    "\n",
    "Namespace(LSTM_num_layers=2, batch_size=128, data_dir='/home/dc/cs230_project/dataset', decay=0.99, dpout_fc=0.2, dpout_model=0.2, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=256, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=2, n_enc_layers=1, n_epochs=25, nonlinear_fc=1, optimizer='sgd,lr=0.3', outputdir='savedir/', outputmodelname='3layernonlinear_small.pickle', pool_type='max', seed=4, weight_decay=0.0005, word_emb_dim=300)\n",
    "loading small\n",
    "quora checkpoint len(train[s1]):60623,len(train[s2]):60623,          len(train[label]):60623\n",
    "============\n",
    "len(valid['s1']):20208, len(valid[s2]):20208,           len(valid['label']):20208\n",
    "============\n",
    "len(test['s1']):20208,len(test['s2']):20208,           len(test['label']):20208\n",
    "Found 47877(/106290) words with glove vectors\n",
    "Vocab size : 47877\n",
    "checkpoint after formatting: len(train[s1]):60623 ,len(train[s2]):60623       ,len(train[label]):60623, len(valid[s2]):20208 ,len(valid[s2]):20208,       len(valid[label]):20208,len(test[s2]):20208, len(test[s2]):20208       ,len(valid[label]):20208,len(word_vec):47877\n",
    "NLINet(\n",
    "  (encoder): InferSent(\n",
    "    (enc_lstm): LSTM(300, 2048, num_layers=2, dropout=0.2, bidirectional=True)\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
    "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU()\n",
    "    (3): Dropout(p=0.2)\n",
    "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
    "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (6): ReLU()\n",
    "    (7): Dropout(p=0.2)\n",
    "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
    "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (10): ReLU()\n",
    "    (11): Dropout(p=0.2)\n",
    "    (12): Linear(in_features=256, out_features=2, bias=True)\n",
    "  )\n",
    ")\n",
    "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:451: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
    "total num epochs:25\n",
    "\n",
    "TRAINING : Epoch 1\n",
    "Learning rate : 0.3\n",
    "<class 'torch.Tensor'> tensor(7724) 7724\n",
    "12672 ; loss 0.66 ; sentence/s 182 ; words/s 13521 ; accuracy train : 60.34\n",
    "<class 'torch.Tensor'> tensor(15976) 15976\n",
    "25472 ; loss 0.63 ; sentence/s 178 ; words/s 13709 ; accuracy train : 62.41\n",
    "<class 'torch.Tensor'> tensor(24324) 24324\n",
    "38272 ; loss 0.62 ; sentence/s 180 ; words/s 13447 ; accuracy train : 63.34\n",
    "<class 'torch.Tensor'> tensor(32860) 32860\n",
    "51072 ; loss 0.61 ; sentence/s 181 ; words/s 13423 ; accuracy train : 64.18\n",
    "results : epoch 1 ; mean accuracy train : 64.72\n",
    "\n",
    "VALIDATION : Epoch 1\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 1 ; mean accuracy valid :              66.25\n",
    "saving model at epoch 1\n",
    "\n",
    "TRAINING : Epoch 2\n",
    "Learning rate : 0.297\n",
    "<class 'torch.Tensor'> tensor(8740) 8740\n",
    "12672 ; loss 0.59 ; sentence/s 180 ; words/s 13240 ; accuracy train : 68.28\n",
    "<class 'torch.Tensor'> tensor(17463) 17463\n",
    "25472 ; loss 0.59 ; sentence/s 181 ; words/s 13371 ; accuracy train : 68.21\n",
    "<class 'torch.Tensor'> tensor(26365) 26365\n",
    "38272 ; loss 0.58 ; sentence/s 181 ; words/s 13430 ; accuracy train : 68.66\n",
    "<class 'torch.Tensor'> tensor(35268) 35268\n",
    "51072 ; loss 0.58 ; sentence/s 181 ; words/s 13474 ; accuracy train : 68.88\n",
    "results : epoch 2 ; mean accuracy train : 69.0\n",
    "\n",
    "VALIDATION : Epoch 2\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 2 ; mean accuracy valid :              66.92\n",
    "saving model at epoch 2\n",
    "\n",
    "TRAINING : Epoch 3\n",
    "Learning rate : 0.29402999999999996\n",
    "<class 'torch.Tensor'> tensor(9016) 9016\n",
    "12672 ; loss 0.57 ; sentence/s 180 ; words/s 13451 ; accuracy train : 70.44\n",
    "<class 'torch.Tensor'> tensor(17996) 17996\n",
    "25472 ; loss 0.57 ; sentence/s 185 ; words/s 12877 ; accuracy train : 70.3\n",
    "<class 'torch.Tensor'> tensor(27052) 27052\n",
    "38272 ; loss 0.56 ; sentence/s 182 ; words/s 13273 ; accuracy train : 70.45\n",
    "<class 'torch.Tensor'> tensor(36128) 36128\n",
    "51072 ; loss 0.56 ; sentence/s 178 ; words/s 13782 ; accuracy train : 70.56\n",
    "results : epoch 3 ; mean accuracy train : 70.6\n",
    "\n",
    "VALIDATION : Epoch 3\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 3 ; mean accuracy valid :              66.43\n",
    "Shrinking lr by : 5. New lr = 0.05880599999999999\n",
    "\n",
    "TRAINING : Epoch 4\n",
    "Learning rate : 0.05821793999999999\n",
    "<class 'torch.Tensor'> tensor(9125) 9125\n",
    "12672 ; loss 0.55 ; sentence/s 177 ; words/s 13757 ; accuracy train : 71.29\n",
    "<class 'torch.Tensor'> tensor(18423) 18423\n",
    "25472 ; loss 0.55 ; sentence/s 180 ; words/s 13430 ; accuracy train : 71.96\n",
    "<class 'torch.Tensor'> tensor(27569) 27569\n",
    "38272 ; loss 0.55 ; sentence/s 181 ; words/s 13338 ; accuracy train : 71.79\n",
    "<class 'torch.Tensor'> tensor(36777) 36777\n",
    "51072 ; loss 0.55 ; sentence/s 183 ; words/s 13084 ; accuracy train : 71.83\n",
    "results : epoch 4 ; mean accuracy train : 71.75\n",
    "\n",
    "VALIDATION : Epoch 4\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 4 ; mean accuracy valid :              70.16\n",
    "saving model at epoch 4\n",
    "\n",
    "TRAINING : Epoch 5\n",
    "Learning rate : 0.05763576059999999\n",
    "<class 'torch.Tensor'> tensor(9195) 9195\n",
    "12672 ; loss 0.55 ; sentence/s 181 ; words/s 13350 ; accuracy train : 71.84\n",
    "<class 'torch.Tensor'> tensor(18383) 18383\n",
    "25472 ; loss 0.55 ; sentence/s 180 ; words/s 13397 ; accuracy train : 71.81\n",
    "<class 'torch.Tensor'> tensor(27510) 27510\n",
    "38272 ; loss 0.55 ; sentence/s 181 ; words/s 13490 ; accuracy train : 71.64\n",
    "<class 'torch.Tensor'> tensor(36632) 36632\n",
    "51072 ; loss 0.55 ; sentence/s 181 ; words/s 13213 ; accuracy train : 71.55\n",
    "results : epoch 5 ; mean accuracy train : 71.65\n",
    "\n",
    "VALIDATION : Epoch 5\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 5 ; mean accuracy valid :              69.7\n",
    "Shrinking lr by : 5. New lr = 0.011527152119999998\n",
    "\n",
    "TRAINING : Epoch 6\n",
    "Learning rate : 0.011411880598799998\n",
    "<class 'torch.Tensor'> tensor(9211) 9211\n",
    "12672 ; loss 0.54 ; sentence/s 179 ; words/s 13467 ; accuracy train : 71.96\n",
    "<class 'torch.Tensor'> tensor(18398) 18398\n",
    "25472 ; loss 0.55 ; sentence/s 182 ; words/s 13372 ; accuracy train : 71.87\n",
    "<class 'torch.Tensor'> tensor(27667) 27667\n",
    "38272 ; loss 0.54 ; sentence/s 180 ; words/s 13345 ; accuracy train : 72.05\n",
    "<class 'torch.Tensor'> tensor(36835) 36835\n",
    "51072 ; loss 0.55 ; sentence/s 182 ; words/s 13495 ; accuracy train : 71.94\n",
    "results : epoch 6 ; mean accuracy train : 71.93\n",
    "\n",
    "VALIDATION : Epoch 6\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 6 ; mean accuracy valid :              69.74\n",
    "Shrinking lr by : 5. New lr = 0.0022823761197599997\n",
    "\n",
    "TRAINING : Epoch 7\n",
    "Learning rate : 0.0022595523585623996\n",
    "<class 'torch.Tensor'> tensor(9264) 9264\n",
    "12672 ; loss 0.54 ; sentence/s 180 ; words/s 13662 ; accuracy train : 72.38\n",
    "<class 'torch.Tensor'> tensor(18452) 18452\n",
    "25472 ; loss 0.55 ; sentence/s 180 ; words/s 13283 ; accuracy train : 72.08\n",
    "<class 'torch.Tensor'> tensor(27661) 27661\n",
    "38272 ; loss 0.55 ; sentence/s 181 ; words/s 13376 ; accuracy train : 72.03\n",
    "<class 'torch.Tensor'> tensor(36859) 36859\n",
    "51072 ; loss 0.54 ; sentence/s 181 ; words/s 13275 ; accuracy train : 71.99\n",
    "results : epoch 7 ; mean accuracy train : 72.13\n",
    "\n",
    "VALIDATION : Epoch 7\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 7 ; mean accuracy valid :              69.45\n",
    "Shrinking lr by : 5. New lr = 0.0004519104717124799\n",
    "\n",
    "TRAINING : Epoch 8\n",
    "Learning rate : 0.0004473913669953551\n",
    "<class 'torch.Tensor'> tensor(9227) 9227\n",
    "12672 ; loss 0.54 ; sentence/s 182 ; words/s 13296 ; accuracy train : 72.09\n",
    "<class 'torch.Tensor'> tensor(18526) 18526\n",
    "25472 ; loss 0.54 ; sentence/s 179 ; words/s 13446 ; accuracy train : 72.37\n",
    "<class 'torch.Tensor'> tensor(27761) 27761\n",
    "38272 ; loss 0.54 ; sentence/s 184 ; words/s 13172 ; accuracy train : 72.29\n",
    "<class 'torch.Tensor'> tensor(36894) 36894\n",
    "51072 ; loss 0.55 ; sentence/s 181 ; words/s 13316 ; accuracy train : 72.06\n",
    "results : epoch 8 ; mean accuracy train : 72.18\n",
    "\n",
    "VALIDATION : Epoch 8\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 8 ; mean accuracy valid :              69.71\n",
    "Shrinking lr by : 5. New lr = 8.947827339907103e-05\n",
    "\n",
    "TRAINING : Epoch 9\n",
    "Learning rate : 8.858349066508032e-05\n",
    "<class 'torch.Tensor'> tensor(9242) 9242\n",
    "12672 ; loss 0.54 ; sentence/s 181 ; words/s 13385 ; accuracy train : 72.2\n",
    "<class 'torch.Tensor'> tensor(18388) 18388\n",
    "25472 ; loss 0.54 ; sentence/s 178 ; words/s 13567 ; accuracy train : 71.83\n",
    "<class 'torch.Tensor'> tensor(27586) 27586\n",
    "38272 ; loss 0.54 ; sentence/s 181 ; words/s 13301 ; accuracy train : 71.84\n",
    "<class 'torch.Tensor'> tensor(36838) 36838\n",
    "51072 ; loss 0.54 ; sentence/s 183 ; words/s 13382 ; accuracy train : 71.95\n",
    "results : epoch 9 ; mean accuracy train : 72.03\n",
    "\n",
    "VALIDATION : Epoch 9\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 9 ; mean accuracy valid :              69.83\n",
    "Shrinking lr by : 5. New lr = 1.7716698133016062e-05\n",
    "\n",
    "TRAINING : Epoch 10\n",
    "Learning rate : 1.7539531151685903e-05\n",
    "<class 'torch.Tensor'> tensor(9265) 9265\n",
    "12672 ; loss 0.54 ; sentence/s 181 ; words/s 13567 ; accuracy train : 72.38\n",
    "<class 'torch.Tensor'> tensor(18519) 18519\n",
    "25472 ; loss 0.54 ; sentence/s 183 ; words/s 13264 ; accuracy train : 72.34\n",
    "<class 'torch.Tensor'> tensor(27786) 27786\n",
    "38272 ; loss 0.54 ; sentence/s 179 ; words/s 13558 ; accuracy train : 72.36\n",
    "<class 'torch.Tensor'> tensor(36979) 36979\n",
    "51072 ; loss 0.54 ; sentence/s 182 ; words/s 13262 ; accuracy train : 72.22\n",
    "results : epoch 10 ; mean accuracy train : 72.27\n",
    "\n",
    "VALIDATION : Epoch 10\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 10 ; mean accuracy valid :              69.34\n",
    "Shrinking lr by : 5. New lr = 3.507906230337181e-06\n",
    "saving state dict\n",
    "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type NLINet. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type InferSent. It won't be checked for correctness upon loading.\n",
    "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
    "done saving state dict\n",
    "\n",
    "TEST : Epoch 11\n",
    "calculating validation error\n",
    "\n",
    "VALIDATION : Epoch 1000000.0\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "finalgrep : accuracy valid : 69.34\n",
    "calculating test error\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "finalgrep : accuracy test : 69.22\n",
    "fin 3793.925459384918"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
