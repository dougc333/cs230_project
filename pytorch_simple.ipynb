{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch tensors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "seed=4\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "processing:('weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.3948,  0.3234, -0.5316],\n",
      "        [-0.5314, -0.5114,  0.5051],\n",
      "        [-0.0374,  0.4788,  0.3158],\n",
      "        [ 0.4603,  0.5318,  0.5515],\n",
      "        [-0.4121,  0.3356, -0.1010],\n",
      "        [ 0.2258, -0.0862,  0.2007],\n",
      "        [ 0.1558,  0.0313,  0.5004],\n",
      "        [ 0.1755, -0.1903,  0.2413],\n",
      "        [ 0.2875,  0.3691, -0.3227],\n",
      "        [-0.2263,  0.5773,  0.3577],\n",
      "        [-0.3565, -0.5711,  0.4456],\n",
      "        [ 0.4936,  0.4473,  0.4038]], requires_grad=True))\n",
      "processing:('weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.0454, -0.1491, -0.0678],\n",
      "        [-0.5677,  0.2599, -0.3704],\n",
      "        [-0.2456,  0.1175,  0.4153],\n",
      "        [ 0.2887, -0.2673,  0.4662],\n",
      "        [-0.3467,  0.3040,  0.3173],\n",
      "        [ 0.0727,  0.5667,  0.4245],\n",
      "        [ 0.0882,  0.0305, -0.4309],\n",
      "        [-0.3195,  0.4416, -0.0095],\n",
      "        [-0.2118, -0.5731, -0.3095],\n",
      "        [-0.2579, -0.3883,  0.0132],\n",
      "        [ 0.5683, -0.5317,  0.2970],\n",
      "        [-0.1425,  0.0019,  0.0334]], requires_grad=True))\n",
      "processing:('bias_ih_l0', Parameter containing:\n",
      "tensor([-0.2179,  0.3632,  0.1596, -0.5418, -0.5242, -0.2019,  0.1405, -0.0281,\n",
      "         0.2896,  0.4976,  0.3959,  0.2113], requires_grad=True))\n",
      "processing:('bias_hh_l0', Parameter containing:\n",
      "tensor([-0.2505, -0.0320, -0.1968, -0.5106, -0.0051,  0.0642, -0.3471,  0.2161,\n",
      "        -0.2343, -0.3538,  0.4961,  0.4631], requires_grad=True))\n",
      "processing:('weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.3688, -0.0504,  0.5282],\n",
      "        [-0.2411, -0.0720,  0.3088],\n",
      "        [ 0.4097, -0.5289, -0.2632],\n",
      "        [-0.0381, -0.4102, -0.5074],\n",
      "        [ 0.1452,  0.3174,  0.0096],\n",
      "        [-0.0420,  0.4713, -0.5023],\n",
      "        [-0.4476, -0.2141, -0.1769],\n",
      "        [ 0.5159,  0.5083,  0.2187],\n",
      "        [-0.5481,  0.3790, -0.0245],\n",
      "        [ 0.3675,  0.4668, -0.0616],\n",
      "        [-0.4933,  0.1818,  0.3456],\n",
      "        [ 0.1220,  0.3118,  0.3633]], requires_grad=True))\n",
      "processing:('weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[-0.2691,  0.0140,  0.0488],\n",
      "        [ 0.2209,  0.1105, -0.0942],\n",
      "        [-0.5319,  0.1574,  0.3647],\n",
      "        [ 0.3912, -0.1749, -0.3159],\n",
      "        [-0.4463, -0.4352, -0.4205],\n",
      "        [-0.2379, -0.3078, -0.5661],\n",
      "        [ 0.0391, -0.4239, -0.5032],\n",
      "        [-0.5412, -0.3780,  0.4904],\n",
      "        [-0.1536, -0.5710, -0.4212],\n",
      "        [ 0.5414, -0.0867,  0.5610],\n",
      "        [ 0.5071, -0.3025, -0.2713],\n",
      "        [ 0.2883,  0.2359, -0.3390]], requires_grad=True))\n",
      "processing:('bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([ 0.5401,  0.2775, -0.4029, -0.4520,  0.4928, -0.1908, -0.0050,  0.4162,\n",
      "        -0.0210, -0.1086,  0.2487, -0.0234], requires_grad=True))\n",
      "processing:('bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([-0.5495, -0.3341,  0.3404, -0.1506, -0.1410, -0.0353,  0.2053, -0.3590,\n",
      "         0.4447,  0.1768, -0.1101,  0.5237], requires_grad=True))\n",
      "processing:('weight_ih_l1', Parameter containing:\n",
      "tensor([[-0.4309, -0.5287, -0.3796,  0.2288,  0.0320,  0.5072],\n",
      "        [ 0.3133, -0.5397, -0.2306,  0.3493, -0.3158, -0.4969],\n",
      "        [ 0.0832,  0.4658,  0.5563, -0.4407, -0.2412, -0.4398],\n",
      "        [ 0.4615, -0.5387, -0.0255,  0.2383,  0.5442,  0.1994],\n",
      "        [ 0.1554, -0.4846,  0.2571,  0.0337,  0.3517,  0.1683],\n",
      "        [-0.0817, -0.1956, -0.5137,  0.5123, -0.0955, -0.0630],\n",
      "        [-0.4234, -0.4343,  0.4705, -0.1747, -0.0705,  0.2910],\n",
      "        [ 0.0358,  0.4237, -0.3234, -0.1432, -0.0858, -0.4235],\n",
      "        [-0.2346,  0.2080, -0.2511, -0.0190, -0.2062, -0.4417],\n",
      "        [ 0.3699,  0.2672,  0.1579, -0.5155,  0.4839,  0.5199],\n",
      "        [-0.0784, -0.4968, -0.2110,  0.4992,  0.0298,  0.1244],\n",
      "        [ 0.4605,  0.4627,  0.5144, -0.3336, -0.2156,  0.5653]],\n",
      "       requires_grad=True))\n",
      "processing:('weight_hh_l1', Parameter containing:\n",
      "tensor([[ 0.2937, -0.0559, -0.1136],\n",
      "        [-0.2505, -0.0379,  0.1587],\n",
      "        [ 0.0830, -0.3351,  0.0849],\n",
      "        [ 0.4612,  0.0417,  0.0332],\n",
      "        [-0.1250,  0.3898, -0.2983],\n",
      "        [-0.2784, -0.1126,  0.1454],\n",
      "        [-0.2154,  0.5678,  0.2169],\n",
      "        [-0.5283,  0.2746, -0.0951],\n",
      "        [-0.2056, -0.0992, -0.3688],\n",
      "        [-0.5258,  0.2574,  0.3062],\n",
      "        [ 0.2806, -0.3293, -0.0733],\n",
      "        [-0.2115, -0.1617, -0.1527]], requires_grad=True))\n",
      "processing:('bias_ih_l1', Parameter containing:\n",
      "tensor([-0.0285,  0.3770,  0.4694,  0.5605, -0.4122,  0.1262, -0.4136, -0.4180,\n",
      "        -0.0098,  0.5227,  0.3746, -0.3973], requires_grad=True))\n",
      "processing:('bias_hh_l1', Parameter containing:\n",
      "tensor([ 0.4286, -0.2348, -0.2245,  0.0281,  0.4668, -0.1737,  0.3693, -0.4085,\n",
      "         0.4735, -0.1564,  0.2942,  0.4042], requires_grad=True))\n",
      "processing:('weight_ih_l1_reverse', Parameter containing:\n",
      "tensor([[ 0.1360,  0.4040, -0.3107,  0.2632, -0.0661, -0.1930],\n",
      "        [ 0.3408, -0.3244, -0.5613, -0.1711,  0.0161, -0.4717],\n",
      "        [ 0.5592, -0.1991, -0.3169, -0.0754,  0.4491,  0.4310],\n",
      "        [-0.0117, -0.2233, -0.1072,  0.3644,  0.2674, -0.3427],\n",
      "        [ 0.4656,  0.0968,  0.1868,  0.2267,  0.1172,  0.2274],\n",
      "        [-0.4084,  0.4475,  0.4710,  0.1405, -0.2528, -0.4487],\n",
      "        [-0.4491,  0.2422, -0.5576, -0.3206,  0.3885, -0.1942],\n",
      "        [ 0.4855,  0.0452, -0.3985, -0.5211,  0.0862, -0.0009],\n",
      "        [ 0.2438, -0.1496, -0.3631, -0.2835,  0.4049, -0.4273],\n",
      "        [-0.4362,  0.1238,  0.3432, -0.2401, -0.2954,  0.5223],\n",
      "        [-0.4979, -0.2856, -0.2608,  0.4903,  0.3391,  0.3434],\n",
      "        [-0.1998,  0.4437,  0.4228,  0.0453, -0.2733, -0.5537]],\n",
      "       requires_grad=True))\n",
      "processing:('weight_hh_l1_reverse', Parameter containing:\n",
      "tensor([[-0.4983, -0.1395, -0.2211],\n",
      "        [-0.4695,  0.2817, -0.0330],\n",
      "        [ 0.4550, -0.4112,  0.3963],\n",
      "        [-0.0009,  0.0896,  0.4162],\n",
      "        [ 0.2200,  0.1748,  0.1692],\n",
      "        [-0.4958, -0.2429, -0.2509],\n",
      "        [-0.0204, -0.1115, -0.1912],\n",
      "        [ 0.1996,  0.1375,  0.2010],\n",
      "        [ 0.2092, -0.1659,  0.2028],\n",
      "        [-0.5556,  0.2112,  0.3899],\n",
      "        [ 0.1894,  0.0512, -0.5747],\n",
      "        [ 0.3303,  0.1969,  0.3171]], requires_grad=True))\n",
      "processing:('bias_ih_l1_reverse', Parameter containing:\n",
      "tensor([ 0.1352, -0.1513, -0.4722,  0.1850,  0.4611,  0.0187, -0.4094,  0.1193,\n",
      "        -0.2018, -0.0011,  0.3691,  0.0169], requires_grad=True))\n",
      "processing:('bias_hh_l1_reverse', Parameter containing:\n",
      "tensor([ 0.0858,  0.5051, -0.1335, -0.4952, -0.3508, -0.1960,  0.2976, -0.0476,\n",
      "         0.2764,  0.4927,  0.4487,  0.0909], requires_grad=True))\n",
      "----------------------\n",
      "tensor([[-0.1169,  0.1498,  0.1977],\n",
      "        [ 0.2515, -0.5998, -0.0835],\n",
      "        [ 0.1832, -0.0548,  0.2678],\n",
      "        [-0.5422, -0.0856,  0.3533],\n",
      "        [-0.0218, -0.0459, -0.6019],\n",
      "        [-0.4224,  0.1905, -0.4720],\n",
      "        [-0.0947,  0.3788, -0.3846],\n",
      "        [-0.2999, -0.9327,  0.4385],\n",
      "        [-0.0299, -0.2146, -0.0756],\n",
      "        [ 0.3873,  0.5593,  1.5127],\n",
      "        [-0.1896, -0.2953, -0.5361],\n",
      "        [-0.1976, -0.1503,  0.1543]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3,3,num_layers=2,bidirectional=True)\n",
    "#there are 4 parameters per LSTM cell. For a 2 layer deep LSTM cell. \n",
    "print(lstm.weight_ih_l0.data.is_cuda)\n",
    "print(lstm.weight_ih_l1.data.is_cuda)\n",
    "print(lstm.weight_hh_l0.data.is_cuda)\n",
    "print(lstm.weight_hh_l1.data.is_cuda)\n",
    "\n",
    "print(lstm.bias_ih_l0.data.is_cuda)\n",
    "print(lstm.bias_ih_l1.data.is_cuda)\n",
    "print(lstm.bias_hh_l0.data.is_cuda)\n",
    "print(lstm.bias_hh_l1.data.is_cuda)\n",
    "def init_lstm(lstm):\n",
    "    '''\n",
    "    input: torch lstm\n",
    "    output: weights xavier initialized, biases set to 0. \n",
    "    '''\n",
    "    for name, param in lstm.named_parameters():\n",
    "        print(f\"processing:{name,param}\")\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant(param, 0.0)\n",
    "        elif 'weight' in name:\n",
    "            nn.init.xavier_normal(param)\n",
    "init_lstm(lstm)\n",
    "#check lstm\n",
    "print(\"----------------------\")\n",
    "print(lstm.weight_hh_l1.data)\n",
    "#verify just this isn't 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:('weight_ih_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 3]))\n",
      "processing:('weight_hh_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('weight_ih_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 3]))\n",
      "processing:('weight_hh_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n"
     ]
    }
   ],
   "source": [
    "#what happens when there are more hidden units than input? the weight parameters are different\n",
    "lstm = nn.LSTM(3,10,num_layers=1,bidirectional=True)\n",
    "for name, param in lstm.named_parameters():\n",
    "        print(f\"processing:{name,type(param),param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{split}\\begin{array}{ll}\n",
    "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
    "h_t = o_t \\tanh(c_t)\n",
    "\\end{array}\\end{split}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:('weight_ih_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 3]))\n",
      "processing:('weight_hh_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l0', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('weight_ih_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 3]))\n",
      "processing:('weight_hh_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l0_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('weight_ih_l1', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 20]))\n",
      "processing:('weight_hh_l1', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l1', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l1', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('weight_ih_l1_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 20]))\n",
      "processing:('weight_hh_l1_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l1_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l1_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('weight_ih_l2', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 20]))\n",
      "processing:('weight_hh_l2', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l2', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l2', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('weight_ih_l2_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 20]))\n",
      "processing:('weight_hh_l2_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40, 10]))\n",
      "processing:('bias_ih_l2_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n",
      "processing:('bias_hh_l2_reverse', <class 'torch.nn.parameter.Parameter'>, torch.Size([40]))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3,10,num_layers=3,bidirectional=True)\n",
    "for name, param in lstm.named_parameters():\n",
    "        print(f\"processing:{name,type(param),param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>notes to myself. df.values() cant be shuffled in place</h6>\n",
    "   #foo = np.copy(df.values)\n",
    "    #np.random.shuffle(df.values)\n",
    "    #print(df.head()) #not shuffled\n",
    "    #np.random.shuffle(foo)\n",
    "    #print(foo[:4])\n",
    "    #indices = np.random.permutation(foo.shape[0])\n",
    "    #print(404290*.8, 404290*.2)\n",
    "    #trainidx,testidx = indices[:((int)(404290*.8))],indices[((int)(404290*.8)):]\n",
    "    #train,test=foo[trainidx,:],foo[testidx,:]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2  How can I increase the speed of my internet co...   \n",
      "3  Why am I mentally very lonely? How can I solve...   \n",
      "4  Which one dissolve in water quikly sugar, salt...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "2  How can Internet speed be increased by hacking...             0  \n",
      "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
      "4            Which fish would survive in salt water?             0  \n",
      "num rows dataframe:404290\n",
      "(404290, 3)\n",
      "X_train.shape:(242574, 2) X_train.shape:(242574, 1)\n",
      "X_test.shape:(80858, 2) y_test.shape:(80858, 1)\n",
      "X_valid.shape:(80858, 2) y_valid.shape:(80858, 1)\n",
      "[['How should I prepare to get a job as a programmer in a startup companies in India?'\n",
      "  'How do I get a job in startup companies in India?']\n",
      " ['The NPV rule says to go for the project that has a higher net present value. What happens if you have one project that has a lower present value but a higher discount rate and the other one has a higher present value but a lower discount rate? Which project would you choose?'\n",
      "  'What is adjusted net present value approach?']\n",
      " ['What if god was an engineer?' 'What is God like?']\n",
      " ['How can I be a better husband?' 'How can I be a better wife?']\n",
      " ['Where is the best home appliance service center in hyderabad?'\n",
      "  'Which one is the best home appliances service center in Hyderabad?']\n",
      " ['Game of Thrones Season 3: How did Baelish discover that Ros was spying for Varys?'\n",
      "  'Game Of Thrones Season 3 Episode 4 (And Now His Watch Has Ended): What was the nature of the magic (?) in the blue flame that Varys described in the awful story about his youth?']] [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "---------------------\n",
      "[[\"Why do many people call football 'soccer'?\"\n",
      "  'Why do renegade countries call football as \"soccer\"?']\n",
      " ['If Trump is elected U.S. president, how bad will the impending stock market crash be?'\n",
      "  'What will happen to stock market when Trump becomes a president?']\n",
      " ['What is a good video game?'\n",
      "  'Which were some of the best games you have played so far?']\n",
      " ['What does it mean to take pride in your work?'\n",
      "  \"What's classy if youâ€™re rich, but trashy if you're poor?\"]\n",
      " ['What is the best moderately-priced whiskey?'\n",
      "  'What is a good and reasonably-priced whiskey?']\n",
      " ['Where can I buy a tank of pure nitrogen gas?'\n",
      "  'What are the effects of bleach in gas tanks?']] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "---------------------\n",
      "[['What is the odds of getting black or red correct 21 times in a row?'\n",
      "  'Why do we have to cope with high school subjects when half of them dont even help you?']\n",
      " ['What is the difference between CrPC and IPC?'\n",
      "  \"What's the difference between IPC and CrPC?\"]\n",
      " ['What are the best ways to lose weight fast?'\n",
      "  'What is the fastest way to lose weight successfully?']\n",
      " ['Which is the worst movie in Bollywood?'\n",
      "  'Which are worst movies ever made in Bollywood?']\n",
      " ['Is time a human creation for the purpose of measurement or is time an actual physical property of the universe?'\n",
      "  'Can we travel back in time?']\n",
      " ['How do I learn machine learning?'\n",
      "  'How can I learn machine learning better?']] [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def save(X_train,X_valid,X_test,y_train,y_valid,y_test):\n",
    "    save_single_file(\"X_train\",X_train)\n",
    "    save_single_file(\"X_valid\",X_valid)\n",
    "    save_single_file(\"X_test\",X_test)\n",
    "    save_single_file(\"y_train\",y_train)\n",
    "    save_single_file(\"y_valid\",y_valid)\n",
    "    save_single_file(\"y_test\",y_test)\n",
    "    \n",
    "    \n",
    "def save_single_file(filename,data):\n",
    "    fh = open(filename+'.pkl', 'wb+')\n",
    "    pickle.dump(data, fh)\n",
    "    fh.close()\n",
    "    \n",
    "def load_single_file(filename):\n",
    "    fh = open(filename+'.pkl','rb')\n",
    "    data = pickle.load(fh)\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "def load_data():\n",
    "    X_train = load_single_file(\"X_train\")\n",
    "    X_valid = load_single_file(\"X_valid\")\n",
    "    X_test = load_single_file(\"X_test\")\n",
    "    y_train = load_single_file(\"y_train\")\n",
    "    y_valid = load_single_file(\"y_valid\")\n",
    "    y_test = load_single_file(\"y_test\")\n",
    "    return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "    \n",
    "def make_dataset(path):\n",
    "    '''\n",
    "    input: path: path where quora_duplicate.tsv\n",
    "    output: train, dev, valid tsv datasets\n",
    "    '''\n",
    "    input_file = 'quora_duplicate_questions.tsv'\n",
    "    df = pd.read_csv(os.path.join(path,input_file),sep='\\t')\n",
    "    df = df.drop([\"id\",\"qid1\",\"qid2\"],axis=1)\n",
    "    print(df.head())\n",
    "    print(f\"num rows dataframe:{len(df)}\")\n",
    "    print(df.values.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[['question1','question2']].values, df[['is_duplicate']].values, test_size=0.40, random_state=42)\n",
    "    X_test,X_valid,y_test,y_valid = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n",
    "    print(f\"X_train.shape:{X_train.shape} X_train.shape:{y_train.shape}\")\n",
    "    print(f\"X_test.shape:{X_test.shape} y_test.shape:{y_test.shape}\")\n",
    "    print(f\"X_valid.shape:{X_valid.shape} y_valid.shape:{y_valid.shape}\")\n",
    "    \n",
    "    print(X_train[:6],y_train[:6])\n",
    "    print('---------------------')\n",
    "    print(X_test[:6],y_test[:6])\n",
    "    print('---------------------')\n",
    "    print(X_valid[:6],y_valid[:6])\n",
    "    \n",
    "    return X_train,X_valid,X_test,y_train,y_valid,y_test\n",
    "    \n",
    "X_train,X_valid,X_test,y_train,y_valid,y_test = make_dataset('/home/dc/cs230_project')\n",
    "save(X_train,X_valid,X_test,y_train,y_valid,y_test)\n",
    "X_train,X_valid,X_test,y_train,y_valid,y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242574, 2) (80858, 2) (80858, 2) (242574, 1) (80858, 1) (80858, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,X_test,y_train,y_valid,y_test = load_data()\n",
    "print(X_train.shape,X_valid.shape,X_test.shape,y_train.shape,y_valid.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#torch.tensor copies numpy array into CPU memory\n",
    "a = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "print(type(a))\n",
    "print(a.view(-1))\n",
    "b=torch.tensor(np.array([[1,2],[3,4],[5,6]]))\n",
    "print(a.size(),b.size())\n",
    "print(b.view(-1)) #view flattens\n",
    "#.numpy() to get back numpy \n",
    "print(type(a.numpy()),a.numpy())\n",
    "#\n",
    "print(\"current torch device\",torch.cuda.current_device(),torch.cuda.is_available())\n",
    "c = torch.cuda.FloatTensor(1000, 1000).fill_(0)\n",
    "#images numpy is HxWxChannels, torch is channelsxHxW use transform.ToTensor\n",
    "#\n",
    "X1a = torch.rand((100,1))\n",
    "X1b = torch.rand((100,1))\n",
    "X2a = torch.rand(100,1)+.2\n",
    "X2b = torch.rand(100,1)+.2\n",
    "\n",
    "Y1 = torch.zeros([1,100],dtype=torch.int32)\n",
    "Y2 = torch.ones([1,100],dtype=torch.int32)\n",
    "\n",
    "X = torch.cat([X1a,X1b],dim=1)\n",
    "Y = torch.cat([X2a,X2b],dim=1)\n",
    "#print (X.size(),X.numpy())\n",
    "#print (Y.size(),type(Y.numpy()))\n",
    "plt.scatter(X[:,0],X[:,1],color='red')\n",
    "plt.scatter(Y[:,0],Y[:,1],color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format matrix for DL. 50 features(columns) in a row giving 25 data points and 2000 examples/rows\n",
    "X1 = torch.rand(1000,50)\n",
    "X2 = torch.rand(1000,50) + .2\n",
    "X = torch.cat([X1,X2],dim=0)\n",
    "Y1 = torch.zeros(1000,1)\n",
    "Y2 = torch.ones(1000,1)\n",
    "Y = torch.cat([Y1,Y2],dim=0)\n",
    "print(X.size(),Y1.size(),Y2.size(),Y.size())\n",
    "plt.scatter(X1[:,2],X1[:,3],color=\"red\")\n",
    "plt.scatter(X2[:,0],X2[:,1],color=\"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.size(),Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.input_dim = 50\n",
    "        super(NN,self).__init__()\n",
    "        self.linear1 = nn.Linear(self.input_dim, self.input_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.input_dim,100)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(100,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,X):\n",
    "        self.l1_linear = self.linear1(X)\n",
    "        self.l1_relu = self.relu1(self.l1_linear)\n",
    "        self.l2_linear = self.linear2(self.l1_relu)\n",
    "        self.l2_relu = self.relu2(self.l2_linear)\n",
    "        self.l3_linear=self.linear3(self.l2_relu)\n",
    "        self.l3_sigmoid = self.sigmoid(self.l3_linear)\n",
    "        return self.l3_sigmoid\n",
    "\n",
    "model  = NN()\n",
    "opt = optim.Adam(model.parameters(), lr = .001)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,opt,criterion):\n",
    "    model.train()\n",
    "    batch_size=50\n",
    "    losses=[]\n",
    "    for i in range(0,X.size(0),batch_size):\n",
    "        X_batch=Variable(X[i:i+batch_size,:])\n",
    "        Y_batch=Variable(Y[i:i+batch_size,:])\n",
    "        #print(X_batch.size(), Y_batch.size())\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(X_batch)\n",
    "        loss = criterion(y_hat,Y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(i,loss.data.numpy())\n",
    "        losses.append(loss.data.numpy())\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "losses=[]\n",
    "for x in range(epochs):\n",
    "    losses += train(model,opt,criterion)\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLogistic(nn.Module):\n",
    "    def __init__(self,input_size,num_classes):\n",
    "        super(SimpleLogistic,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "num_classes=10\n",
    "learning_rate = .001\n",
    "num_epochs = 5\n",
    "input_size = 28*28\n",
    "\n",
    "model = SimpleLogistic(input_size,num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for x in range(num_epochs):\n",
    "    #\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        #print(type(images),type(labels))\n",
    "        #print(images.numpy())\n",
    "        #print(labels.numpy())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(i%100==0):\n",
    "            print(f\"epoch:{x} i:{i} loss:{loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
