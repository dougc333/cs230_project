{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "togrep : ['-f', '/run/user/1000/jupyter/kernel-efa83de2-ae14-49ea-82a7-6edd1f5604a2.json']\n",
      "\n",
      "Namespace(LSTM_num_layers=1, batch_size=64, data_dir='/home/dc/cs230_project/dataset', decay=0.99, dpout_fc=0.0, dpout_model=0.0, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=512, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=2, n_enc_layers=1, n_epochs=40, nlipath='/home/dc/InferSent/dataset/SNLI', nonlinear_fc=1, optimizer='sgd,lr=0.1', outputdir='savedir/', outputmodelname='infersent.pickle', pool_type='max', seed=1234, weight_decay=0.0005, word_emb_dim=300)\n",
      "quora checkpoint len(train[s1]):242494,len(train[s2]):242494,          len(train[label]):242494\n",
      "============\n",
      "len(valid['s1']):80832, len(valid[s2]):80832,           len(valid['label']):80832\n",
      "============\n",
      "len(test['s1']):80832,len(test['s2']):80832,           len(test['label']):80832\n",
      "Found 88571(/232484) words with glove vectors\n",
      "Vocab size : 88571\n",
      "NLINet(\n",
      "  (encoder): InferSent(\n",
      "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.0)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Tanh()\n",
      "    (7): Dropout(p=0.0)\n",
      "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Tanh()\n",
      "    (11): Dropout(p=0.0)\n",
      "    (12): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:621: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.1\n",
      "<class 'torch.Tensor'> tensor(4270) 4270\n",
      "6336 ; loss 0.61 accuracy:66.72 ;\n",
      "<class 'torch.Tensor'> tensor(8832) 8832\n",
      "12736 ; loss 0.57 accuracy:69.0 ;\n",
      "<class 'torch.Tensor'> tensor(13464) 13464\n",
      "19136 ; loss 0.56 accuracy:70.12 ;\n",
      "<class 'torch.Tensor'> tensor(18120) 18120\n",
      "25536 ; loss 0.54 accuracy:70.78 ;\n",
      "<class 'torch.Tensor'> tensor(22855) 22855\n",
      "31936 ; loss 0.52 accuracy:71.42 ;\n",
      "<class 'torch.Tensor'> tensor(27557) 27557\n",
      "38336 ; loss 0.53 accuracy:71.76 ;\n",
      "<class 'torch.Tensor'> tensor(32323) 32323\n",
      "44736 ; loss 0.52 accuracy:72.15 ;\n",
      "<class 'torch.Tensor'> tensor(37084) 37084\n",
      "51136 ; loss 0.51 accuracy:72.43 ;\n",
      "<class 'torch.Tensor'> tensor(41882) 41882\n",
      "57536 ; loss 0.5 accuracy:72.71 ;\n",
      "<class 'torch.Tensor'> tensor(46738) 46738\n",
      "63936 ; loss 0.49 accuracy:73.03 ;\n",
      "<class 'torch.Tensor'> tensor(51628) 51628\n",
      "70336 ; loss 0.49 accuracy:73.34 ;\n",
      "<class 'torch.Tensor'> tensor(56481) 56481\n",
      "76736 ; loss 0.49 accuracy:73.54 ;\n",
      "<class 'torch.Tensor'> tensor(61408) 61408\n",
      "83136 ; loss 0.48 accuracy:73.81 ;\n",
      "<class 'torch.Tensor'> tensor(66297) 66297\n",
      "89536 ; loss 0.47 accuracy:73.99 ;\n",
      "<class 'torch.Tensor'> tensor(71255) 71255\n",
      "95936 ; loss 0.47 accuracy:74.22 ;\n",
      "<class 'torch.Tensor'> tensor(76120) 76120\n",
      "102336 ; loss 0.48 accuracy:74.34 ;\n",
      "<class 'torch.Tensor'> tensor(81068) 81068\n",
      "108736 ; loss 0.46 accuracy:74.51 ;\n",
      "<class 'torch.Tensor'> tensor(86028) 86028\n",
      "115136 ; loss 0.46 accuracy:74.68 ;\n",
      "<class 'torch.Tensor'> tensor(91033) 91033\n",
      "121536 ; loss 0.45 accuracy:74.86 ;\n",
      "<class 'torch.Tensor'> tensor(95988) 95988\n",
      "127936 ; loss 0.46 accuracy:74.99 ;\n",
      "<class 'torch.Tensor'> tensor(100920) 100920\n",
      "134336 ; loss 0.47 accuracy:75.09 ;\n",
      "<class 'torch.Tensor'> tensor(105908) 105908\n",
      "140736 ; loss 0.45 accuracy:75.22 ;\n",
      "<class 'torch.Tensor'> tensor(110879) 110879\n",
      "147136 ; loss 0.45 accuracy:75.33 ;\n",
      "<class 'torch.Tensor'> tensor(115889) 115889\n",
      "153536 ; loss 0.44 accuracy:75.45 ;\n",
      "<class 'torch.Tensor'> tensor(120880) 120880\n",
      "159936 ; loss 0.45 accuracy:75.55 ;\n",
      "<class 'torch.Tensor'> tensor(125826) 125826\n",
      "166336 ; loss 0.46 accuracy:75.62 ;\n",
      "<class 'torch.Tensor'> tensor(130845) 130845\n",
      "172736 ; loss 0.44 accuracy:75.72 ;\n",
      "<class 'torch.Tensor'> tensor(135885) 135885\n",
      "179136 ; loss 0.44 accuracy:75.83 ;\n",
      "<class 'torch.Tensor'> tensor(140908) 140908\n",
      "185536 ; loss 0.44 accuracy:75.92 ;\n",
      "<class 'torch.Tensor'> tensor(145935) 145935\n",
      "191936 ; loss 0.44 accuracy:76.01 ;\n",
      "<class 'torch.Tensor'> tensor(150949) 150949\n",
      "198336 ; loss 0.44 accuracy:76.08 ;\n",
      "<class 'torch.Tensor'> tensor(155959) 155959\n",
      "204736 ; loss 0.44 accuracy:76.15 ;\n",
      "<class 'torch.Tensor'> tensor(160955) 160955\n",
      "211136 ; loss 0.44 accuracy:76.21 ;\n",
      "<class 'torch.Tensor'> tensor(165971) 165971\n",
      "217536 ; loss 0.44 accuracy:76.27 ;\n",
      "<class 'torch.Tensor'> tensor(170962) 170962\n",
      "223936 ; loss 0.45 accuracy:76.32 ;\n",
      "<class 'torch.Tensor'> tensor(176007) 176007\n",
      "230336 ; loss 0.44 accuracy:76.39 ;\n",
      "<class 'torch.Tensor'> tensor(181030) 181030\n",
      "236736 ; loss 0.45 accuracy:76.45 ;\n",
      "results : epoch 1 ; mean accuracy train : 76.49\n",
      "\n",
      "VALIDATION : Epoch 1\n",
      "togrep : results : epoch 1 ; mean accuracy valid :              77.86\n",
      "saving model at epoch 1\n",
      "\n",
      "TRAINING : Epoch 2\n",
      "Learning rate : 0.099\n",
      "<class 'torch.Tensor'> tensor(5205) 5205\n",
      "6336 ; loss 0.4 accuracy:81.33 ;\n",
      "<class 'torch.Tensor'> tensor(10392) 10392\n",
      "12736 ; loss 0.4 accuracy:81.19 ;\n",
      "<class 'torch.Tensor'> tensor(15594) 15594\n",
      "19136 ; loss 0.4 accuracy:81.22 ;\n",
      "<class 'torch.Tensor'> tensor(20741) 20741\n",
      "25536 ; loss 0.41 accuracy:81.02 ;\n",
      "<class 'torch.Tensor'> tensor(25957) 25957\n",
      "31936 ; loss 0.4 accuracy:81.12 ;\n",
      "<class 'torch.Tensor'> tensor(31070) 31070\n",
      "38336 ; loss 0.41 accuracy:80.91 ;\n",
      "<class 'torch.Tensor'> tensor(36188) 36188\n",
      "44736 ; loss 0.41 accuracy:80.78 ;\n",
      "<class 'torch.Tensor'> tensor(41385) 41385\n",
      "51136 ; loss 0.4 accuracy:80.83 ;\n",
      "<class 'torch.Tensor'> tensor(46526) 46526\n",
      "57536 ; loss 0.41 accuracy:80.77 ;\n",
      "<class 'torch.Tensor'> tensor(51693) 51693\n",
      "63936 ; loss 0.4 accuracy:80.77 ;\n",
      "<class 'torch.Tensor'> tensor(56894) 56894\n",
      "70336 ; loss 0.41 accuracy:80.82 ;\n",
      "<class 'torch.Tensor'> tensor(62034) 62034\n",
      "76736 ; loss 0.41 accuracy:80.77 ;\n",
      "<class 'torch.Tensor'> tensor(67239) 67239\n",
      "83136 ; loss 0.4 accuracy:80.82 ;\n",
      "<class 'torch.Tensor'> tensor(72437) 72437\n",
      "89536 ; loss 0.4 accuracy:80.84 ;\n",
      "<class 'torch.Tensor'> tensor(77600) 77600\n",
      "95936 ; loss 0.4 accuracy:80.83 ;\n",
      "<class 'torch.Tensor'> tensor(82784) 82784\n",
      "102336 ; loss 0.4 accuracy:80.84 ;\n",
      "<class 'torch.Tensor'> tensor(88005) 88005\n",
      "108736 ; loss 0.39 accuracy:80.89 ;\n",
      "<class 'torch.Tensor'> tensor(93157) 93157\n",
      "115136 ; loss 0.4 accuracy:80.87 ;\n",
      "<class 'torch.Tensor'> tensor(98350) 98350\n",
      "121536 ; loss 0.4 accuracy:80.88 ;\n",
      "<class 'torch.Tensor'> tensor(103577) 103577\n",
      "127936 ; loss 0.39 accuracy:80.92 ;\n",
      "<class 'torch.Tensor'> tensor(108830) 108830\n",
      "134336 ; loss 0.38 accuracy:80.97 ;\n",
      "<class 'torch.Tensor'> tensor(114022) 114022\n",
      "140736 ; loss 0.4 accuracy:80.98 ;\n",
      "<class 'torch.Tensor'> tensor(119256) 119256\n",
      "147136 ; loss 0.39 accuracy:81.02 ;\n",
      "<class 'torch.Tensor'> tensor(124463) 124463\n",
      "153536 ; loss 0.39 accuracy:81.03 ;\n",
      "<class 'torch.Tensor'> tensor(129654) 129654\n",
      "159936 ; loss 0.39 accuracy:81.03 ;\n",
      "<class 'torch.Tensor'> tensor(134843) 134843\n",
      "166336 ; loss 0.4 accuracy:81.04 ;\n",
      "<class 'torch.Tensor'> tensor(140034) 140034\n",
      "172736 ; loss 0.39 accuracy:81.04 ;\n",
      "<class 'torch.Tensor'> tensor(145258) 145258\n",
      "179136 ; loss 0.4 accuracy:81.06 ;\n",
      "<class 'torch.Tensor'> tensor(150515) 150515\n",
      "185536 ; loss 0.38 accuracy:81.1 ;\n",
      "<class 'torch.Tensor'> tensor(155704) 155704\n",
      "191936 ; loss 0.4 accuracy:81.1 ;\n",
      "<class 'torch.Tensor'> tensor(160909) 160909\n",
      "198336 ; loss 0.39 accuracy:81.1 ;\n",
      "<class 'torch.Tensor'> tensor(166110) 166110\n",
      "204736 ; loss 0.4 accuracy:81.11 ;\n",
      "<class 'torch.Tensor'> tensor(171331) 171331\n",
      "211136 ; loss 0.39 accuracy:81.12 ;\n",
      "<class 'torch.Tensor'> tensor(176551) 176551\n",
      "217536 ; loss 0.39 accuracy:81.14 ;\n",
      "<class 'torch.Tensor'> tensor(181802) 181802\n",
      "223936 ; loss 0.39 accuracy:81.16 ;\n",
      "<class 'torch.Tensor'> tensor(187013) 187013\n",
      "230336 ; loss 0.39 accuracy:81.17 ;\n",
      "<class 'torch.Tensor'> tensor(192244) 192244\n",
      "236736 ; loss 0.39 accuracy:81.18 ;\n",
      "results : epoch 2 ; mean accuracy train : 81.2\n",
      "\n",
      "VALIDATION : Epoch 2\n",
      "togrep : results : epoch 2 ; mean accuracy valid :              80.57\n",
      "saving model at epoch 2\n",
      "\n",
      "TRAINING : Epoch 3\n",
      "Learning rate : 0.09801\n",
      "<class 'torch.Tensor'> tensor(5381) 5381\n",
      "6336 ; loss 0.35 accuracy:84.08 ;\n",
      "<class 'torch.Tensor'> tensor(10689) 10689\n",
      "12736 ; loss 0.36 accuracy:83.51 ;\n",
      "<class 'torch.Tensor'> tensor(16052) 16052\n",
      "19136 ; loss 0.35 accuracy:83.6 ;\n",
      "<class 'torch.Tensor'> tensor(21508) 21508\n",
      "25536 ; loss 0.33 accuracy:84.02 ;\n",
      "<class 'torch.Tensor'> tensor(26882) 26882\n",
      "31936 ; loss 0.34 accuracy:84.01 ;\n",
      "<class 'torch.Tensor'> tensor(32273) 32273\n",
      "38336 ; loss 0.35 accuracy:84.04 ;\n",
      "<class 'torch.Tensor'> tensor(37642) 37642\n",
      "44736 ; loss 0.36 accuracy:84.02 ;\n",
      "<class 'torch.Tensor'> tensor(42986) 42986\n",
      "51136 ; loss 0.36 accuracy:83.96 ;\n",
      "<class 'torch.Tensor'> tensor(48315) 48315\n",
      "57536 ; loss 0.36 accuracy:83.88 ;\n",
      "<class 'torch.Tensor'> tensor(53609) 53609\n",
      "63936 ; loss 0.37 accuracy:83.76 ;\n",
      "<class 'torch.Tensor'> tensor(58977) 58977\n",
      "70336 ; loss 0.35 accuracy:83.77 ;\n",
      "<class 'torch.Tensor'> tensor(64313) 64313\n",
      "76736 ; loss 0.36 accuracy:83.74 ;\n",
      "<class 'torch.Tensor'> tensor(69642) 69642\n",
      "83136 ; loss 0.36 accuracy:83.7 ;\n",
      "<class 'torch.Tensor'> tensor(75020) 75020\n",
      "89536 ; loss 0.35 accuracy:83.73 ;\n",
      "<class 'torch.Tensor'> tensor(80367) 80367\n",
      "95936 ; loss 0.36 accuracy:83.72 ;\n",
      "<class 'torch.Tensor'> tensor(85712) 85712\n",
      "102336 ; loss 0.36 accuracy:83.7 ;\n",
      "<class 'torch.Tensor'> tensor(91118) 91118\n",
      "108736 ; loss 0.34 accuracy:83.75 ;\n",
      "<class 'torch.Tensor'> tensor(96525) 96525\n",
      "115136 ; loss 0.35 accuracy:83.79 ;\n",
      "<class 'torch.Tensor'> tensor(101888) 101888\n",
      "121536 ; loss 0.36 accuracy:83.79 ;\n",
      "<class 'torch.Tensor'> tensor(107215) 107215\n",
      "127936 ; loss 0.36 accuracy:83.76 ;\n",
      "<class 'torch.Tensor'> tensor(112636) 112636\n",
      "134336 ; loss 0.34 accuracy:83.81 ;\n",
      "<class 'torch.Tensor'> tensor(118017) 118017\n",
      "140736 ; loss 0.35 accuracy:83.82 ;\n",
      "<class 'torch.Tensor'> tensor(123369) 123369\n",
      "147136 ; loss 0.36 accuracy:83.81 ;\n",
      "<class 'torch.Tensor'> tensor(128771) 128771\n",
      "153536 ; loss 0.35 accuracy:83.84 ;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor(134126) 134126\n",
      "159936 ; loss 0.35 accuracy:83.83 ;\n",
      "<class 'torch.Tensor'> tensor(139498) 139498\n",
      "166336 ; loss 0.35 accuracy:83.83 ;\n",
      "<class 'torch.Tensor'> tensor(144843) 144843\n",
      "172736 ; loss 0.36 accuracy:83.82 ;\n",
      "<class 'torch.Tensor'> tensor(150173) 150173\n",
      "179136 ; loss 0.35 accuracy:83.8 ;\n",
      "<class 'torch.Tensor'> tensor(155541) 155541\n",
      "185536 ; loss 0.35 accuracy:83.8 ;\n",
      "<class 'torch.Tensor'> tensor(160908) 160908\n",
      "191936 ; loss 0.36 accuracy:83.81 ;\n",
      "<class 'torch.Tensor'> tensor(166288) 166288\n",
      "198336 ; loss 0.35 accuracy:83.81 ;\n",
      "<class 'torch.Tensor'> tensor(171669) 171669\n",
      "204736 ; loss 0.35 accuracy:83.82 ;\n",
      "<class 'torch.Tensor'> tensor(177019) 177019\n",
      "211136 ; loss 0.35 accuracy:83.82 ;\n",
      "<class 'torch.Tensor'> tensor(182405) 182405\n",
      "217536 ; loss 0.35 accuracy:83.83 ;\n",
      "<class 'torch.Tensor'> tensor(187787) 187787\n",
      "223936 ; loss 0.35 accuracy:83.83 ;\n",
      "<class 'torch.Tensor'> tensor(193106) 193106\n",
      "230336 ; loss 0.36 accuracy:83.81 ;\n",
      "<class 'torch.Tensor'> tensor(198411) 198411\n",
      "236736 ; loss 0.37 accuracy:83.79 ;\n",
      "results : epoch 3 ; mean accuracy train : 83.78\n",
      "\n",
      "VALIDATION : Epoch 3\n",
      "togrep : results : epoch 3 ; mean accuracy valid :              80.24\n",
      "Shrinking lr by : 5. New lr = 0.019602\n",
      "\n",
      "TRAINING : Epoch 4\n",
      "Learning rate : 0.01940598\n",
      "<class 'torch.Tensor'> tensor(5565) 5565\n",
      "6336 ; loss 0.3 accuracy:86.95 ;\n",
      "<class 'torch.Tensor'> tensor(11146) 11146\n",
      "12736 ; loss 0.3 accuracy:87.08 ;\n",
      "<class 'torch.Tensor'> tensor(16713) 16713\n",
      "19136 ; loss 0.3 accuracy:87.05 ;\n",
      "<class 'torch.Tensor'> tensor(22303) 22303\n",
      "25536 ; loss 0.29 accuracy:87.12 ;\n",
      "<class 'torch.Tensor'> tensor(27868) 27868\n",
      "31936 ; loss 0.3 accuracy:87.09 ;\n",
      "<class 'torch.Tensor'> tensor(33504) 33504\n",
      "38336 ; loss 0.28 accuracy:87.25 ;\n",
      "<class 'torch.Tensor'> tensor(39103) 39103\n",
      "44736 ; loss 0.29 accuracy:87.28 ;\n",
      "<class 'torch.Tensor'> tensor(44728) 44728\n",
      "51136 ; loss 0.29 accuracy:87.36 ;\n",
      "<class 'torch.Tensor'> tensor(50377) 50377\n",
      "57536 ; loss 0.28 accuracy:87.46 ;\n",
      "<class 'torch.Tensor'> tensor(55975) 55975\n",
      "63936 ; loss 0.29 accuracy:87.46 ;\n",
      "<class 'torch.Tensor'> tensor(61632) 61632\n",
      "70336 ; loss 0.27 accuracy:87.55 ;\n",
      "<class 'torch.Tensor'> tensor(67222) 67222\n",
      "76736 ; loss 0.29 accuracy:87.53 ;\n",
      "<class 'torch.Tensor'> tensor(72828) 72828\n",
      "83136 ; loss 0.29 accuracy:87.53 ;\n",
      "<class 'torch.Tensor'> tensor(78454) 78454\n",
      "89536 ; loss 0.28 accuracy:87.56 ;\n",
      "<class 'torch.Tensor'> tensor(84083) 84083\n",
      "95936 ; loss 0.28 accuracy:87.59 ;\n",
      "<class 'torch.Tensor'> tensor(89743) 89743\n",
      "102336 ; loss 0.28 accuracy:87.64 ;\n",
      "<class 'torch.Tensor'> tensor(95355) 95355\n",
      "108736 ; loss 0.29 accuracy:87.64 ;\n",
      "<class 'torch.Tensor'> tensor(101000) 101000\n",
      "115136 ; loss 0.27 accuracy:87.67 ;\n",
      "<class 'torch.Tensor'> tensor(106679) 106679\n",
      "121536 ; loss 0.27 accuracy:87.73 ;\n",
      "<class 'torch.Tensor'> tensor(112304) 112304\n",
      "127936 ; loss 0.28 accuracy:87.74 ;\n",
      "<class 'torch.Tensor'> tensor(117974) 117974\n",
      "134336 ; loss 0.28 accuracy:87.78 ;\n",
      "<class 'torch.Tensor'> tensor(123643) 123643\n",
      "140736 ; loss 0.27 accuracy:87.81 ;\n",
      "<class 'torch.Tensor'> tensor(129276) 129276\n",
      "147136 ; loss 0.28 accuracy:87.82 ;\n",
      "<class 'torch.Tensor'> tensor(134890) 134890\n",
      "153536 ; loss 0.28 accuracy:87.82 ;\n",
      "<class 'torch.Tensor'> tensor(140547) 140547\n",
      "159936 ; loss 0.28 accuracy:87.84 ;\n",
      "<class 'torch.Tensor'> tensor(146194) 146194\n",
      "166336 ; loss 0.28 accuracy:87.86 ;\n",
      "<class 'torch.Tensor'> tensor(151809) 151809\n",
      "172736 ; loss 0.28 accuracy:87.85 ;\n",
      "<class 'torch.Tensor'> tensor(157462) 157462\n",
      "179136 ; loss 0.28 accuracy:87.87 ;\n",
      "<class 'torch.Tensor'> tensor(163069) 163069\n",
      "185536 ; loss 0.28 accuracy:87.86 ;\n",
      "<class 'torch.Tensor'> tensor(168711) 168711\n",
      "191936 ; loss 0.28 accuracy:87.87 ;\n",
      "<class 'torch.Tensor'> tensor(174309) 174309\n",
      "198336 ; loss 0.29 accuracy:87.86 ;\n",
      "<class 'torch.Tensor'> tensor(179956) 179956\n",
      "204736 ; loss 0.27 accuracy:87.87 ;\n",
      "<class 'torch.Tensor'> tensor(185545) 185545\n",
      "211136 ; loss 0.29 accuracy:87.85 ;\n",
      "<class 'torch.Tensor'> tensor(191186) 191186\n",
      "217536 ; loss 0.28 accuracy:87.86 ;\n",
      "<class 'torch.Tensor'> tensor(196815) 196815\n",
      "223936 ; loss 0.27 accuracy:87.86 ;\n",
      "<class 'torch.Tensor'> tensor(202473) 202473\n",
      "230336 ; loss 0.27 accuracy:87.88 ;\n",
      "<class 'torch.Tensor'> tensor(208111) 208111\n",
      "236736 ; loss 0.28 accuracy:87.88 ;\n",
      "results : epoch 4 ; mean accuracy train : 87.91\n",
      "\n",
      "VALIDATION : Epoch 4\n",
      "togrep : results : epoch 4 ; mean accuracy valid :              82.26\n",
      "saving model at epoch 4\n",
      "\n",
      "TRAINING : Epoch 5\n",
      "Learning rate : 0.0192119202\n",
      "<class 'torch.Tensor'> tensor(5651) 5651\n",
      "6336 ; loss 0.28 accuracy:88.3 ;\n",
      "<class 'torch.Tensor'> tensor(11401) 11401\n",
      "12736 ; loss 0.25 accuracy:89.07 ;\n",
      "<class 'torch.Tensor'> tensor(17089) 17089\n",
      "19136 ; loss 0.26 accuracy:89.01 ;\n",
      "<class 'torch.Tensor'> tensor(22731) 22731\n",
      "25536 ; loss 0.27 accuracy:88.79 ;\n",
      "<class 'torch.Tensor'> tensor(28461) 28461\n",
      "31936 ; loss 0.25 accuracy:88.94 ;\n",
      "<class 'torch.Tensor'> tensor(34140) 34140\n",
      "38336 ; loss 0.27 accuracy:88.91 ;\n",
      "<class 'torch.Tensor'> tensor(39873) 39873\n",
      "44736 ; loss 0.25 accuracy:89.0 ;\n",
      "<class 'torch.Tensor'> tensor(45565) 45565\n",
      "51136 ; loss 0.26 accuracy:88.99 ;\n",
      "<class 'torch.Tensor'> tensor(51280) 51280\n",
      "57536 ; loss 0.26 accuracy:89.03 ;\n",
      "<class 'torch.Tensor'> tensor(57013) 57013\n",
      "63936 ; loss 0.25 accuracy:89.08 ;\n",
      "<class 'torch.Tensor'> tensor(62712) 62712\n",
      "70336 ; loss 0.26 accuracy:89.08 ;\n",
      "<class 'torch.Tensor'> tensor(68439) 68439\n",
      "76736 ; loss 0.25 accuracy:89.11 ;\n",
      "<class 'torch.Tensor'> tensor(74153) 74153\n",
      "83136 ; loss 0.25 accuracy:89.13 ;\n",
      "<class 'torch.Tensor'> tensor(79899) 79899\n",
      "89536 ; loss 0.25 accuracy:89.17 ;\n",
      "<class 'torch.Tensor'> tensor(85618) 85618\n",
      "95936 ; loss 0.26 accuracy:89.19 ;\n",
      "<class 'torch.Tensor'> tensor(91347) 91347\n",
      "102336 ; loss 0.25 accuracy:89.21 ;\n",
      "<class 'torch.Tensor'> tensor(97022) 97022\n",
      "108736 ; loss 0.27 accuracy:89.17 ;\n",
      "<class 'torch.Tensor'> tensor(102735) 102735\n",
      "115136 ; loss 0.25 accuracy:89.18 ;\n",
      "<class 'torch.Tensor'> tensor(108447) 108447\n",
      "121536 ; loss 0.26 accuracy:89.18 ;\n",
      "<class 'torch.Tensor'> tensor(114149) 114149\n",
      "127936 ; loss 0.26 accuracy:89.18 ;\n",
      "<class 'torch.Tensor'> tensor(119828) 119828\n",
      "134336 ; loss 0.26 accuracy:89.16 ;\n",
      "<class 'torch.Tensor'> tensor(125532) 125532\n",
      "140736 ; loss 0.26 accuracy:89.16 ;\n",
      "<class 'torch.Tensor'> tensor(131249) 131249\n",
      "147136 ; loss 0.26 accuracy:89.16 ;\n",
      "<class 'torch.Tensor'> tensor(136943) 136943\n",
      "153536 ; loss 0.26 accuracy:89.16 ;\n",
      "<class 'torch.Tensor'> tensor(142645) 142645\n",
      "159936 ; loss 0.26 accuracy:89.15 ;\n",
      "<class 'torch.Tensor'> tensor(148285) 148285\n",
      "166336 ; loss 0.27 accuracy:89.11 ;\n",
      "<class 'torch.Tensor'> tensor(153946) 153946\n",
      "172736 ; loss 0.27 accuracy:89.09 ;\n",
      "<class 'torch.Tensor'> tensor(159679) 159679\n",
      "179136 ; loss 0.26 accuracy:89.11 ;\n",
      "<class 'torch.Tensor'> tensor(165368) 165368\n",
      "185536 ; loss 0.26 accuracy:89.1 ;\n",
      "<class 'torch.Tensor'> tensor(171071) 171071\n",
      "191936 ; loss 0.26 accuracy:89.1 ;\n",
      "<class 'torch.Tensor'> tensor(176791) 176791\n",
      "198336 ; loss 0.25 accuracy:89.11 ;\n",
      "<class 'torch.Tensor'> tensor(182464) 182464\n",
      "204736 ; loss 0.26 accuracy:89.09 ;\n",
      "<class 'torch.Tensor'> tensor(188144) 188144\n",
      "211136 ; loss 0.27 accuracy:89.08 ;\n",
      "<class 'torch.Tensor'> tensor(193895) 193895\n",
      "217536 ; loss 0.26 accuracy:89.11 ;\n",
      "<class 'torch.Tensor'> tensor(199598) 199598\n",
      "223936 ; loss 0.26 accuracy:89.11 ;\n",
      "<class 'torch.Tensor'> tensor(205264) 205264\n",
      "230336 ; loss 0.27 accuracy:89.09 ;\n",
      "<class 'torch.Tensor'> tensor(210979) 210979\n",
      "236736 ; loss 0.26 accuracy:89.1 ;\n",
      "results : epoch 5 ; mean accuracy train : 89.09\n",
      "\n",
      "VALIDATION : Epoch 5\n",
      "togrep : results : epoch 5 ; mean accuracy valid :              82.2\n",
      "Shrinking lr by : 5. New lr = 0.0038423840399999997\n",
      "\n",
      "TRAINING : Epoch 6\n",
      "Learning rate : 0.0038039601995999996\n",
      "<class 'torch.Tensor'> tensor(5804) 5804\n",
      "6336 ; loss 0.24 accuracy:90.69 ;\n",
      "<class 'torch.Tensor'> tensor(11583) 11583\n",
      "12736 ; loss 0.24 accuracy:90.49 ;\n",
      "<class 'torch.Tensor'> tensor(17402) 17402\n",
      "19136 ; loss 0.23 accuracy:90.64 ;\n",
      "<class 'torch.Tensor'> tensor(23214) 23214\n",
      "25536 ; loss 0.23 accuracy:90.68 ;\n",
      "<class 'torch.Tensor'> tensor(28999) 28999\n",
      "31936 ; loss 0.23 accuracy:90.62 ;\n",
      "<class 'torch.Tensor'> tensor(34781) 34781\n",
      "38336 ; loss 0.24 accuracy:90.58 ;\n",
      "<class 'torch.Tensor'> tensor(40617) 40617\n",
      "44736 ; loss 0.23 accuracy:90.66 ;\n",
      "<class 'torch.Tensor'> tensor(46382) 46382\n",
      "51136 ; loss 0.24 accuracy:90.59 ;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor(52159) 52159\n",
      "57536 ; loss 0.24 accuracy:90.55 ;\n",
      "<class 'torch.Tensor'> tensor(57957) 57957\n",
      "63936 ; loss 0.23 accuracy:90.56 ;\n",
      "<class 'torch.Tensor'> tensor(63749) 63749\n",
      "70336 ; loss 0.23 accuracy:90.55 ;\n",
      "<class 'torch.Tensor'> tensor(69556) 69556\n",
      "76736 ; loss 0.23 accuracy:90.57 ;\n",
      "<class 'torch.Tensor'> tensor(75362) 75362\n",
      "83136 ; loss 0.24 accuracy:90.58 ;\n",
      "<class 'torch.Tensor'> tensor(81207) 81207\n",
      "89536 ; loss 0.23 accuracy:90.63 ;\n",
      "<class 'torch.Tensor'> tensor(87048) 87048\n",
      "95936 ; loss 0.22 accuracy:90.67 ;\n",
      "<class 'torch.Tensor'> tensor(92803) 92803\n",
      "102336 ; loss 0.24 accuracy:90.63 ;\n",
      "<class 'torch.Tensor'> tensor(98608) 98608\n",
      "108736 ; loss 0.23 accuracy:90.63 ;\n",
      "<class 'torch.Tensor'> tensor(104410) 104410\n",
      "115136 ; loss 0.23 accuracy:90.63 ;\n",
      "<class 'torch.Tensor'> tensor(110246) 110246\n",
      "121536 ; loss 0.23 accuracy:90.66 ;\n",
      "<class 'torch.Tensor'> tensor(116102) 116102\n",
      "127936 ; loss 0.22 accuracy:90.7 ;\n",
      "<class 'torch.Tensor'> tensor(121917) 121917\n",
      "134336 ; loss 0.22 accuracy:90.71 ;\n",
      "<class 'torch.Tensor'> tensor(127717) 127717\n",
      "140736 ; loss 0.23 accuracy:90.71 ;\n",
      "<class 'torch.Tensor'> tensor(133518) 133518\n",
      "147136 ; loss 0.23 accuracy:90.71 ;\n",
      "<class 'torch.Tensor'> tensor(139307) 139307\n",
      "153536 ; loss 0.24 accuracy:90.69 ;\n",
      "<class 'torch.Tensor'> tensor(145126) 145126\n",
      "159936 ; loss 0.23 accuracy:90.7 ;\n",
      "<class 'torch.Tensor'> tensor(150897) 150897\n",
      "166336 ; loss 0.24 accuracy:90.68 ;\n",
      "<class 'torch.Tensor'> tensor(156682) 156682\n",
      "172736 ; loss 0.24 accuracy:90.67 ;\n",
      "<class 'torch.Tensor'> tensor(162516) 162516\n",
      "179136 ; loss 0.23 accuracy:90.69 ;\n",
      "<class 'torch.Tensor'> tensor(168296) 168296\n",
      "185536 ; loss 0.23 accuracy:90.68 ;\n",
      "<class 'torch.Tensor'> tensor(174059) 174059\n",
      "191936 ; loss 0.24 accuracy:90.66 ;\n",
      "<class 'torch.Tensor'> tensor(179868) 179868\n",
      "198336 ; loss 0.23 accuracy:90.66 ;\n",
      "<class 'torch.Tensor'> tensor(185675) 185675\n",
      "204736 ; loss 0.23 accuracy:90.66 ;\n",
      "<class 'torch.Tensor'> tensor(191467) 191467\n",
      "211136 ; loss 0.24 accuracy:90.66 ;\n",
      "<class 'torch.Tensor'> tensor(197286) 197286\n",
      "217536 ; loss 0.22 accuracy:90.66 ;\n",
      "<class 'torch.Tensor'> tensor(203061) 203061\n",
      "223936 ; loss 0.24 accuracy:90.65 ;\n",
      "<class 'torch.Tensor'> tensor(208837) 208837\n",
      "230336 ; loss 0.24 accuracy:90.64 ;\n",
      "<class 'torch.Tensor'> tensor(214661) 214661\n",
      "236736 ; loss 0.23 accuracy:90.65 ;\n",
      "results : epoch 6 ; mean accuracy train : 90.64\n",
      "\n",
      "VALIDATION : Epoch 6\n",
      "togrep : results : epoch 6 ; mean accuracy valid :              82.11\n",
      "Shrinking lr by : 5. New lr = 0.00076079203992\n",
      "\n",
      "TRAINING : Epoch 7\n",
      "Learning rate : 0.0007531841195208\n",
      "<class 'torch.Tensor'> tensor(5821) 5821\n",
      "6336 ; loss 0.22 accuracy:90.95 ;\n",
      "<class 'torch.Tensor'> tensor(11660) 11660\n",
      "12736 ; loss 0.22 accuracy:91.09 ;\n",
      "<class 'torch.Tensor'> tensor(17496) 17496\n",
      "19136 ; loss 0.22 accuracy:91.12 ;\n",
      "<class 'torch.Tensor'> tensor(23293) 23293\n",
      "25536 ; loss 0.23 accuracy:90.99 ;\n",
      "<class 'torch.Tensor'> tensor(29088) 29088\n",
      "31936 ; loss 0.23 accuracy:90.9 ;\n",
      "<class 'torch.Tensor'> tensor(34939) 34939\n",
      "38336 ; loss 0.23 accuracy:90.99 ;\n",
      "<class 'torch.Tensor'> tensor(40800) 40800\n",
      "44736 ; loss 0.22 accuracy:91.07 ;\n",
      "<class 'torch.Tensor'> tensor(46613) 46613\n",
      "51136 ; loss 0.23 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(52439) 52439\n",
      "57536 ; loss 0.23 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(58264) 58264\n",
      "63936 ; loss 0.23 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(64073) 64073\n",
      "70336 ; loss 0.23 accuracy:91.01 ;\n",
      "<class 'torch.Tensor'> tensor(69909) 69909\n",
      "76736 ; loss 0.22 accuracy:91.03 ;\n",
      "<class 'torch.Tensor'> tensor(75746) 75746\n",
      "83136 ; loss 0.22 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(81557) 81557\n",
      "89536 ; loss 0.23 accuracy:91.02 ;\n",
      "<class 'torch.Tensor'> tensor(87389) 87389\n",
      "95936 ; loss 0.22 accuracy:91.03 ;\n",
      "<class 'torch.Tensor'> tensor(93230) 93230\n",
      "102336 ; loss 0.22 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(99036) 99036\n",
      "108736 ; loss 0.23 accuracy:91.03 ;\n",
      "<class 'torch.Tensor'> tensor(104853) 104853\n",
      "115136 ; loss 0.22 accuracy:91.02 ;\n",
      "<class 'torch.Tensor'> tensor(110682) 110682\n",
      "121536 ; loss 0.23 accuracy:91.02 ;\n",
      "<class 'torch.Tensor'> tensor(116548) 116548\n",
      "127936 ; loss 0.21 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(122364) 122364\n",
      "134336 ; loss 0.22 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(128184) 128184\n",
      "140736 ; loss 0.22 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(133998) 133998\n",
      "147136 ; loss 0.23 accuracy:91.03 ;\n",
      "<class 'torch.Tensor'> tensor(139816) 139816\n",
      "153536 ; loss 0.23 accuracy:91.03 ;\n",
      "<class 'torch.Tensor'> tensor(145637) 145637\n",
      "159936 ; loss 0.23 accuracy:91.02 ;\n",
      "<class 'torch.Tensor'> tensor(151504) 151504\n",
      "166336 ; loss 0.22 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(157352) 157352\n",
      "172736 ; loss 0.22 accuracy:91.06 ;\n",
      "<class 'torch.Tensor'> tensor(163139) 163139\n",
      "179136 ; loss 0.24 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(168971) 168971\n",
      "185536 ; loss 0.23 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(174802) 174802\n",
      "191936 ; loss 0.22 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(180631) 180631\n",
      "198336 ; loss 0.23 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(186459) 186459\n",
      "204736 ; loss 0.22 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(192289) 192289\n",
      "211136 ; loss 0.22 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(198129) 198129\n",
      "217536 ; loss 0.22 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(203959) 203959\n",
      "223936 ; loss 0.22 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(209759) 209759\n",
      "230336 ; loss 0.23 accuracy:91.04 ;\n",
      "<class 'torch.Tensor'> tensor(215568) 215568\n",
      "236736 ; loss 0.23 accuracy:91.03 ;\n",
      "results : epoch 7 ; mean accuracy train : 91.05\n",
      "\n",
      "VALIDATION : Epoch 7\n",
      "togrep : results : epoch 7 ; mean accuracy valid :              82.31\n",
      "saving model at epoch 7\n",
      "\n",
      "TRAINING : Epoch 8\n",
      "Learning rate : 0.000745652278325592\n",
      "<class 'torch.Tensor'> tensor(5860) 5860\n",
      "6336 ; loss 0.22 accuracy:91.56 ;\n",
      "<class 'torch.Tensor'> tensor(11741) 11741\n",
      "12736 ; loss 0.22 accuracy:91.73 ;\n",
      "<class 'torch.Tensor'> tensor(17581) 17581\n",
      "19136 ; loss 0.22 accuracy:91.57 ;\n",
      "<class 'torch.Tensor'> tensor(23407) 23407\n",
      "25536 ; loss 0.22 accuracy:91.43 ;\n",
      "<class 'torch.Tensor'> tensor(29264) 29264\n",
      "31936 ; loss 0.22 accuracy:91.45 ;\n",
      "<class 'torch.Tensor'> tensor(35118) 35118\n",
      "38336 ; loss 0.22 accuracy:91.45 ;\n",
      "<class 'torch.Tensor'> tensor(40985) 40985\n",
      "44736 ; loss 0.22 accuracy:91.48 ;\n",
      "<class 'torch.Tensor'> tensor(46830) 46830\n",
      "51136 ; loss 0.22 accuracy:91.46 ;\n",
      "<class 'torch.Tensor'> tensor(52680) 52680\n",
      "57536 ; loss 0.22 accuracy:91.46 ;\n",
      "<class 'torch.Tensor'> tensor(58487) 58487\n",
      "63936 ; loss 0.23 accuracy:91.39 ;\n",
      "<class 'torch.Tensor'> tensor(64321) 64321\n",
      "70336 ; loss 0.22 accuracy:91.37 ;\n",
      "<class 'torch.Tensor'> tensor(70170) 70170\n",
      "76736 ; loss 0.22 accuracy:91.37 ;\n",
      "<class 'torch.Tensor'> tensor(75982) 75982\n",
      "83136 ; loss 0.23 accuracy:91.32 ;\n",
      "<class 'torch.Tensor'> tensor(81835) 81835\n",
      "89536 ; loss 0.22 accuracy:91.33 ;\n",
      "<class 'torch.Tensor'> tensor(87661) 87661\n",
      "95936 ; loss 0.22 accuracy:91.31 ;\n",
      "<class 'torch.Tensor'> tensor(93497) 93497\n",
      "102336 ; loss 0.22 accuracy:91.31 ;\n",
      "<class 'torch.Tensor'> tensor(99333) 99333\n",
      "108736 ; loss 0.22 accuracy:91.3 ;\n",
      "<class 'torch.Tensor'> tensor(105126) 105126\n",
      "115136 ; loss 0.23 accuracy:91.26 ;\n",
      "<class 'torch.Tensor'> tensor(110955) 110955\n",
      "121536 ; loss 0.23 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(116807) 116807\n",
      "127936 ; loss 0.22 accuracy:91.26 ;\n",
      "<class 'torch.Tensor'> tensor(122633) 122633\n",
      "134336 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(128468) 128468\n",
      "140736 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(134287) 134287\n",
      "147136 ; loss 0.23 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(140137) 140137\n",
      "153536 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(145974) 145974\n",
      "159936 ; loss 0.23 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(151796) 151796\n",
      "166336 ; loss 0.22 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(157613) 157613\n",
      "172736 ; loss 0.23 accuracy:91.21 ;\n",
      "<class 'torch.Tensor'> tensor(163417) 163417\n",
      "179136 ; loss 0.23 accuracy:91.19 ;\n",
      "<class 'torch.Tensor'> tensor(169234) 169234\n",
      "185536 ; loss 0.23 accuracy:91.18 ;\n",
      "<class 'torch.Tensor'> tensor(175021) 175021\n",
      "191936 ; loss 0.24 accuracy:91.16 ;\n",
      "<class 'torch.Tensor'> tensor(180838) 180838\n",
      "198336 ; loss 0.23 accuracy:91.15 ;\n",
      "<class 'torch.Tensor'> tensor(186686) 186686\n",
      "204736 ; loss 0.22 accuracy:91.16 ;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor(192520) 192520\n",
      "211136 ; loss 0.22 accuracy:91.16 ;\n",
      "<class 'torch.Tensor'> tensor(198344) 198344\n",
      "217536 ; loss 0.23 accuracy:91.15 ;\n",
      "<class 'torch.Tensor'> tensor(204164) 204164\n",
      "223936 ; loss 0.23 accuracy:91.14 ;\n",
      "<class 'torch.Tensor'> tensor(209969) 209969\n",
      "230336 ; loss 0.23 accuracy:91.13 ;\n",
      "<class 'torch.Tensor'> tensor(215817) 215817\n",
      "236736 ; loss 0.22 accuracy:91.14 ;\n",
      "results : epoch 8 ; mean accuracy train : 91.14\n",
      "\n",
      "VALIDATION : Epoch 8\n",
      "togrep : results : epoch 8 ; mean accuracy valid :              82.17\n",
      "Shrinking lr by : 5. New lr = 0.0001491304556651184\n",
      "\n",
      "TRAINING : Epoch 9\n",
      "Learning rate : 0.0001476391511084672\n",
      "<class 'torch.Tensor'> tensor(5815) 5815\n",
      "6336 ; loss 0.23 accuracy:90.86 ;\n",
      "<class 'torch.Tensor'> tensor(11643) 11643\n",
      "12736 ; loss 0.22 accuracy:90.96 ;\n",
      "<class 'torch.Tensor'> tensor(17494) 17494\n",
      "19136 ; loss 0.22 accuracy:91.11 ;\n",
      "<class 'torch.Tensor'> tensor(23295) 23295\n",
      "25536 ; loss 0.23 accuracy:91.0 ;\n",
      "<class 'torch.Tensor'> tensor(29129) 29129\n",
      "31936 ; loss 0.22 accuracy:91.03 ;\n",
      "<class 'torch.Tensor'> tensor(35004) 35004\n",
      "38336 ; loss 0.21 accuracy:91.16 ;\n",
      "<class 'torch.Tensor'> tensor(40873) 40873\n",
      "44736 ; loss 0.22 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(46677) 46677\n",
      "51136 ; loss 0.22 accuracy:91.17 ;\n",
      "<class 'torch.Tensor'> tensor(52442) 52442\n",
      "57536 ; loss 0.24 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(58292) 58292\n",
      "63936 ; loss 0.22 accuracy:91.08 ;\n",
      "<class 'torch.Tensor'> tensor(64101) 64101\n",
      "70336 ; loss 0.23 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(69964) 69964\n",
      "76736 ; loss 0.22 accuracy:91.1 ;\n",
      "<class 'torch.Tensor'> tensor(75810) 75810\n",
      "83136 ; loss 0.22 accuracy:91.12 ;\n",
      "<class 'torch.Tensor'> tensor(81634) 81634\n",
      "89536 ; loss 0.22 accuracy:91.11 ;\n",
      "<class 'torch.Tensor'> tensor(87441) 87441\n",
      "95936 ; loss 0.22 accuracy:91.08 ;\n",
      "<class 'torch.Tensor'> tensor(93233) 93233\n",
      "102336 ; loss 0.23 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(99095) 99095\n",
      "108736 ; loss 0.22 accuracy:91.08 ;\n",
      "<class 'torch.Tensor'> tensor(104912) 104912\n",
      "115136 ; loss 0.22 accuracy:91.07 ;\n",
      "<class 'torch.Tensor'> tensor(110762) 110762\n",
      "121536 ; loss 0.22 accuracy:91.09 ;\n",
      "<class 'torch.Tensor'> tensor(116578) 116578\n",
      "127936 ; loss 0.23 accuracy:91.08 ;\n",
      "<class 'torch.Tensor'> tensor(122388) 122388\n",
      "134336 ; loss 0.23 accuracy:91.06 ;\n",
      "<class 'torch.Tensor'> tensor(128215) 128215\n",
      "140736 ; loss 0.23 accuracy:91.06 ;\n",
      "<class 'torch.Tensor'> tensor(134062) 134062\n",
      "147136 ; loss 0.22 accuracy:91.07 ;\n",
      "<class 'torch.Tensor'> tensor(139861) 139861\n",
      "153536 ; loss 0.23 accuracy:91.06 ;\n",
      "<class 'torch.Tensor'> tensor(145684) 145684\n",
      "159936 ; loss 0.23 accuracy:91.05 ;\n",
      "<class 'torch.Tensor'> tensor(151557) 151557\n",
      "166336 ; loss 0.21 accuracy:91.08 ;\n",
      "<class 'torch.Tensor'> tensor(157407) 157407\n",
      "172736 ; loss 0.22 accuracy:91.09 ;\n",
      "<class 'torch.Tensor'> tensor(163251) 163251\n",
      "179136 ; loss 0.22 accuracy:91.1 ;\n",
      "<class 'torch.Tensor'> tensor(169095) 169095\n",
      "185536 ; loss 0.23 accuracy:91.11 ;\n",
      "<class 'torch.Tensor'> tensor(174935) 174935\n",
      "191936 ; loss 0.22 accuracy:91.11 ;\n",
      "<class 'torch.Tensor'> tensor(180760) 180760\n",
      "198336 ; loss 0.22 accuracy:91.11 ;\n",
      "<class 'torch.Tensor'> tensor(186612) 186612\n",
      "204736 ; loss 0.22 accuracy:91.12 ;\n",
      "<class 'torch.Tensor'> tensor(192479) 192479\n",
      "211136 ; loss 0.22 accuracy:91.14 ;\n",
      "<class 'torch.Tensor'> tensor(198310) 198310\n",
      "217536 ; loss 0.22 accuracy:91.14 ;\n",
      "<class 'torch.Tensor'> tensor(204179) 204179\n",
      "223936 ; loss 0.22 accuracy:91.15 ;\n",
      "<class 'torch.Tensor'> tensor(210002) 210002\n",
      "230336 ; loss 0.22 accuracy:91.15 ;\n",
      "<class 'torch.Tensor'> tensor(215870) 215870\n",
      "236736 ; loss 0.22 accuracy:91.16 ;\n",
      "results : epoch 9 ; mean accuracy train : 91.15\n",
      "\n",
      "VALIDATION : Epoch 9\n",
      "togrep : results : epoch 9 ; mean accuracy valid :              82.33\n",
      "saving model at epoch 9\n",
      "\n",
      "TRAINING : Epoch 10\n",
      "Learning rate : 0.00014616275959738253\n",
      "<class 'torch.Tensor'> tensor(5855) 5855\n",
      "6336 ; loss 0.22 accuracy:91.48 ;\n",
      "<class 'torch.Tensor'> tensor(11682) 11682\n",
      "12736 ; loss 0.22 accuracy:91.27 ;\n",
      "<class 'torch.Tensor'> tensor(17464) 17464\n",
      "19136 ; loss 0.23 accuracy:90.96 ;\n",
      "<class 'torch.Tensor'> tensor(23328) 23328\n",
      "25536 ; loss 0.22 accuracy:91.12 ;\n",
      "<class 'torch.Tensor'> tensor(29162) 29162\n",
      "31936 ; loss 0.23 accuracy:91.13 ;\n",
      "<class 'torch.Tensor'> tensor(35016) 35016\n",
      "38336 ; loss 0.21 accuracy:91.19 ;\n",
      "<class 'torch.Tensor'> tensor(40864) 40864\n",
      "44736 ; loss 0.22 accuracy:91.21 ;\n",
      "<class 'torch.Tensor'> tensor(46696) 46696\n",
      "51136 ; loss 0.22 accuracy:91.2 ;\n",
      "<class 'torch.Tensor'> tensor(52505) 52505\n",
      "57536 ; loss 0.23 accuracy:91.15 ;\n",
      "<class 'torch.Tensor'> tensor(58325) 58325\n",
      "63936 ; loss 0.22 accuracy:91.13 ;\n",
      "<class 'torch.Tensor'> tensor(64171) 64171\n",
      "70336 ; loss 0.22 accuracy:91.15 ;\n",
      "<class 'torch.Tensor'> tensor(70001) 70001\n",
      "76736 ; loss 0.22 accuracy:91.15 ;\n",
      "<class 'torch.Tensor'> tensor(75845) 75845\n",
      "83136 ; loss 0.22 accuracy:91.16 ;\n",
      "<class 'torch.Tensor'> tensor(81697) 81697\n",
      "89536 ; loss 0.22 accuracy:91.18 ;\n",
      "<class 'torch.Tensor'> tensor(87551) 87551\n",
      "95936 ; loss 0.22 accuracy:91.2 ;\n",
      "<class 'torch.Tensor'> tensor(93375) 93375\n",
      "102336 ; loss 0.22 accuracy:91.19 ;\n",
      "<class 'torch.Tensor'> tensor(99276) 99276\n",
      "108736 ; loss 0.21 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(105107) 105107\n",
      "115136 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(110955) 110955\n",
      "121536 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(116788) 116788\n",
      "127936 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(122631) 122631\n",
      "134336 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(128451) 128451\n",
      "140736 ; loss 0.22 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(134304) 134304\n",
      "147136 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(140162) 140162\n",
      "153536 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(145994) 145994\n",
      "159936 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(151809) 151809\n",
      "166336 ; loss 0.22 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(157674) 157674\n",
      "172736 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(163519) 163519\n",
      "179136 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(169352) 169352\n",
      "185536 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(175162) 175162\n",
      "191936 ; loss 0.24 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(181028) 181028\n",
      "198336 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(186847) 186847\n",
      "204736 ; loss 0.23 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(192684) 192684\n",
      "211136 ; loss 0.23 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(198522) 198522\n",
      "217536 ; loss 0.22 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(204372) 204372\n",
      "223936 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(210216) 210216\n",
      "230336 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(216037) 216037\n",
      "236736 ; loss 0.22 accuracy:91.23 ;\n",
      "results : epoch 10 ; mean accuracy train : 91.23\n",
      "\n",
      "VALIDATION : Epoch 10\n",
      "togrep : results : epoch 10 ; mean accuracy valid :              82.26\n",
      "Shrinking lr by : 5. New lr = 2.9232551919476505e-05\n",
      "\n",
      "TRAINING : Epoch 11\n",
      "Learning rate : 2.894022640028174e-05\n",
      "<class 'torch.Tensor'> tensor(5858) 5858\n",
      "6336 ; loss 0.22 accuracy:91.53 ;\n",
      "<class 'torch.Tensor'> tensor(11697) 11697\n",
      "12736 ; loss 0.22 accuracy:91.38 ;\n",
      "<class 'torch.Tensor'> tensor(17518) 17518\n",
      "19136 ; loss 0.23 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(23372) 23372\n",
      "25536 ; loss 0.22 accuracy:91.3 ;\n",
      "<class 'torch.Tensor'> tensor(29235) 29235\n",
      "31936 ; loss 0.22 accuracy:91.36 ;\n",
      "<class 'torch.Tensor'> tensor(35064) 35064\n",
      "38336 ; loss 0.22 accuracy:91.31 ;\n",
      "<class 'torch.Tensor'> tensor(40909) 40909\n",
      "44736 ; loss 0.22 accuracy:91.31 ;\n",
      "<class 'torch.Tensor'> tensor(46707) 46707\n",
      "51136 ; loss 0.23 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(52559) 52559\n",
      "57536 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(58408) 58408\n",
      "63936 ; loss 0.22 accuracy:91.26 ;\n",
      "<class 'torch.Tensor'> tensor(64211) 64211\n",
      "70336 ; loss 0.23 accuracy:91.21 ;\n",
      "<class 'torch.Tensor'> tensor(70020) 70020\n",
      "76736 ; loss 0.22 accuracy:91.17 ;\n",
      "<class 'torch.Tensor'> tensor(75861) 75861\n",
      "83136 ; loss 0.22 accuracy:91.18 ;\n",
      "<class 'torch.Tensor'> tensor(81706) 81706\n",
      "89536 ; loss 0.22 accuracy:91.19 ;\n",
      "<class 'torch.Tensor'> tensor(87528) 87528\n",
      "95936 ; loss 0.23 accuracy:91.17 ;\n",
      "<class 'torch.Tensor'> tensor(93369) 93369\n",
      "102336 ; loss 0.22 accuracy:91.18 ;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor(99265) 99265\n",
      "108736 ; loss 0.21 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(105147) 105147\n",
      "115136 ; loss 0.21 accuracy:91.27 ;\n",
      "<class 'torch.Tensor'> tensor(110960) 110960\n",
      "121536 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(116816) 116816\n",
      "127936 ; loss 0.22 accuracy:91.26 ;\n",
      "<class 'torch.Tensor'> tensor(122612) 122612\n",
      "134336 ; loss 0.23 accuracy:91.23 ;\n",
      "<class 'torch.Tensor'> tensor(128476) 128476\n",
      "140736 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(134270) 134270\n",
      "147136 ; loss 0.23 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(140113) 140113\n",
      "153536 ; loss 0.22 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(145951) 145951\n",
      "159936 ; loss 0.22 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(151793) 151793\n",
      "166336 ; loss 0.22 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(157626) 157626\n",
      "172736 ; loss 0.23 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(163473) 163473\n",
      "179136 ; loss 0.22 accuracy:91.22 ;\n",
      "<class 'torch.Tensor'> tensor(169342) 169342\n",
      "185536 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(175220) 175220\n",
      "191936 ; loss 0.22 accuracy:91.26 ;\n",
      "<class 'torch.Tensor'> tensor(181034) 181034\n",
      "198336 ; loss 0.23 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(186880) 186880\n",
      "204736 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(192704) 192704\n",
      "211136 ; loss 0.22 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(198564) 198564\n",
      "217536 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(204399) 204399\n",
      "223936 ; loss 0.22 accuracy:91.25 ;\n",
      "<class 'torch.Tensor'> tensor(210219) 210219\n",
      "230336 ; loss 0.23 accuracy:91.24 ;\n",
      "<class 'torch.Tensor'> tensor(216053) 216053\n",
      "236736 ; loss 0.22 accuracy:91.24 ;\n",
      "results : epoch 11 ; mean accuracy train : 91.24\n",
      "\n",
      "VALIDATION : Epoch 11\n",
      "togrep : results : epoch 11 ; mean accuracy valid :              82.29\n",
      "Shrinking lr by : 5. New lr = 5.788045280056347e-06\n",
      "\n",
      "TEST : Epoch 12\n",
      "\n",
      "VALIDATION : Epoch 1000000.0\n",
      "finalgrep : accuracy valid : 82.29\n",
      "finalgrep : accuracy test : 82.05\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "#from data import get_nli, get_batch, build_vocab\n",
    "#from mutils import get_optimizer\n",
    "#from models import NLINet\n",
    "\n",
    "W2V_PATH = \"/home/dc/cs230_project/dataset/GloVe/glove.840B.300d.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='/home/dc/InferSent/dataset/SNLI', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='infersent.pickle')\n",
    "\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=40)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"weight decay for sgd\")\n",
    "\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSent', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=2, help=\"duplicate/not duplicate\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default='300', help=\"embedding dim\")\n",
    "parser.add_argument(\"--LSTM_num_layers\", type=int, default='1', help=\"LSTM num layers\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default='/home/dc/cs230_project/dataset', help=\"store duplicate questions\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "\n",
    "# set gpu device\n",
    "torch.cuda.set_device(params.gpu_id)\n",
    "\n",
    "# print parameters passed, and all parameters\n",
    "print('\\ntogrep : {0}\\n'.format(sys.argv[1:]))\n",
    "print(params)\n",
    "\n",
    "\"\"\"\n",
    "SEED\n",
    "\"\"\"\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "\n",
    "\n",
    "#data formatting\n",
    "QUORA_PATH=\"/home/dc/cs230_project/dataset\"\n",
    "\n",
    "def clean_quora(quora_path):\n",
    "    '''\n",
    "    input: path of quora tsv file downloaded from kaggle\n",
    "    output: df with questions <10 chars removed\n",
    "    \n",
    "    '''\n",
    "    df = pd.read_csv(os.path.join(quora_path,\"quora_duplicate_questions.tsv\"),sep=\"\\t\")\n",
    "    print(df.head())\n",
    "    df = df.drop([\"id\",\"qid1\",\"qid2\"],axis=1)\n",
    "    print(df.count())\n",
    "    df=df.dropna()\n",
    "    print(df.count())\n",
    "    df['q1_len'] = df['question1'].apply(len)\n",
    "    df['q2_len'] = df['question2'].apply(len)\n",
    "    print(df.head())\n",
    "    #print(df.loc[df['q1_len'] < 10])\n",
    "    #print(df.loc[df['q2_len'] < 10])\n",
    "    df = df.loc[ (df['q1_len'] > 10) & (df['q2_len'] > 10)]\n",
    "    print(df.count())\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_nli(data_path):\n",
    "    s1 = {}\n",
    "    s2 = {}\n",
    "    target = {}\n",
    "\n",
    "    dico_label = {'entailment': 0,  'neutral': 1, 'contradiction': 2}\n",
    "\n",
    "    for data_type in ['train', 'dev', 'test']:\n",
    "        s1[data_type], s2[data_type], target[data_type] = {}, {}, {}\n",
    "        s1[data_type]['path'] = os.path.join(data_path, 's1.' + data_type)\n",
    "        s2[data_type]['path'] = os.path.join(data_path, 's2.' + data_type)\n",
    "        target[data_type]['path'] = os.path.join(data_path,\n",
    "                                                 'labels.' + data_type)\n",
    "\n",
    "        s1[data_type]['sent'] = [line.rstrip() for line in\n",
    "                                 open(s1[data_type]['path'], 'r')]\n",
    "        s2[data_type]['sent'] = [line.rstrip() for line in\n",
    "                                 open(s2[data_type]['path'], 'r')]\n",
    "        target[data_type]['data'] = np.array([dico_label[line.rstrip('\\n')]\n",
    "                for line in open(target[data_type]['path'], 'r')])\n",
    "\n",
    "        assert len(s1[data_type]['sent']) == len(s2[data_type]['sent']) == \\\n",
    "            len(target[data_type]['data'])\n",
    "\n",
    "        print('** {0} DATA : Found {1} pairs of {2} sentences.'.format(\n",
    "                data_type.upper(), len(s1[data_type]['sent']), data_type))\n",
    "\n",
    "    train = {'s1': s1['train']['sent'], 's2': s2['train']['sent'],\n",
    "             'label': target['train']['data']}\n",
    "    dev = {'s1': s1['dev']['sent'], 's2': s2['dev']['sent'],\n",
    "           'label': target['dev']['data']}\n",
    "    test = {'s1': s1['test']['sent'], 's2': s2['test']['sent'],\n",
    "            'label': target['test']['data']}\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "def load_single_file(filename):\n",
    "    fh = open(os.path.join(params.data_dir,filename+'.pkl'),'rb')\n",
    "    data = pickle.load(fh)\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "def load_data():\n",
    "    X_train = load_single_file(\"X_train\")\n",
    "    X_valid = load_single_file(\"X_valid\")\n",
    "    X_test = load_single_file(\"X_test\")\n",
    "    y_train = load_single_file(\"y_train\")\n",
    "    y_valid = load_single_file(\"y_valid\")\n",
    "    y_test = load_single_file(\"y_test\")\n",
    "    return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "    \n",
    "def format_data(X_train, X_valid,X_test, y_train,y_valid,y_test):\n",
    "    s1 = {}\n",
    "    s2 = {}\n",
    "    target = {}\n",
    "    s1['train'],s1['dev'],s1['test'],s2['train'],s2['dev'],s2['test'] = {},{},{},{},{},{}\n",
    "    target['train'],target['dev'],target['test']={},{},{}\n",
    "\n",
    "    s1['train']['sent'] = [x for x in X_train[:,0]]\n",
    "    s2['train']['sent'] = [x for x in X_train[:,1]]\n",
    "    s1['dev']['sent'] = [x for x in X_valid[:,0]]\n",
    "    s2['dev']['sent'] = [x for x in X_valid[:,1]]\n",
    "    s1['test']['sent'] = [x for x in X_test[:,0]]\n",
    "    s2['test']['sent'] = [x for x in X_test[:,1]]\n",
    "    target['train']['data'] = np.array([x[0] for x in y_train])\n",
    "    target['dev']['data'] = np.array([x[0] for x in y_valid])\n",
    "    target['test']['data'] = np.array([x[0] for x in y_test.tolist()])\n",
    "\n",
    "    train = {'s1': s1['train']['sent'], 's2': s2['train']['sent'],\n",
    "             'label': target['train']['data']}\n",
    "    dev = {'s1': s1['dev']['sent'], 's2': s2['dev']['sent'],\n",
    "           'label': target['dev']['data']}\n",
    "    test = {'s1': s1['test']['sent'], 's2': s2['test']['sent'],\n",
    "            'label': target['test']['data']}\n",
    "    return train,dev,test\n",
    "\n",
    "\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "def orig(params,W2V_PATH):\n",
    "    print(f\"loading from:{params.nlipath}\")\n",
    "    train, valid, test = get_nli(params.nlipath)\n",
    "    print(f\"orig checkpoint len(train[s1]):{len(train['s1'])} len(valid[s1]):{len(valid['s1'])} len(test[s1]):{len(test['s1'])}\")\n",
    "    word_vec = build_vocab(train['s1'] + train['s2'] +\n",
    "                       valid['s1'] + valid['s2'] +\n",
    "                       test['s1'] + test['s2'], W2V_PATH)\n",
    "    \n",
    "    for split in ['s1', 's2']:\n",
    "        for data_type in ['train', 'valid', 'test']:\n",
    "            eval(data_type)[split] = np.array([['<s>'] +\n",
    "            [word for word in sent.split() if word in word_vec] +\n",
    "            ['</s>'] for sent in eval(data_type)[split]])\n",
    "    \n",
    "    return train,valid,test,word_vec\n",
    "    \n",
    "def quora():\n",
    "    X_train,X_valid,X_test,y_train,y_valid,y_test = load_data()\n",
    "    train,valid,test = format_data(X_train, X_valid,X_test, y_train,y_valid,y_test)\n",
    "    print(f\"quora checkpoint len(train[s1]):{len(train['s1'])},len(train[s2]):{len(train['s2'])},\\\n",
    "          len(train[label]):{len(train['label'])}\")\n",
    "    print('============')\n",
    "    print(f\"len(valid['s1']):{len(valid['s1'])}, len(valid[s2]):{len(valid['s2'])}, \\\n",
    "          len(valid['label']):{len(valid['label'])}\")\n",
    "    print('============')\n",
    "    print(f\"len(test['s1']):{len(test['s1'])},len(test['s2']):{len(test['s2'])}, \\\n",
    "          len(test['label']):{len(test['label'])}\")\n",
    "          \n",
    "    word_vec = build_vocab(train['s1'] + train['s2'] +\n",
    "                       valid['s1'] + valid['s2'] +\n",
    "                       test['s1'] + test['s2'], W2V_PATH)\n",
    "    for split in ['s1', 's2']:\n",
    "        for data_type in ['train', 'valid', 'test']:\n",
    "            eval(data_type)[split] = np.array([['<s>'] +\n",
    "            [word for word in sent.split() if word in word_vec] +\n",
    "            ['</s>'] for sent in eval(data_type)[split]])\n",
    "    \n",
    "    return train,valid,test,word_vec\n",
    "\n",
    "#train, valid, test,word_vec = orig(params,W2V_PATH)\n",
    "train, valid, test,word_vec = quora()\n",
    "#print(f\"checkpoint after formatting: len(train[s1]):{len(train['s1'])} ,len(train[s2]):{len(train['s2'])} \\\n",
    "#      ,len(train[label]):{len(train['label'])}, len(valid[s2]):{len(valid['s1'])} ,len(valid[s2]):{len(valid['s2'])}, \\\n",
    "#      len(valid[label]):{len(valid['label'])},len(test[s2]):{len(test['s1'])}, len(test[s2]):{len(test['s2'])} \\\n",
    "#      ,len(valid[label]):{len(valid['label'])},len(word_vec):{len(word_vec)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODEL\n",
    "\"\"\"\n",
    "# model config\n",
    "config_nli_model = {\n",
    "    'n_words'        :  300          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, params.LSTM_num_layers,\n",
    "                                bidirectional=True, dropout=self.dpout_model)\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # either all weights are on cpu or they are on gpu\n",
    "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len: [max_len, ..., min_len] (bsize)\n",
    "        # sent: Variable(seqlen x bsize x worddim)\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # Sort by length (keep idx)\n",
    "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() else torch.from_numpy(idx_sort)\n",
    "        sent = sent.index_select(1, Variable(idx_sort))\n",
    "\n",
    "        # Handling padding in Recurrent Networks\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
    "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
    "\n",
    "        # Un-sort by length\n",
    "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_unsort)\n",
    "        sent_output = sent_output.index_select(1, Variable(idx_unsort))\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            sent_len = Variable(torch.FloatTensor(sent_len.copy())).unsqueeze(1).cuda()\n",
    "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
    "            emb = emb / sent_len.expand_as(emb)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb = torch.max(sent_output, 0)[0]\n",
    "            if emb.ndimension() == 3:\n",
    "                emb = emb.squeeze(0)\n",
    "                assert emb.ndimension() == 2\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with w2v vectors\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with k first w2v vectors\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if k <= K:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "                    k += 1\n",
    "                if k > K:\n",
    "                    if word in [self.bos, self.eos]:\n",
    "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    # build w2v vocab with k most frequent words\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (K))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # keep only new words\n",
    "        for word in self.word_vec:\n",
    "            if word in word_dict:\n",
    "                del word_dict[word]\n",
    "\n",
    "        # udpate vocabulary\n",
    "        if word_dict:\n",
    "            new_word_vec = self.get_w2v(word_dict)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "        else:\n",
    "            new_word_vec = []\n",
    "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # sent in batch in decreasing order of lengths\n",
    "        # batch: (bsize, max_len, word_dim)\n",
    "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "        return torch.FloatTensor(embed)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        if self.moses_tok:\n",
    "            s = ' '.join(word_tokenize(s))\n",
    "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
    "            return s.split()\n",
    "        else:\n",
    "            return word_tokenize(s)\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
    "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # filters words without w2v vectors\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
    "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
    "                s_f = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
    "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
    "\n",
    "        # sort by decreasing length\n",
    "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "        sentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "        return sentences, lengths, idx_sort\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "        sentences, lengths, idx_sort = self.prepare_samples(\n",
    "                        sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences), bsize):\n",
    "            batch = Variable(self.get_batch(\n",
    "                        sentences[stidx:stidx + bsize]), volatile=True)\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            batch = self.forward(\n",
    "                (batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "            embeddings.append(batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # unsort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
    "\n",
    "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
    "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        batch = Variable(self.get_batch(sent), volatile=True)\n",
    "\n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.enc_lstm(batch)[0]\n",
    "        output, idxs = torch.max(output, 0)\n",
    "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
    "        idxs = idxs.data.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "        # visualize model\n",
    "        import matplotlib.pyplot as plt\n",
    "        x = range(len(sent[0]))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent[0], rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs\n",
    "\n",
    "\n",
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = ((int)(self.inputdim/2)) if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                \n",
    "                )\n",
    "        else:\n",
    "            print(f\"self.inputdim:{self.inputdim}, self.fc_dim:{self.fc_dim}\")\n",
    "            print(type(self.inputdim),type(self.fc_dim))\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def get_optimizer(s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    assert expected_args[:2] == ['self', 'params']\n",
    "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "            str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn, optim_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "encoder_types = ['InferSent', 'BLSTMprojEncoder', 'BGRUlastEncoder',\n",
    "                 'InnerAttentionMILAEncoder', 'InnerAttentionYANGEncoder',\n",
    "                 'InnerAttentionNAACLEncoder', 'ConvNetEncoder', 'LSTMEncoder']\n",
    "assert params.encoder_type in encoder_types, \"encoder_type must be in \" + \\\n",
    "                                             str(encoder_types)\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)\n",
    "\n",
    "\n",
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "#BCE next w2 categories\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = -1e10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None\n",
    "\n",
    "\n",
    "def get_batch(batch, word_vec):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), 300))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "\n",
    "\n",
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['s1']))\n",
    "    #print(f\"type(permutation):{type(permutation)}\")\n",
    "    #print(f\"type(train['s1']):{type(train['s1'])}\")\n",
    "    \n",
    "    s1 = train['s1'][permutation]\n",
    "    s2 = train['s2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "    \n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        #print(type(s1_batch),type(s2_batch)) #should be list\n",
    "        #print(f\"s1_len:{s1_len},s2_len:{s2_len}\")\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        target_batch=target[stidx:stidx + params.batch_size]\n",
    "        #print(f\"target_batch.shape:{target_batch.shape}\")\n",
    "        #print(f\"target_batch:{target_batch}\")\n",
    "        #print(f\"target shape:{target.shape}\")\n",
    "        #print(f\"target:{target[stidx:stidx + params.batch_size]}\")\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "        #print(f\"tgt_batch:{tgt_batch}\")\n",
    "        #print(f\"k:{k}\")\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        #print(f\"type(tgt_batch):{type(tgt_batch)}\")\n",
    "        #print(f\"type(output):{type(output)}\")\n",
    "        #print(f\"output size:{output.size()}\")\n",
    "        \n",
    "        #print(f\"output:{output}\")\n",
    "        #\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        all_costs.append(loss.item())\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "        \n",
    "        if len(all_costs) == 100:\n",
    "            print(type(correct),correct,correct.item())\n",
    "            #logs.append('{0} ; loss {1} accuracy:{2} ;'.format(stidx,round(np.mean(all_costs), 2),round(100.*correct.item()/(stidx+k), 2)))\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)), \n",
    "                            round(100.*correct.item()/(stidx+k), 2)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = round(100 * correct.item()/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['s1'] if eval_type == 'valid' else test['s1']\n",
    "    s2 = valid['s2'] if eval_type == 'valid' else test['s2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "\n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "    # save model\n",
    "    eval_acc = round(100 * correct.item() / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train model on Natural Language Inference task\n",
    "\"\"\"\n",
    "epoch = 1\n",
    "print(f\"total num epochs:{params.n_epochs}\")\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1\n",
    "    \n",
    "\n",
    "# Run best model on test set.\n",
    "#nli_net.load_state_dict(os.path.join(params.outputdir, params.outputmodelname))\n",
    "\n",
    "print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "evaluate(1e6, 'valid', True)\n",
    "evaluate(0, 'test', True)\n",
    "\n",
    "# Save encoder instead of full model\n",
    "torch.save(nli_net.encoder.state_dict(), os.path.join(params.outputdir, params.outputmodelname + '.encoder.pkl'))\n",
    "#save entire model...\n",
    "\n",
    "\n",
    "print(\"fin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
