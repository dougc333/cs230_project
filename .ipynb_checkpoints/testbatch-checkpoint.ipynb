{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "dict_keys(['the', 'brown', 'fox', 'jumped', 'over', 'lazy', 'dog', 'tree', 'is', 'green', '<s>', '</s>', '<p>'])\n",
      "{'the': '', 'brown': '', 'fox': '', 'jumped': '', 'over': '', 'lazy': '', 'dog': '', 'tree': '', 'is': '', 'green': '', '<s>': '', '</s>': '', '<p>': ''}\n"
     ]
    }
   ],
   "source": [
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict\n",
    "\n",
    "dict = get_word_dict([\"the brown fox jumped over the lazy dog\",\"the tree is green\"])\n",
    "print(len(dict))\n",
    "print(dict.keys())\n",
    "print(dict)\n",
    "print(\"==================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13(/13) words with glove vectors\n",
      "dict_keys(['the', 'is', 'over', 'green', 'dog', 'tree', 'brown', 'lazy', 'jumped', 'fox', '<p>', '</s>', '<s>'])\n",
      "<class 'numpy.ndarray'>\n",
      "(300,)\n",
      "\n",
      "Found 11(/11) words with glove vectors\n",
      "Vocab size : 11\n",
      "(300,)\n",
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "word_vec = get_glove(dict,\"/home/dc/InferSent/dataset/GloVe/glove.840B.300d.txt\")\n",
    "print(word_vec.keys())\n",
    "print(type(word_vec[\"the\"]))\n",
    "print(word_vec[\"the\"].shape)\n",
    "print()\n",
    "\n",
    "def build_vocab(sentences, glove_path):\n",
    "    '''\n",
    "    input: sentences. list of words\n",
    "    output: dict of words->embeddings\n",
    "    '''\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "#dict of word/embeddings\n",
    "word_vec = build_vocab([\"the orange hair man has a big mouth\"],\"/home/dc/InferSent/dataset/GloVe/glove.840B.300d.txt\")\n",
    "print(word_vec['the'].shape)\n",
    "print(word_vec['man'].shape)\n",
    "print(word_vec['orange'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** TRAIN DATA : Found 549367 pairs of train sentences.\n",
      "** DEV DATA : Found 9842 pairs of dev sentences.\n",
      "** TEST DATA : Found 9824 pairs of test sentences.\n",
      "<class 'dict'> <class 'dict'> <class 'dict'>\n",
      "\n",
      "togrep : ['-f', '/run/user/1000/jupyter/kernel-69e5a421-fb17-46e0-91fa-010a3f36109f.json']\n",
      "\n",
      "Namespace(batch_size=64, decay=0.99, dpout_fc=0.0, dpout_model=0.0, enc_lstm_dim=2048, encoder_type='LSTMEncoder', fc_dim=512, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=3, n_enc_layers=1, n_epochs=20, nlipath='/home/dc/InferSent/dataset/SNLI', nonlinear_fc=0, optimizer='sgd,lr=0.1', outputdir='savedir/', outputmodelname='model.pickle', pool_type='max', seed=1234)\n",
      "3 3 3\n",
      "dict_keys(['s1', 's2', 'label'])\n",
      "549367 549367 549367\n",
      "** TRAIN DATA : Found 549367 pairs of train sentences.\n",
      "** DEV DATA : Found 9842 pairs of dev sentences.\n",
      "** TEST DATA : Found 9824 pairs of test sentences.\n",
      "Found 38957(/43479) words with glove vectors\n",
      "Vocab size : 38957\n",
      "549367 549367 549367\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from data import get_nli, get_batch, build_vocab\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#valid=dev\n",
    "train, valid, test = get_nli('/home/dc/InferSent/dataset/SNLI')\n",
    "print(type(train),type(valid),type(test))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='/home/dc/InferSent/dataset/SNLI', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=20)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=0, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='LSTMEncoder', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "\n",
    "# set gpu device\n",
    "torch.cuda.set_device(params.gpu_id)\n",
    "\n",
    "# print parameters passed, and all parameters\n",
    "print('\\ntogrep : {0}\\n'.format(sys.argv[1:]))\n",
    "print(params)\n",
    "\n",
    "\n",
    "print(len(train),len(valid),len(test))\n",
    "print(train.keys())\n",
    "train_s1,train_s2,train_labels = train['s1'],train['s2'],train['label']\n",
    "print(len(train_s1),len(train_s2),len(train_labels))\n",
    "W2V_PATH = \"/home/dc/InferSent/dataset/GloVe/glove.840B.300d.txt\"\n",
    "train, valid, test = get_nli(params.nlipath)\n",
    "word_vec = build_vocab(train['s1'] + train['s2'] +\n",
    "                       valid['s1'] + valid['s2'] +\n",
    "                       test['s1'] + test['s2'], W2V_PATH)\n",
    "\n",
    "for split in ['s1', 's2']:\n",
    "    for data_type in ['train', 'valid', 'test']:\n",
    "        eval(data_type)[split] = np.array([['<s>'] +\n",
    "            [word for word in sent.split() if word in word_vec] +\n",
    "            ['</s>'] for sent in eval(data_type)[split]])\n",
    "\n",
    "params.word_emb_dim = 300\n",
    "#train_s1 is list\n",
    "print(len(train_s1),len(train_s2),len(train_labels))\n",
    "print(type(train_s1[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "['<s>', 'A', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.', '</s>']\n",
      "['<s>', 'A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.', '</s>']\n",
      "1\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([29, 64, 300])\n",
      "(29, 64, 300)\n"
     ]
    }
   ],
   "source": [
    "print(type(train['s1']))\n",
    "print(type(train['s1'][0]))\n",
    "\n",
    "print(train['s1'][0])\n",
    "print(train['s2'][0])\n",
    "print(train['label'][0])\n",
    "#where are these turned into embeddings for input? \n",
    "\n",
    "s1 = train['s1']\n",
    "s2 = train['s2']\n",
    "target = train['label']\n",
    "stidx=0\n",
    "\n",
    "def get_batch(batch, word_vec):\n",
    "    '''\n",
    "    input: list of words which are sentences\n",
    "    output: tensor of embeddings which are sentences\n",
    "    '''\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), 300))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "\n",
    "\n",
    "s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "print(type(s1_batch))\n",
    "print(s1_batch.size())\n",
    "#print(s1_batch) #tensor prints out values unless it is a single value then use tensor.item()\n",
    "#convert pytorch tensor to numpy \n",
    "n = s1_batch.numpy()\n",
    "print(n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
