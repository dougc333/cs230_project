{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_dir='/home/dc/cs230_project/dataset')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import inspect\n",
    "import time \n",
    "import torch\n",
    "from torchqrnn import QRNN\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from MakeData import MakeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "togrep : ['-f', '/run/user/1000/jupyter/kernel-c4d4d516-faa0-4623-83a8-f10f3d1992a9.json']\n",
      "\n",
      "Namespace(LSTM_num_layers=1, batch_size=128, data_dir='/home/dc/cs230_project/dataset', decay=0.99, dpout_fc=0.2, dpout_model=0.0, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=512, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=2, n_enc_layers=1, n_epochs=12, nlipath='/home/dc/InferSent/dataset/SNLI', nonlinear_fc=1, optimizer='adam', outputdir='savedir/', outputmodelname='3layernonlinearT7600.pickle', pool_type='max', seed=1234, weight_decay=0.0005, word_emb_dim=300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from data import get_nli, get_batch, build_vocab\n",
    "#from mutils import get_optimizer\n",
    "#from models import NLINet\n",
    "\n",
    "#start_time = time.time()\n",
    "W2V_PATH = \"/home/dc/cs230_project/dataset/GloVe/glove.840B.300d.txt\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='/home/dc/InferSent/dataset/SNLI', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='3layernonlinearT7600.pickle')\n",
    "\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=12)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "#this only works if num_layers>1\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "#this is only for the dropout after batchnorm in nonlinear\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0.2, help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"adam\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"weight decay for sgd\")\n",
    "\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSent', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=2, help=\"duplicate/not duplicate\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default='300', help=\"embedding dim\")\n",
    "parser.add_argument(\"--LSTM_num_layers\", type=int, default='1', help=\"LSTM num layers\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default='/home/dc/cs230_project/dataset', help=\"store duplicate questions\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "# set gpu device\n",
    "torch.cuda.set_device(params.gpu_id)\n",
    "\n",
    "# print parameters passed, and all parameters\n",
    "print('\\ntogrep : {0}\\n'.format(sys.argv[1:]))\n",
    "print(params)\n",
    "\n",
    "\"\"\"\n",
    "SEED\n",
    "\"\"\"\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "#print(f\"elapsed time:\"{time.time()-start_time})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading big\n",
      "quora checkpoint len(train[s1]):242494,len(train[s2]):242494,          len(train[label]):242494\n",
      "============\n",
      "len(valid['s1']):80832, len(valid[s2]):80832,           len(valid['label']):80832\n",
      "============\n",
      "len(test['s1']):80832,len(test['s2']):80832,           len(test['label']):80832\n",
      "Found 88571(/232484) words with glove vectors\n",
      "Vocab size : 88571\n",
      "checkpoint after formatting: len(train[s1]):242494 ,len(train[s2]):242494       ,len(train[label]):242494, len(valid[s2]):80832 ,len(valid[s2]):80832,       len(valid[label]):80832,len(test[s2]):80832, len(test[s2]):80832       ,len(valid[label]):80832,len(word_vec):88571\n",
      "-----------------------------\n",
      "making small\n",
      "loading small\n",
      "quora checkpoint len(train[s1]):60623,len(train[s2]):60623,          len(train[label]):60623\n",
      "============\n",
      "len(valid['s1']):20208, len(valid[s2]):20208,           len(valid['label']):20208\n",
      "============\n",
      "len(test['s1']):20208,len(test['s2']):20208,           len(test['label']):20208\n",
      "Found 47877(/106290) words with glove vectors\n",
      "Vocab size : 47877\n",
      "checkpoint after formatting: len(train[s1]):60623 ,len(train[s2]):60623       ,len(train[label]):60623, len(valid[s2]):20208 ,len(valid[s2]):20208,       len(valid[label]):20208,len(test[s2]):20208, len(test[s2]):20208       ,len(valid[label]):20208,len(word_vec):47877\n",
      "-----------------------------\n",
      "making clean\n",
      "loading clean\n",
      "quora checkpoint len(train[s1]):219666,len(train[s2]):219666,          len(train[label]):219666\n",
      "============\n",
      "len(valid['s1']):73223, len(valid[s2]):73223,           len(valid['label']):73223\n",
      "============\n",
      "len(test['s1']):73222,len(test['s2']):73222,           len(test['label']):73222\n",
      "Found 79019(/198506) words with glove vectors\n",
      "Vocab size : 79019\n",
      "checkpoint after formatting: len(train[s1]):219666 ,len(train[s2]):219666       ,len(train[label]):219666, len(valid[s2]):73223 ,len(valid[s2]):73223,       len(valid[label]):73223,len(test[s2]):73222, len(test[s2]):73222       ,len(valid[label]):73223,len(word_vec):79019\n",
      "-----------------------------\n",
      "orig code base verify python lib\n",
      "quora checkpoint len(train[s1]):242494,len(train[s2]):242494,          len(train[label]):242494\n",
      "============\n",
      "len(valid['s1']):80832, len(valid[s2]):80832,           len(valid['label']):80832\n",
      "============\n",
      "len(test['s1']):80832,len(test['s2']):80832,           len(test['label']):80832\n",
      "Found 88571(/232484) words with glove vectors\n",
      "Vocab size : 88571\n",
      "checkpoint after formatting: len(train[s1]):242494 ,len(train[s2]):242494       ,len(train[label]):242494, len(valid[s2]):80832 ,len(valid[s2]):80832,       len(valid[label]):80832,len(test[s2]):80832, len(test[s2]):80832       ,len(valid[label]):80832,len(word_vec):88571\n"
     ]
    }
   ],
   "source": [
    "#data formatting\n",
    "#QUORA_PATH=\"/home/dc/cs230_project/dataset\"\n",
    "\n",
    "def clean_quora(quora_path):\n",
    "    '''\n",
    "    input: path of quora tsv file downloaded from kaggle\n",
    "    output: df with questions <10 chars removed\n",
    "    \n",
    "    '''\n",
    "    df = pd.read_csv(os.path.join(quora_path,\"quora_duplicate_questions.tsv\"),sep=\"\\t\")\n",
    "    print(df.head())\n",
    "    df = df.drop([\"id\",\"qid1\",\"qid2\"],axis=1)\n",
    "    print(df.count())\n",
    "    df=df.dropna()\n",
    "    print(df.count())\n",
    "    df['q1_len'] = df['question1'].apply(len)\n",
    "    df['q2_len'] = df['question2'].apply(len)\n",
    "    print(df.head())\n",
    "    #print(df.loc[df['q1_len'] < 10])\n",
    "    #print(df.loc[df['q2_len'] < 10])\n",
    "    df = df.loc[ (df['q1_len'] > 10) & (df['q2_len'] > 10)]\n",
    "    print(df.count())\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def load_single_file(filename):\n",
    "    fh = open(os.path.join(params.data_dir,filename+'.pkl'),'rb')\n",
    "    data = pickle.load(fh)\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    remove <10 char length question rows. this isnt enough cleaning.\n",
    "    '''\n",
    "    X_train = load_single_file(\"X_train\")\n",
    "    X_valid = load_single_file(\"X_valid\")\n",
    "    X_test = load_single_file(\"X_test\")\n",
    "    y_train = load_single_file(\"y_train\")\n",
    "    y_valid = load_single_file(\"y_valid\")\n",
    "    y_test = load_single_file(\"y_test\")\n",
    "    return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "    \n",
    "def load_data_1sent():\n",
    "    '''\n",
    "    1 sentence is same as 0 periods or 1 period? \n",
    "    loads X_train_1sent where each question is only 1 sentence for both question 1 and question 2.\n",
    "    '''\n",
    "    X_train = load_single_file(\"X_train_1sent\")\n",
    "    X_valid = load_single_file(\"X_valid_1sent\")\n",
    "    X_test = load_single_file(\"X_test_1sent\")\n",
    "    y_train = load_single_file(\"y_train_1sent\")\n",
    "    y_valid = load_single_file(\"y_valid_1sent\")\n",
    "    y_test = load_single_file(\"y_test_1sent\")\n",
    "    return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "\n",
    "def load_data_2sent():\n",
    "    '''\n",
    "    has to have 2 periods or more. starts to become a document. do we remove the 0 sentences? \n",
    "    then we have very little data so we have to go to a different dataset. \n",
    "    loads X_train_2sent which contains data with max of 2 sentences per question\n",
    "    '''\n",
    "    X_train = load_single_file(\"X_train_2sent\")\n",
    "    X_valid = load_single_file(\"X_valid_2sent\")\n",
    "    X_test = load_single_file(\"X_test_2sent\")\n",
    "    y_train = load_single_file(\"y_train_2sent\")\n",
    "    y_valid = load_single_file(\"y_valid_2sent\")\n",
    "    y_test = load_single_file(\"y_test_2sent\")\n",
    "    return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "    \n",
    "\n",
    "def format_data(X_train, X_valid,X_test, y_train,y_valid,y_test):\n",
    "    s1 = {}\n",
    "    s2 = {}\n",
    "    target = {}\n",
    "    s1['train'],s1['dev'],s1['test'],s2['train'],s2['dev'],s2['test'] = {},{},{},{},{},{}\n",
    "    target['train'],target['dev'],target['test']={},{},{}\n",
    "\n",
    "    s1['train']['sent'] = [x for x in X_train[:,0]]\n",
    "    s2['train']['sent'] = [x for x in X_train[:,1]]\n",
    "    s1['dev']['sent'] = [x for x in X_valid[:,0]]\n",
    "    s2['dev']['sent'] = [x for x in X_valid[:,1]]\n",
    "    s1['test']['sent'] = [x for x in X_test[:,0]]\n",
    "    s2['test']['sent'] = [x for x in X_test[:,1]]\n",
    "    target['train']['data'] = np.array([x[0] for x in y_train])\n",
    "    target['dev']['data'] = np.array([x[0] for x in y_valid])\n",
    "    target['test']['data'] = np.array([x[0] for x in y_test.tolist()])\n",
    "\n",
    "    train = {'s1': s1['train']['sent'], 's2': s2['train']['sent'],\n",
    "             'label': target['train']['data']}\n",
    "    dev = {'s1': s1['dev']['sent'], 's2': s2['dev']['sent'],\n",
    "           'label': target['dev']['data']}\n",
    "    test = {'s1': s1['test']['sent'], 's2': s2['test']['sent'],\n",
    "            'label': target['test']['data']}\n",
    "    return train,dev,test\n",
    "\n",
    "\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "def quora():\n",
    "    X_train,X_valid,X_test,y_train,y_valid,y_test = load_data()\n",
    "    train,valid,test = format_data(X_train, X_valid,X_test, y_train,y_valid,y_test)\n",
    "    print(f\"quora checkpoint len(train[s1]):{len(train['s1'])},len(train[s2]):{len(train['s2'])},\\\n",
    "          len(train[label]):{len(train['label'])}\")\n",
    "    print('============')\n",
    "    print(f\"len(valid['s1']):{len(valid['s1'])}, len(valid[s2]):{len(valid['s2'])}, \\\n",
    "          len(valid['label']):{len(valid['label'])}\")\n",
    "    print('============')\n",
    "    print(f\"len(test['s1']):{len(test['s1'])},len(test['s2']):{len(test['s2'])}, \\\n",
    "          len(test['label']):{len(test['label'])}\")\n",
    "          \n",
    "    word_vec = build_vocab(train['s1'] + train['s2'] +\n",
    "                       valid['s1'] + valid['s2'] +\n",
    "                       test['s1'] + test['s2'], W2V_PATH)\n",
    "    for split in ['s1', 's2']:\n",
    "        for data_type in ['train', 'valid', 'test']:\n",
    "            eval(data_type)[split] = np.array([['<s>'] +\n",
    "            [word for word in sent.split() if word in word_vec] +\n",
    "            ['</s>'] for sent in eval(data_type)[split]])\n",
    "    \n",
    "    return train,valid,test,word_vec\n",
    "\n",
    "#train, valid, test,word_vec = orig(params,W2V_PATH)\n",
    "make_data=MakeData()\n",
    "train, valid, test,word_vec = make_data.quora(big=True,small=False,clean=False)\n",
    "print(f\"checkpoint after formatting: len(train[s1]):{len(train['s1'])} ,len(train[s2]):{len(train['s2'])} \\\n",
    "      ,len(train[label]):{len(train['label'])}, len(valid[s2]):{len(valid['s1'])} ,len(valid[s2]):{len(valid['s2'])}, \\\n",
    "      len(valid[label]):{len(valid['label'])},len(test[s2]):{len(test['s1'])}, len(test[s2]):{len(test['s2'])} \\\n",
    "      ,len(valid[label]):{len(valid['label'])},len(word_vec):{len(word_vec)}\")\n",
    "print(\"-----------------------------\")\n",
    "print(\"making small\")\n",
    "train, valid, test,word_vec = make_data.quora(big=False,small=True,clean=False)\n",
    "print(f\"checkpoint after formatting: len(train[s1]):{len(train['s1'])} ,len(train[s2]):{len(train['s2'])} \\\n",
    "      ,len(train[label]):{len(train['label'])}, len(valid[s2]):{len(valid['s1'])} ,len(valid[s2]):{len(valid['s2'])}, \\\n",
    "      len(valid[label]):{len(valid['label'])},len(test[s2]):{len(test['s1'])}, len(test[s2]):{len(test['s2'])} \\\n",
    "      ,len(valid[label]):{len(valid['label'])},len(word_vec):{len(word_vec)}\")\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print(\"making clean\")\n",
    "train, valid, test,word_vec = make_data.quora(big=False,small=False,clean=True)\n",
    "print(f\"checkpoint after formatting: len(train[s1]):{len(train['s1'])} ,len(train[s2]):{len(train['s2'])} \\\n",
    "      ,len(train[label]):{len(train['label'])}, len(valid[s2]):{len(valid['s1'])} ,len(valid[s2]):{len(valid['s2'])}, \\\n",
    "      len(valid[label]):{len(valid['label'])},len(test[s2]):{len(test['s1'])}, len(test[s2]):{len(test['s2'])} \\\n",
    "      ,len(valid[label]):{len(valid['label'])},len(word_vec):{len(word_vec)}\")\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "print(\"orig code base verify python lib\")\n",
    "train, valid, test,word_vec = quora()\n",
    "print(f\"checkpoint after formatting: len(train[s1]):{len(train['s1'])} ,len(train[s2]):{len(train['s2'])} \\\n",
    "      ,len(train[label]):{len(train['label'])}, len(valid[s2]):{len(valid['s1'])} ,len(valid[s2]):{len(valid['s2'])}, \\\n",
    "      len(valid[label]):{len(valid['label'])},len(test[s2]):{len(test['s1'])}, len(test[s2]):{len(test['s2'])} \\\n",
    "      ,len(valid[label]):{len(valid['label'])},len(word_vec):{len(word_vec)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "MODEL\n",
    "\"\"\"\n",
    "# model config\n",
    "config_nli_model = {\n",
    "    'n_words'        :  300          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, params.LSTM_num_layers,\n",
    "                                bidirectional=True, dropout=self.dpout_model)\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # either all weights are on cpu or they are on gpu\n",
    "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len: [max_len, ..., min_len] (bsize)\n",
    "        # sent: Variable(seqlen x bsize x worddim)\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # Sort by length (keep idx) https://github.com/pytorch/pytorch/issues/3584\n",
    "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() else torch.from_numpy(idx_sort)\n",
    "        sent = sent.index_select(1, Variable(idx_sort))\n",
    "\n",
    "        # Padding perf increase\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
    "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
    "\n",
    "        # Un-sort by length\n",
    "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_unsort)\n",
    "        sent_output = sent_output.index_select(1, Variable(idx_unsort))\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            sent_len = Variable(torch.FloatTensor(sent_len.copy())).unsqueeze(1).cuda()\n",
    "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
    "            emb = emb / sent_len.expand_as(emb)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb = torch.max(sent_output, 0)[0]\n",
    "            if emb.ndimension() == 3:\n",
    "                emb = emb.squeeze(0)\n",
    "                assert emb.ndimension() == 2\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with w2v vectors\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with k first w2v vectors\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if k <= K:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "                    k += 1\n",
    "                if k > K:\n",
    "                    if word in [self.bos, self.eos]:\n",
    "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    # build w2v vocab with k most frequent words\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (K))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # keep only new words\n",
    "        for word in self.word_vec:\n",
    "            if word in word_dict:\n",
    "                del word_dict[word]\n",
    "\n",
    "        # udpate vocabulary\n",
    "        if word_dict:\n",
    "            new_word_vec = self.get_w2v(word_dict)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "        else:\n",
    "            new_word_vec = []\n",
    "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # sent in batch in decreasing order of lengths\n",
    "        # batch: (bsize, max_len, word_dim)\n",
    "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "        return torch.FloatTensor(embed)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        if self.moses_tok:\n",
    "            s = ' '.join(word_tokenize(s))\n",
    "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
    "            return s.split()\n",
    "        else:\n",
    "            return word_tokenize(s)\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
    "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # filters words without w2v vectors\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
    "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
    "                s_f = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
    "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
    "\n",
    "        # sort by decreasing length\n",
    "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "        sentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "        return sentences, lengths, idx_sort\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "        sentences, lengths, idx_sort = self.prepare_samples(\n",
    "                        sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences), bsize):\n",
    "            batch = Variable(self.get_batch(\n",
    "                        sentences[stidx:stidx + bsize]), volatile=True)\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            batch = self.forward(\n",
    "                (batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "            embeddings.append(batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # unsort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
    "\n",
    "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
    "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        batch = Variable(self.get_batch(sent), volatile=True)\n",
    "\n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.enc_lstm(batch)[0]\n",
    "        output, idxs = torch.max(output, 0)\n",
    "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
    "        idxs = idxs.data.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "        # visualize model\n",
    "        import matplotlib.pyplot as plt\n",
    "        x = range(len(sent[0]))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent[0], rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs\n",
    "\n",
    "\n",
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = eval(self.encoder_type)(config)\n",
    "        self.inputdim = 4*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = ((int)(self.inputdim/2)) if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                \n",
    "                )\n",
    "        else:\n",
    "            print(f\"self.inputdim:{self.inputdim}, self.fc_dim:{self.fc_dim}\")\n",
    "            print(type(self.inputdim),type(self.fc_dim))\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def get_optimizer(s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    assert expected_args[:2] == ['self', 'params']\n",
    "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "            str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn, optim_params\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "encoder_types = ['InferSent', 'BLSTMprojEncoder', 'BGRUlastEncoder',\n",
    "                 'InnerAttentionMILAEncoder', 'InnerAttentionYANGEncoder',\n",
    "                 'InnerAttentionNAACLEncoder', 'ConvNetEncoder', 'LSTMEncoder']\n",
    "assert params.encoder_type in encoder_types, \"encoder_type must be in \" + \\\n",
    "                                             str(encoder_types)\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)\n",
    "\n",
    "\n",
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "#BCE next w2 categories\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = 10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None\n",
    "\n",
    "\n",
    "def get_batch(batch, word_vec):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), 300))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "\n",
    "\n",
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['s1']))\n",
    "    #print(f\"type(permutation):{type(permutation)}\")\n",
    "    #print(f\"type(train['s1']):{type(train['s1'])}\")\n",
    "    \n",
    "    s1 = train['s1'][permutation]\n",
    "    s2 = train['s2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "    \n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        #print(type(s1_batch),type(s2_batch)) #should be list\n",
    "        #print(f\"s1_len:{s1_len},s2_len:{s2_len}\")\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        target_batch=target[stidx:stidx + params.batch_size]\n",
    "        #print(f\"target_batch.shape:{target_batch.shape}\")\n",
    "        #print(f\"target_batch:{target_batch}\")\n",
    "        #print(f\"target shape:{target.shape}\")\n",
    "        #print(f\"target:{target[stidx:stidx + params.batch_size]}\")\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "        #print(f\"tgt_batch:{tgt_batch}\")\n",
    "        #print(f\"k:{k}\")\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        #print(f\"type(tgt_batch):{type(tgt_batch)}\")\n",
    "        #print(f\"type(output):{type(output)}\")\n",
    "        #print(f\"output size:{output.size()}\")\n",
    "        \n",
    "        #print(f\"output:{output}\")\n",
    "        #\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        all_costs.append(loss.item())\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "        \n",
    "        if len(all_costs) == 100:\n",
    "            print(type(correct),correct,correct.item())\n",
    "            #logs.append('{0} ; loss {1} accuracy:{2} ;'.format(stidx,round(np.mean(all_costs), 2),round(100.*correct.item()/(stidx+k), 2)))\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)), \n",
    "                            round(100.*correct.item()/(stidx+k), 2)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = round(100 * correct.item()/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['s1'] if eval_type == 'valid' else test['s1']\n",
    "    s2 = valid['s2'] if eval_type == 'valid' else test['s2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "    \n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "    # save model\n",
    "    eval_acc = round(100 * correct.item() / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train model on Natural Language Inference task\n",
    "\"\"\"\n",
    "epoch = 1\n",
    "print(f\"total num epochs:{params.n_epochs}\")\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1\n",
    "    \n",
    "#nli_net.save_state_dict(os.path.join(params.outputdir, params.outputmodelname))\n",
    "# Run best model on test set.\n",
    "#nli_net.load_state_dict(os.path.join(params.outputdir, params.outputmodelname))\n",
    "print(\"saving state dict\")\n",
    "torch.save(nli_net.state_dict,os.path.join(params.outputdir, params.outputmodelname + \"3layernonlinearT7600_fullmodel.pt\"))\n",
    "print(\"done saving state dict\")\n",
    "\n",
    "\n",
    "print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "print('calculating validation error')\n",
    "evaluate(1e6, 'valid', True)\n",
    "print('calculating test error')\n",
    "evaluate(0, 'test', True)\n",
    "\n",
    "# Save encoder instead of full model\n",
    "torch.save(nli_net.encoder.state_dict(), os.path.join(params.outputdir, params.outputmodelname + '.encoder.pkl'))\n",
    "#save entire model...\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"fin\",elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_net = NLINet(config_nli_model)\n",
    "checkpoint = torch.load('/home/dc/cs230_project/savedir/3layernonlinearT7600.pickle')\n",
    "#print(type(checkpoint),checkpoint.keys())\n",
    "nli_net.load_state_dict(checkpoint)\n",
    "s1=test['s1']\n",
    "s2=test['s2']\n",
    "target=test['label']\n",
    "print(type(s1),type(s2),type(target),s1.shape,s2.shape,target.shape)\n",
    "print(s1[1])\n",
    "print('------')\n",
    "print(s2[1])\n",
    "print('------')\n",
    "print(target[1])\n",
    "print('------')\n",
    "print(type(nli_net))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_list(my_list,filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(my_list,f)\n",
    "        f.close()\n",
    "    print(\"list saved to file!\")\n",
    "\n",
    "def read_list(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        my_list = pickle.load(f)\n",
    "    return my_list\n",
    "\n",
    "save_list([1,2,3],\"simple.pkl\")\n",
    "\n",
    "restore_list = read_list(\"simple.pkl\")\n",
    "print(restore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['s1'] if eval_type == 'valid' else test['s1']\n",
    "    s2 = valid['s2'] if eval_type == 'valid' else test['s2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "    \n",
    "    predictions=[]\n",
    "    targets=[]\n",
    "    \n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        predictions.append(pred.cpu().data.numpy().tolist())\n",
    "        targets.append(tgt_batch.cpu().data.numpy().tolist())\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "    \n",
    "    #save predictions\n",
    "    # save model\n",
    "    if eval_type == 'valid':\n",
    "        save_list(predictions,\"valid_3layernonlinear_predict.pkl\")\n",
    "        save_list(targets,\"valid_3layernonlinear_targets.pkl\")\n",
    "    else:\n",
    "        save_list(predictions,\"test_3layernonlinear_predict.pkl\")\n",
    "        save_list(targets,\"test_3layernonlinear_targets.pkl\")\n",
    "        \n",
    "    eval_acc = round(100 * correct.item() / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n",
    "\n",
    "val_acc_best = 10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None\n",
    "\n",
    "\n",
    "def get_batch(batch, word_vec):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), 300))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "\n",
    "\n",
    "evaluate(0, 'test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "predict = read_list(\"test_3layernonlinear_predict.pkl\")\n",
    "targets = read_list(\"test_3layernonlinear_targets.pkl\")\n",
    "print(len(predict),len(targets),len(predict[0]),len(targets[0]))\n",
    "predict_flatten = [x for sublist in predict for x in sublist]\n",
    "target_flatten = [x for sublist in targets for x in sublist]\n",
    "print(len(predict_flatten), len(target_flatten))\n",
    "print(f\"roc_auc_score:{roc_auc_score(target_flatten,predict_flatten)}\")\n",
    "fpr, tpr, thresholds = roc_curve(target_flatten, predict_flatten, pos_label=1)\n",
    "#plot fpr vs. tpr;tpr = y axis\n",
    "print(type(fpr),type(tpr))\n",
    "print('fpr',fpr)\n",
    "print('tpr',tpr)\n",
    "np.save('fpr.npy', fpr)\n",
    "np.save('tpr.npy', tpr)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title(\"test roc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        print(output)\n",
    "        pred = output.data.max(1)[1]\n",
    "        print(pred)\n",
    "        #correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(s1),type(s2),type(target),s1.shape[0],s2.shape[0],target.shape[0])\n",
    "nli_net_restore.eval()\n",
    "s1_batch, s1_len = get_batch(s1, word_vec)\n",
    "s2_batch, s2_len = get_batch(s2, word_vec)\n",
    "s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "tgt_batch = Variable(torch.LongTensor(target)).cuda()\n",
    "\n",
    "output = nli_net_restore((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
