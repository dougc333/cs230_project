<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>test</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[nltk_data] Downloading package punkt to /home/dc/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[1]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls
<span class="o">!</span>cat models.py
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>data.py  encoder  models.py  mutils.py	  README.md  train_nli.py
dataset  LICENSE  MultiNLI   __pycache__  SNLI	     Untitled.ipynb
# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

&#34;&#34;&#34;
This file contains the definition of encoders used in https://arxiv.org/pdf/1705.02364.pdf
&#34;&#34;&#34;

import numpy as np
import time

import torch
from torch.autograd import Variable
import torch.nn as nn

&#34;&#34;&#34;
BLSTM (max/mean) encoder
&#34;&#34;&#34;

class InferSent(nn.Module):

    def __init__(self, config):
        super(InferSent, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]
        self.version = 1 if &#39;version&#39; not in config else config[&#39;version&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True, dropout=self.dpout_model)

        assert self.version in [1, 2]
        if self.version == 1:
            self.bos = &#39;&lt;s&gt;&#39;
            self.eos = &#39;&lt;/s&gt;&#39;
            self.max_pad = True
            self.moses_tok = False
        elif self.version == 2:
            self.bos = &#39;&lt;p&gt;&#39;
            self.eos = &#39;&lt;/p&gt;&#39;
            self.max_pad = False
            self.moses_tok = True

    def is_cuda(self):
        # either all weights are on cpu or they are on gpu
        return &#39;cuda&#39; in str(type(self.enc_lstm.bias_hh_l0.data))

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (bsize)
        # sent: Variable(seqlen x bsize x worddim)
        sent, sent_len = sent_tuple

        # Sort by length (keep idx)
        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        idx_unsort = np.argsort(idx_sort)

        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \
            else torch.from_numpy(idx_sort)
        sent = sent.index_select(1, Variable(idx_sort))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)
        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]

        # Un-sort by length
        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \
            else torch.from_numpy(idx_unsort)
        sent_output = sent_output.index_select(1, Variable(idx_unsort))

        # Pooling
        if self.pool_type == &#34;mean&#34;:
            sent_len = Variable(torch.FloatTensor(sent_len.copy())).unsqueeze(1).cuda()
            emb = torch.sum(sent_output, 0).squeeze(0)
            emb = emb / sent_len.expand_as(emb)
        elif self.pool_type == &#34;max&#34;:
            if not self.max_pad:
                sent_output[sent_output == 0] = -1e9
            emb = torch.max(sent_output, 0)[0]
            if emb.ndimension() == 3:
                emb = emb.squeeze(0)
                assert emb.ndimension() == 2

        return emb

    def set_w2v_path(self, w2v_path):
        self.w2v_path = w2v_path

    def get_word_dict(self, sentences, tokenize=True):
        # create vocab of words
        word_dict = {}
        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]
        for sent in sentences:
            for word in sent:
                if word not in word_dict:
                    word_dict[word] = &#39;&#39;
        word_dict[self.bos] = &#39;&#39;
        word_dict[self.eos] = &#39;&#39;
        return word_dict

    def get_w2v(self, word_dict):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        # create word_vec with w2v vectors
        word_vec = {}
        with open(self.w2v_path) as f:
            for line in f:
                word, vec = line.split(&#39; &#39;, 1)
                if word in word_dict:
                    word_vec[word] = np.fromstring(vec, sep=&#39; &#39;)
        print(&#39;Found %s(/%s) words with w2v vectors&#39; % (len(word_vec), len(word_dict)))
        return word_vec

    def get_w2v_k(self, K):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        # create word_vec with k first w2v vectors
        k = 0
        word_vec = {}
        with open(self.w2v_path) as f:
            for line in f:
                word, vec = line.split(&#39; &#39;, 1)
                if k &lt;= K:
                    word_vec[word] = np.fromstring(vec, sep=&#39; &#39;)
                    k += 1
                if k &gt; K:
                    if word in [self.bos, self.eos]:
                        word_vec[word] = np.fromstring(vec, sep=&#39; &#39;)

                if k &gt; K and all([w in word_vec for w in [self.bos, self.eos]]):
                    break
        return word_vec

    def build_vocab(self, sentences, tokenize=True):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        word_dict = self.get_word_dict(sentences, tokenize)
        self.word_vec = self.get_w2v(word_dict)
        print(&#39;Vocab size : %s&#39; % (len(self.word_vec)))

    # build w2v vocab with k most frequent words
    def build_vocab_k_words(self, K):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        self.word_vec = self.get_w2v_k(K)
        print(&#39;Vocab size : %s&#39; % (K))

    def update_vocab(self, sentences, tokenize=True):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;warning : w2v path not set&#39;
        assert hasattr(self, &#39;word_vec&#39;), &#39;build_vocab before updating it&#39;
        word_dict = self.get_word_dict(sentences, tokenize)

        # keep only new words
        for word in self.word_vec:
            if word in word_dict:
                del word_dict[word]

        # udpate vocabulary
        if word_dict:
            new_word_vec = self.get_w2v(word_dict)
            self.word_vec.update(new_word_vec)
        else:
            new_word_vec = []
        print(&#39;New vocab size : %s (added %s words)&#39;% (len(self.word_vec), len(new_word_vec)))

    def get_batch(self, batch):
        # sent in batch in decreasing order of lengths
        # batch: (bsize, max_len, word_dim)
        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))

        for i in range(len(batch)):
            for j in range(len(batch[i])):
                embed[j, i, :] = self.word_vec[batch[i][j]]

        return torch.FloatTensor(embed)

    def tokenize(self, s):
        from nltk.tokenize import word_tokenize
        if self.moses_tok:
            s = &#39; &#39;.join(word_tokenize(s))
            s = s.replace(&#34; n&#39;t &#34;, &#34;n &#39;t &#34;)  # HACK to get ~MOSES tokenization
            return s.split()
        else:
            return word_tokenize(s)

    def prepare_samples(self, sentences, bsize, tokenize, verbose):
        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else
                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]
        n_w = np.sum([len(x) for x in sentences])

        # filters words without w2v vectors
        for i in range(len(sentences)):
            s_f = [word for word in sentences[i] if word in self.word_vec]
            if not s_f:
                import warnings
                warnings.warn(&#39;No words in &#34;%s&#34; (idx=%s) have w2v vectors. \
                               Replacing by &#34;&lt;/s&gt;&#34;..&#39; % (sentences[i], i))
                s_f = [self.eos]
            sentences[i] = s_f

        lengths = np.array([len(s) for s in sentences])
        n_wk = np.sum(lengths)
        if verbose:
            print(&#39;Nb words kept : %s/%s (%.1f%s)&#39; % (
                        n_wk, n_w, 100.0 * n_wk / n_w, &#39;%&#39;))

        # sort by decreasing length
        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)
        sentences = np.array(sentences)[idx_sort]

        return sentences, lengths, idx_sort

    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):
        tic = time.time()
        sentences, lengths, idx_sort = self.prepare_samples(
                        sentences, bsize, tokenize, verbose)

        embeddings = []
        for stidx in range(0, len(sentences), bsize):
            batch = Variable(self.get_batch(
                        sentences[stidx:stidx + bsize]), volatile=True)
            if self.is_cuda():
                batch = batch.cuda()
            batch = self.forward(
                (batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()
            embeddings.append(batch)
        embeddings = np.vstack(embeddings)

        # unsort
        idx_unsort = np.argsort(idx_sort)
        embeddings = embeddings[idx_unsort]

        if verbose:
            print(&#39;Speed : %.1f sentences/s (%s mode, bsize=%s)&#39; % (
                    len(embeddings)/(time.time()-tic),
                    &#39;gpu&#39; if self.is_cuda() else &#39;cpu&#39;, bsize))
        return embeddings

    def visualize(self, sent, tokenize=True):

        sent = sent.split() if not tokenize else self.tokenize(sent)
        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]

        if &#39; &#39;.join(sent[0]) == &#39;%s %s&#39; % (self.bos, self.eos):
            import warnings
            warnings.warn(&#39;No words in &#34;%s&#34; have w2v vectors. Replacing \
                           by &#34;%s %s&#34;..&#39; % (sent, self.bos, self.eos))
        batch = Variable(self.get_batch(sent), volatile=True)

        if self.is_cuda():
            batch = batch.cuda()
        output = self.enc_lstm(batch)[0]
        output, idxs = torch.max(output, 0)
        # output, idxs = output.squeeze(), idxs.squeeze()
        idxs = idxs.data.cpu().numpy()
        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]

        # visualize model
        import matplotlib.pyplot as plt
        x = range(len(sent[0]))
        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]
        plt.xticks(x, sent[0], rotation=45)
        plt.bar(x, y)
        plt.ylabel(&#39;%&#39;)
        plt.title(&#39;Visualisation of words importance&#39;)
        plt.show()

        return output, idxs

&#34;&#34;&#34;
BiGRU encoder (first/last hidden states)
&#34;&#34;&#34;


class BGRUlastEncoder(nn.Module):
    def __init__(self, config):
        super(BGRUlastEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.GRU(self.word_emb_dim, self.enc_lstm_dim, 1,
                               bidirectional=True, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        _, hn = self.enc_lstm(sent_packed, self.init_lstm)
        emb = torch.cat((hn[0], hn[1]), 1)  # batch x 2*nhid

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        emb = emb.index_select(0, Variable(torch.cuda.LongTensor(idx_unsort)))

        return emb


&#34;&#34;&#34;
BLSTM encoder with projection after BiLSTM
&#34;&#34;&#34;


class BLSTMprojEncoder(nn.Module):
    def __init__(self, config):
        super(BLSTMprojEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()
        self.proj_enc = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=False)

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1,
                Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = self.proj_enc(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(-1, bsize, 2*self.enc_lstm_dim)
        # Pooling
        if self.pool_type == &#34;mean&#34;:
            sent_len = Variable(torch.FloatTensor(sent_len)).unsqueeze(1).cuda()
            emb = torch.sum(sent_output, 0).squeeze(0)
            emb = emb / sent_len.expand_as(emb)
        elif self.pool_type == &#34;max&#34;:
            emb = torch.max(sent_output, 0)[0].squeeze(0)

        return emb


&#34;&#34;&#34;
LSTM encoder
&#34;&#34;&#34;


class LSTMEncoder(nn.Module):
    def __init__(self, config):
        super(LSTMEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=False, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(1, self.bsize,
            self.enc_lstm_dim).zero_()).cuda()

    def forward(self, sent_tuple):
        # sent_len [max_len, ..., min_len] (batch) | sent Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(1, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed, (self.init_lstm,
                      self.init_lstm))[1][0].squeeze(0)  # batch x 2*nhid

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        emb = sent_output.index_select(0, Variable(torch.cuda.LongTensor(idx_unsort)))

        return emb


&#34;&#34;&#34;
GRU encoder
&#34;&#34;&#34;


class GRUEncoder(nn.Module):
    def __init__(self, config):
        super(GRUEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim =  config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.GRU(self.word_emb_dim, self.enc_lstm_dim, 1,
                               bidirectional=False, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(1, self.bsize,
            self.enc_lstm_dim).zero_()).cuda()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(1, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)

        sent_output = self.enc_lstm(sent_packed, self.init_lstm)[1].squeeze(0)
        # batch x 2*nhid

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        emb = sent_output.index_select(0, Variable(torch.cuda.LongTensor(idx_unsort)))

        return emb


&#34;&#34;&#34;
Inner attention from &#34;hierarchical attention for document classification&#34;
&#34;&#34;&#34;


class InnerAttentionNAACLEncoder(nn.Module):
    def __init__(self, config):
        super(InnerAttentionNAACLEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]


        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()

        self.proj_key = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=False)
        self.proj_lstm = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                   bias=False)
        self.query_embedding = nn.Embedding(1, 2*self.enc_lstm_dim)
        self.softmax = nn.Softmax()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))
        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]
        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1, Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = sent_output.transpose(0,1).contiguous()

        sent_output_proj = self.proj_lstm(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_key_proj = self.proj_key(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_key_proj = torch.tanh(sent_key_proj)
        # NAACL paper: u_it=tanh(W_w.h_it + b_w)  (bsize, seqlen, 2nhid)

        sent_w = self.query_embedding(Variable(torch.LongTensor(bsize*[0]).cuda())).unsqueeze(2) #(bsize, 2*nhid, 1)

        Temp = 2
        keys = sent_key_proj.bmm(sent_w).squeeze(2) / Temp

        # Set probas of padding to zero in softmax
        keys = keys + ((keys == 0).float()*-10000)

        alphas = self.softmax(keys/Temp).unsqueeze(2).expand_as(sent_output)
        if int(time.time()) % 100 == 0:
            print(&#39;w&#39;, torch.max(sent_w), torch.min(sent_w))
            print(&#39;alphas&#39;, alphas[0, :, 0])
        emb = torch.sum(alphas * sent_output_proj, 1).squeeze(1)

        return emb


&#34;&#34;&#34;
Inner attention inspired from &#34;Self-attentive ...&#34;
&#34;&#34;&#34;


class InnerAttentionMILAEncoder(nn.Module):
    def __init__(self, config):
        super(InnerAttentionMILAEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim =  config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()

        self.proj_key = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=False)
        self.proj_lstm = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                   bias=False)
        self.query_embedding = nn.Embedding(2, 2*self.enc_lstm_dim)
        self.softmax = nn.Softmax()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))
        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]
        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1,
            Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = sent_output.transpose(0,1).contiguous()
        sent_output_proj = self.proj_lstm(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)
        sent_key_proj = self.proj_key(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)
        sent_key_proj = torch.tanh(sent_key_proj)
        # NAACL : u_it=tanh(W_w.h_it + b_w) like in NAACL paper

        # Temperature
        Temp = 3

        sent_w1 = self.query_embedding(Variable(torch.LongTensor(bsize*[0]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys1 = sent_key_proj.bmm(sent_w1).squeeze(2) / Temp
        keys1 = keys1 + ((keys1 == 0).float()*-1000)
        alphas1 = self.softmax(keys1).unsqueeze(2).expand_as(sent_key_proj)
        emb1 = torch.sum(alphas1 * sent_output_proj, 1).squeeze(1)


        sent_w2 = self.query_embedding(Variable(torch.LongTensor(bsize*[1]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys2 = sent_key_proj.bmm(sent_w2).squeeze(2) / Temp
        keys2 = keys2 + ((keys2 == 0).float()*-1000)
        alphas2 = self.softmax(keys2).unsqueeze(2).expand_as(sent_key_proj)
        emb2 = torch.sum(alphas2 * sent_output_proj, 1).squeeze(1)

        sent_w3 = self.query_embedding(Variable(torch.LongTensor(bsize*[1]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys3 = sent_key_proj.bmm(sent_w3).squeeze(2) / Temp
        keys3 = keys3 + ((keys3 == 0).float()*-1000)
        alphas3 = self.softmax(keys3).unsqueeze(2).expand_as(sent_key_proj)
        emb3 = torch.sum(alphas3 * sent_output_proj, 1).squeeze(1)

        sent_w4 = self.query_embedding(Variable(torch.LongTensor(bsize*[1]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys4 = sent_key_proj.bmm(sent_w4).squeeze(2) / Temp
        keys4 = keys4 + ((keys4 == 0).float()*-1000)
        alphas4 = self.softmax(keys4).unsqueeze(2).expand_as(sent_key_proj)
        emb4 = torch.sum(alphas4 * sent_output_proj, 1).squeeze(1)


        if int(time.time()) % 100 == 0:
            print(&#39;alphas&#39;, torch.cat((alphas1.data[0, :, 0],
                                       alphas2.data[0, :, 0],
                                       torch.abs(alphas1.data[0, :, 0] -
                                                 alphas2.data[0, :, 0])), 1))

        emb = torch.cat((emb1, emb2, emb3, emb4), 1)
        return emb


&#34;&#34;&#34;
Inner attention from Yang et al.
&#34;&#34;&#34;


class InnerAttentionYANGEncoder(nn.Module):
    def __init__(self, config):
        super(InnerAttentionYANGEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
            self.enc_lstm_dim).zero_()).cuda()

        self.proj_lstm = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                   bias=True)
        self.proj_query = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                    bias=True)
        self.proj_enc = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=True)

        self.query_embedding = nn.Embedding(1, 2*self.enc_lstm_dim)
        self.softmax = nn.Softmax()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))
        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]
        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1,
            Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = sent_output.transpose(0,1).contiguous()

        sent_output_proj = self.proj_lstm(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_keys = self.proj_enc(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_max = torch.max(sent_output, 1)[0].squeeze(1)  # (bsize, 2*nhid)
        sent_summary = self.proj_query(
                       sent_max).unsqueeze(1).expand_as(sent_keys)
        # (bsize, seqlen, 2*nhid)

        sent_M = torch.tanh(sent_keys + sent_summary)
        # (bsize, seqlen, 2*nhid) YANG : M = tanh(Wh_i + Wh_avg
        sent_w = self.query_embedding(Variable(torch.LongTensor(
            bsize*[0]).cuda())).unsqueeze(2)  # (bsize, 2*nhid, 1)

        sent_alphas = self.softmax(sent_M.bmm(sent_w).squeeze(2)).unsqueeze(1)
        # (bsize, 1, seqlen)

        if int(time.time()) % 200 == 0:
            print(&#39;w&#39;, torch.max(sent_w[0]), torch.min(sent_w[0]))
            print(&#39;alphas&#39;, sent_alphas[0][0][0:sent_len[0]])
        # Get attention vector
        emb = sent_alphas.bmm(sent_output_proj).squeeze(1)

        return emb



&#34;&#34;&#34;
Hierarchical ConvNet
&#34;&#34;&#34;
class ConvNetEncoder(nn.Module):
    def __init__(self, config):
        super(ConvNetEncoder, self).__init__()

        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]

        self.convnet1 = nn.Sequential(
            nn.Conv1d(self.word_emb_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )
        self.convnet2 = nn.Sequential(
            nn.Conv1d(2*self.enc_lstm_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )
        self.convnet3 = nn.Sequential(
            nn.Conv1d(2*self.enc_lstm_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )
        self.convnet4 = nn.Sequential(
            nn.Conv1d(2*self.enc_lstm_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )



    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple

        sent = sent.transpose(0,1).transpose(1,2).contiguous()
        # batch, nhid, seqlen)

        sent = self.convnet1(sent)
        u1 = torch.max(sent, 2)[0]

        sent = self.convnet2(sent)
        u2 = torch.max(sent, 2)[0]

        sent = self.convnet3(sent)
        u3 = torch.max(sent, 2)[0]

        sent = self.convnet4(sent)
        u4 = torch.max(sent, 2)[0]

        emb = torch.cat((u1, u2, u3, u4), 1)

        return emb


&#34;&#34;&#34;
Main module for Natural Language Inference
&#34;&#34;&#34;


class NLINet(nn.Module):
    def __init__(self, config):
        super(NLINet, self).__init__()

        # classifier
        self.nonlinear_fc = config[&#39;nonlinear_fc&#39;]
        self.fc_dim = config[&#39;fc_dim&#39;]
        self.n_classes = config[&#39;n_classes&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.encoder_type = config[&#39;encoder_type&#39;]
        self.dpout_fc = config[&#39;dpout_fc&#39;]

        self.encoder = eval(self.encoder_type)(config)
        self.inputdim = 4*2*self.enc_lstm_dim
        self.inputdim = 4*self.inputdim if self.encoder_type in \
                        [&#34;ConvNetEncoder&#34;, &#34;InnerAttentionMILAEncoder&#34;] else self.inputdim
        self.inputdim = self.inputdim/2 if self.encoder_type == &#34;LSTMEncoder&#34; \
                                        else self.inputdim
        if self.nonlinear_fc:
            self.classifier = nn.Sequential(
                nn.Dropout(p=self.dpout_fc),
                nn.Linear(self.inputdim, self.fc_dim),
                nn.Tanh(),
                nn.Dropout(p=self.dpout_fc),
                nn.Linear(self.fc_dim, self.fc_dim),
                nn.Tanh(),
                nn.Dropout(p=self.dpout_fc),
                nn.Linear(self.fc_dim, self.n_classes),
                )
        else:
            self.classifier = nn.Sequential(
                nn.Linear(self.inputdim, self.fc_dim),
                nn.Linear(self.fc_dim, self.fc_dim),
                nn.Linear(self.fc_dim, self.n_classes)
                )

    def forward(self, s1, s2):
        # s1 : (s1, s1_len)
        u = self.encoder(s1)
        v = self.encoder(s2)

        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)
        output = self.classifier(features)
        return output

    def encode(self, s1):
        emb = self.encoder(s1)
        return emb


&#34;&#34;&#34;
Main module for Classification
&#34;&#34;&#34;


class ClassificationNet(nn.Module):
    def __init__(self, config):
        super(ClassificationNet, self).__init__()

        # classifier
        self.nonlinear_fc = config[&#39;nonlinear_fc&#39;]
        self.fc_dim = config[&#39;fc_dim&#39;]
        self.n_classes = config[&#39;n_classes&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.encoder_type = config[&#39;encoder_type&#39;]
        self.dpout_fc = config[&#39;dpout_fc&#39;]

        self.encoder = eval(self.encoder_type)(config)
        self.inputdim = 2*self.enc_lstm_dim
        self.inputdim = 4*self.inputdim if self.encoder_type == &#34;ConvNetEncoder&#34; else self.inputdim
        self.inputdim = self.enc_lstm_dim if self.encoder_type ==&#34;LSTMEncoder&#34; else self.inputdim
        self.classifier = nn.Sequential(
            nn.Linear(self.inputdim, 512),
            nn.Linear(512, self.n_classes),
        )

    def forward(self, s1):
        # s1 : (s1, s1_len)
        u = self.encoder(s1)

        output = self.classifier(u)
        return output

    def encode(self, s1):
        emb = self.encoder(s1)
        return emb
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls encoder
<span class="o">!</span>cat encoder/models.py
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>demo.ipynb	     infersent1.pkl  models.py	  README.md
extract_features.py  infersent2.pkl  __pycache__  samples.txt
# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

&#34;&#34;&#34;
This file contains the definition of encoders used in https://arxiv.org/pdf/1705.02364.pdf
&#34;&#34;&#34;

import numpy as np
import time

import torch
from torch.autograd import Variable
import torch.nn as nn

&#34;&#34;&#34;
BLSTM (max/mean) encoder
&#34;&#34;&#34;

class InferSent(nn.Module):

    def __init__(self, config):
        super(InferSent, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]
        self.version = 1 if &#39;version&#39; not in config else config[&#39;version&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True, dropout=self.dpout_model)

        assert self.version in [1, 2]
        if self.version == 1:
            self.bos = &#39;&lt;s&gt;&#39;
            self.eos = &#39;&lt;/s&gt;&#39;
            self.max_pad = True
            self.moses_tok = False
        elif self.version == 2:
            self.bos = &#39;&lt;p&gt;&#39;
            self.eos = &#39;&lt;/p&gt;&#39;
            self.max_pad = False
            self.moses_tok = True

    def is_cuda(self):
        # either all weights are on cpu or they are on gpu
        return &#39;cuda&#39; in str(type(self.enc_lstm.bias_hh_l0.data))

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (bsize)
        # sent: Variable(seqlen x bsize x worddim)
        sent, sent_len = sent_tuple

        # Sort by length (keep idx)
        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        idx_unsort = np.argsort(idx_sort)

        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \
            else torch.from_numpy(idx_sort)
        sent = sent.index_select(1, Variable(idx_sort))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)
        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]

        # Un-sort by length
        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \
            else torch.from_numpy(idx_unsort)
        sent_output = sent_output.index_select(1, Variable(idx_unsort))

        # Pooling
        if self.pool_type == &#34;mean&#34;:
            sent_len = Variable(torch.FloatTensor(sent_len.copy())).unsqueeze(1).cuda()
            emb = torch.sum(sent_output, 0).squeeze(0)
            emb = emb / sent_len.expand_as(emb)
        elif self.pool_type == &#34;max&#34;:
            if not self.max_pad:
                sent_output[sent_output == 0] = -1e9
            emb = torch.max(sent_output, 0)[0]
            if emb.ndimension() == 3:
                emb = emb.squeeze(0)
                assert emb.ndimension() == 2

        return emb

    def set_w2v_path(self, w2v_path):
        self.w2v_path = w2v_path

    def get_word_dict(self, sentences, tokenize=True):
        # create vocab of words
        word_dict = {}
        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]
        for sent in sentences:
            for word in sent:
                if word not in word_dict:
                    word_dict[word] = &#39;&#39;
        word_dict[self.bos] = &#39;&#39;
        word_dict[self.eos] = &#39;&#39;
        return word_dict

    def get_w2v(self, word_dict):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        # create word_vec with w2v vectors
        word_vec = {}
        with open(self.w2v_path) as f:
            for line in f:
                word, vec = line.split(&#39; &#39;, 1)
                if word in word_dict:
                    word_vec[word] = np.fromstring(vec, sep=&#39; &#39;)
        print(&#39;Found %s(/%s) words with w2v vectors&#39; % (len(word_vec), len(word_dict)))
        return word_vec

    def get_w2v_k(self, K):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        # create word_vec with k first w2v vectors
        k = 0
        word_vec = {}
        with open(self.w2v_path) as f:
            for line in f:
                word, vec = line.split(&#39; &#39;, 1)
                if k &lt;= K:
                    word_vec[word] = np.fromstring(vec, sep=&#39; &#39;)
                    k += 1
                if k &gt; K:
                    if word in [self.bos, self.eos]:
                        word_vec[word] = np.fromstring(vec, sep=&#39; &#39;)

                if k &gt; K and all([w in word_vec for w in [self.bos, self.eos]]):
                    break
        return word_vec

    def build_vocab(self, sentences, tokenize=True):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        word_dict = self.get_word_dict(sentences, tokenize)
        self.word_vec = self.get_w2v(word_dict)
        print(&#39;Vocab size : %s&#39; % (len(self.word_vec)))

    # build w2v vocab with k most frequent words
    def build_vocab_k_words(self, K):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;w2v path not set&#39;
        self.word_vec = self.get_w2v_k(K)
        print(&#39;Vocab size : %s&#39; % (K))

    def update_vocab(self, sentences, tokenize=True):
        assert hasattr(self, &#39;w2v_path&#39;), &#39;warning : w2v path not set&#39;
        assert hasattr(self, &#39;word_vec&#39;), &#39;build_vocab before updating it&#39;
        word_dict = self.get_word_dict(sentences, tokenize)

        # keep only new words
        for word in self.word_vec:
            if word in word_dict:
                del word_dict[word]

        # udpate vocabulary
        if word_dict:
            new_word_vec = self.get_w2v(word_dict)
            self.word_vec.update(new_word_vec)
        else:
            new_word_vec = []
        print(&#39;New vocab size : %s (added %s words)&#39;% (len(self.word_vec), len(new_word_vec)))

    def get_batch(self, batch):
        # sent in batch in decreasing order of lengths
        # batch: (bsize, max_len, word_dim)
        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))

        for i in range(len(batch)):
            for j in range(len(batch[i])):
                embed[j, i, :] = self.word_vec[batch[i][j]]

        return torch.FloatTensor(embed)

    def tokenize(self, s):
        from nltk.tokenize import word_tokenize
        if self.moses_tok:
            s = &#39; &#39;.join(word_tokenize(s))
            s = s.replace(&#34; n&#39;t &#34;, &#34;n &#39;t &#34;)  # HACK to get ~MOSES tokenization
            return s.split()
        else:
            return word_tokenize(s)

    def prepare_samples(self, sentences, bsize, tokenize, verbose):
        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else
                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]
        n_w = np.sum([len(x) for x in sentences])

        # filters words without w2v vectors
        for i in range(len(sentences)):
            s_f = [word for word in sentences[i] if word in self.word_vec]
            if not s_f:
                import warnings
                warnings.warn(&#39;No words in &#34;%s&#34; (idx=%s) have w2v vectors. \
                               Replacing by &#34;&lt;/s&gt;&#34;..&#39; % (sentences[i], i))
                s_f = [self.eos]
            sentences[i] = s_f

        lengths = np.array([len(s) for s in sentences])
        n_wk = np.sum(lengths)
        if verbose:
            print(&#39;Nb words kept : %s/%s (%.1f%s)&#39; % (
                        n_wk, n_w, 100.0 * n_wk / n_w, &#39;%&#39;))

        # sort by decreasing length
        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)
        sentences = np.array(sentences)[idx_sort]

        return sentences, lengths, idx_sort

    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):
        tic = time.time()
        sentences, lengths, idx_sort = self.prepare_samples(
                        sentences, bsize, tokenize, verbose)

        embeddings = []
        for stidx in range(0, len(sentences), bsize):
            batch = Variable(self.get_batch(
                        sentences[stidx:stidx + bsize]), volatile=True)
            if self.is_cuda():
                batch = batch.cuda()
            batch = self.forward(
                (batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()
            embeddings.append(batch)
        embeddings = np.vstack(embeddings)

        # unsort
        idx_unsort = np.argsort(idx_sort)
        embeddings = embeddings[idx_unsort]

        if verbose:
            print(&#39;Speed : %.1f sentences/s (%s mode, bsize=%s)&#39; % (
                    len(embeddings)/(time.time()-tic),
                    &#39;gpu&#39; if self.is_cuda() else &#39;cpu&#39;, bsize))
        return embeddings

    def visualize(self, sent, tokenize=True):

        sent = sent.split() if not tokenize else self.tokenize(sent)
        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]

        if &#39; &#39;.join(sent[0]) == &#39;%s %s&#39; % (self.bos, self.eos):
            import warnings
            warnings.warn(&#39;No words in &#34;%s&#34; have w2v vectors. Replacing \
                           by &#34;%s %s&#34;..&#39; % (sent, self.bos, self.eos))
        batch = Variable(self.get_batch(sent), volatile=True)

        if self.is_cuda():
            batch = batch.cuda()
        output = self.enc_lstm(batch)[0]
        output, idxs = torch.max(output, 0)
        # output, idxs = output.squeeze(), idxs.squeeze()
        idxs = idxs.data.cpu().numpy()
        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]

        # visualize model
        import matplotlib.pyplot as plt
        x = range(len(sent[0]))
        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]
        plt.xticks(x, sent[0], rotation=45)
        plt.bar(x, y)
        plt.ylabel(&#39;%&#39;)
        plt.title(&#39;Visualisation of words importance&#39;)
        plt.show()

        return output, idxs

&#34;&#34;&#34;
BiGRU encoder (first/last hidden states)
&#34;&#34;&#34;


class BGRUlastEncoder(nn.Module):
    def __init__(self, config):
        super(BGRUlastEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.GRU(self.word_emb_dim, self.enc_lstm_dim, 1,
                               bidirectional=True, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        _, hn = self.enc_lstm(sent_packed, self.init_lstm)
        emb = torch.cat((hn[0], hn[1]), 1)  # batch x 2*nhid

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        emb = emb.index_select(0, Variable(torch.cuda.LongTensor(idx_unsort)))

        return emb


&#34;&#34;&#34;
BLSTM encoder with projection after BiLSTM
&#34;&#34;&#34;


class BLSTMprojEncoder(nn.Module):
    def __init__(self, config):
        super(BLSTMprojEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()
        self.proj_enc = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=False)

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1,
                Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = self.proj_enc(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(-1, bsize, 2*self.enc_lstm_dim)
        # Pooling
        if self.pool_type == &#34;mean&#34;:
            sent_len = Variable(torch.FloatTensor(sent_len)).unsqueeze(1).cuda()
            emb = torch.sum(sent_output, 0).squeeze(0)
            emb = emb / sent_len.expand_as(emb)
        elif self.pool_type == &#34;max&#34;:
            emb = torch.max(sent_output, 0)[0].squeeze(0)

        return emb


&#34;&#34;&#34;
LSTM encoder
&#34;&#34;&#34;


class LSTMEncoder(nn.Module):
    def __init__(self, config):
        super(LSTMEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=False, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(1, self.bsize,
            self.enc_lstm_dim).zero_()).cuda()

    def forward(self, sent_tuple):
        # sent_len [max_len, ..., min_len] (batch) | sent Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(1, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed, (self.init_lstm,
                      self.init_lstm))[1][0].squeeze(0)  # batch x 2*nhid

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        emb = sent_output.index_select(0, Variable(torch.cuda.LongTensor(idx_unsort)))

        return emb


&#34;&#34;&#34;
GRU encoder
&#34;&#34;&#34;


class GRUEncoder(nn.Module):
    def __init__(self, config):
        super(GRUEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim =  config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]
        self.dpout_model = config[&#39;dpout_model&#39;]

        self.enc_lstm = nn.GRU(self.word_emb_dim, self.enc_lstm_dim, 1,
                               bidirectional=False, dropout=self.dpout_model)
        self.init_lstm = Variable(torch.FloatTensor(1, self.bsize,
            self.enc_lstm_dim).zero_()).cuda()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(1, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))

        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)

        sent_output = self.enc_lstm(sent_packed, self.init_lstm)[1].squeeze(0)
        # batch x 2*nhid

        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        emb = sent_output.index_select(0, Variable(torch.cuda.LongTensor(idx_unsort)))

        return emb


&#34;&#34;&#34;
Inner attention from &#34;hierarchical attention for document classification&#34;
&#34;&#34;&#34;


class InnerAttentionNAACLEncoder(nn.Module):
    def __init__(self, config):
        super(InnerAttentionNAACLEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]


        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()

        self.proj_key = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=False)
        self.proj_lstm = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                   bias=False)
        self.query_embedding = nn.Embedding(1, 2*self.enc_lstm_dim)
        self.softmax = nn.Softmax()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))
        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]
        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1, Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = sent_output.transpose(0,1).contiguous()

        sent_output_proj = self.proj_lstm(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_key_proj = self.proj_key(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_key_proj = torch.tanh(sent_key_proj)
        # NAACL paper: u_it=tanh(W_w.h_it + b_w)  (bsize, seqlen, 2nhid)

        sent_w = self.query_embedding(Variable(torch.LongTensor(bsize*[0]).cuda())).unsqueeze(2) #(bsize, 2*nhid, 1)

        Temp = 2
        keys = sent_key_proj.bmm(sent_w).squeeze(2) / Temp

        # Set probas of padding to zero in softmax
        keys = keys + ((keys == 0).float()*-10000)

        alphas = self.softmax(keys/Temp).unsqueeze(2).expand_as(sent_output)
        if int(time.time()) % 100 == 0:
            print(&#39;w&#39;, torch.max(sent_w), torch.min(sent_w))
            print(&#39;alphas&#39;, alphas[0, :, 0])
        emb = torch.sum(alphas * sent_output_proj, 1).squeeze(1)

        return emb


&#34;&#34;&#34;
Inner attention inspired from &#34;Self-attentive ...&#34;
&#34;&#34;&#34;


class InnerAttentionMILAEncoder(nn.Module):
    def __init__(self, config):
        super(InnerAttentionMILAEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim =  config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
                                  self.enc_lstm_dim).zero_()).cuda()

        self.proj_key = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=False)
        self.proj_lstm = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                   bias=False)
        self.query_embedding = nn.Embedding(2, 2*self.enc_lstm_dim)
        self.softmax = nn.Softmax()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))
        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]
        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1,
            Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = sent_output.transpose(0,1).contiguous()
        sent_output_proj = self.proj_lstm(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)
        sent_key_proj = self.proj_key(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)
        sent_key_proj = torch.tanh(sent_key_proj)
        # NAACL : u_it=tanh(W_w.h_it + b_w) like in NAACL paper

        # Temperature
        Temp = 3

        sent_w1 = self.query_embedding(Variable(torch.LongTensor(bsize*[0]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys1 = sent_key_proj.bmm(sent_w1).squeeze(2) / Temp
        keys1 = keys1 + ((keys1 == 0).float()*-1000)
        alphas1 = self.softmax(keys1).unsqueeze(2).expand_as(sent_key_proj)
        emb1 = torch.sum(alphas1 * sent_output_proj, 1).squeeze(1)


        sent_w2 = self.query_embedding(Variable(torch.LongTensor(bsize*[1]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys2 = sent_key_proj.bmm(sent_w2).squeeze(2) / Temp
        keys2 = keys2 + ((keys2 == 0).float()*-1000)
        alphas2 = self.softmax(keys2).unsqueeze(2).expand_as(sent_key_proj)
        emb2 = torch.sum(alphas2 * sent_output_proj, 1).squeeze(1)

        sent_w3 = self.query_embedding(Variable(torch.LongTensor(bsize*[1]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys3 = sent_key_proj.bmm(sent_w3).squeeze(2) / Temp
        keys3 = keys3 + ((keys3 == 0).float()*-1000)
        alphas3 = self.softmax(keys3).unsqueeze(2).expand_as(sent_key_proj)
        emb3 = torch.sum(alphas3 * sent_output_proj, 1).squeeze(1)

        sent_w4 = self.query_embedding(Variable(torch.LongTensor(bsize*[1]).cuda())).unsqueeze(2) #(bsize, nhid, 1)
        keys4 = sent_key_proj.bmm(sent_w4).squeeze(2) / Temp
        keys4 = keys4 + ((keys4 == 0).float()*-1000)
        alphas4 = self.softmax(keys4).unsqueeze(2).expand_as(sent_key_proj)
        emb4 = torch.sum(alphas4 * sent_output_proj, 1).squeeze(1)


        if int(time.time()) % 100 == 0:
            print(&#39;alphas&#39;, torch.cat((alphas1.data[0, :, 0],
                                       alphas2.data[0, :, 0],
                                       torch.abs(alphas1.data[0, :, 0] -
                                                 alphas2.data[0, :, 0])), 1))

        emb = torch.cat((emb1, emb2, emb3, emb4), 1)
        return emb


&#34;&#34;&#34;
Inner attention from Yang et al.
&#34;&#34;&#34;


class InnerAttentionYANGEncoder(nn.Module):
    def __init__(self, config):
        super(InnerAttentionYANGEncoder, self).__init__()
        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]

        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,
                                bidirectional=True)
        self.init_lstm = Variable(torch.FloatTensor(2, self.bsize,
            self.enc_lstm_dim).zero_()).cuda()

        self.proj_lstm = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                   bias=True)
        self.proj_query = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                    bias=True)
        self.proj_enc = nn.Linear(2*self.enc_lstm_dim, 2*self.enc_lstm_dim,
                                  bias=True)

        self.query_embedding = nn.Embedding(1, 2*self.enc_lstm_dim)
        self.softmax = nn.Softmax()

    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple
        bsize = sent.size(1)

        self.init_lstm = self.init_lstm if bsize == self.init_lstm.size(1) else \
                Variable(torch.FloatTensor(2, bsize, self.enc_lstm_dim).zero_()).cuda()

        # Sort by length (keep idx)
        sent_len, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)
        sent = sent.index_select(1, Variable(torch.cuda.LongTensor(idx_sort)))
        # Handling padding in Recurrent Networks
        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len)
        sent_output = self.enc_lstm(sent_packed,
                                    (self.init_lstm, self.init_lstm))[0]
        # seqlen x batch x 2*nhid
        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]
        # Un-sort by length
        idx_unsort = np.argsort(idx_sort)
        sent_output = sent_output.index_select(1,
            Variable(torch.cuda.LongTensor(idx_unsort)))

        sent_output = sent_output.transpose(0,1).contiguous()

        sent_output_proj = self.proj_lstm(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_keys = self.proj_enc(sent_output.view(-1,
            2*self.enc_lstm_dim)).view(bsize, -1, 2*self.enc_lstm_dim)

        sent_max = torch.max(sent_output, 1)[0].squeeze(1)  # (bsize, 2*nhid)
        sent_summary = self.proj_query(
                       sent_max).unsqueeze(1).expand_as(sent_keys)
        # (bsize, seqlen, 2*nhid)

        sent_M = torch.tanh(sent_keys + sent_summary)
        # (bsize, seqlen, 2*nhid) YANG : M = tanh(Wh_i + Wh_avg
        sent_w = self.query_embedding(Variable(torch.LongTensor(
            bsize*[0]).cuda())).unsqueeze(2)  # (bsize, 2*nhid, 1)

        sent_alphas = self.softmax(sent_M.bmm(sent_w).squeeze(2)).unsqueeze(1)
        # (bsize, 1, seqlen)

        if int(time.time()) % 200 == 0:
            print(&#39;w&#39;, torch.max(sent_w[0]), torch.min(sent_w[0]))
            print(&#39;alphas&#39;, sent_alphas[0][0][0:sent_len[0]])
        # Get attention vector
        emb = sent_alphas.bmm(sent_output_proj).squeeze(1)

        return emb



&#34;&#34;&#34;
Hierarchical ConvNet
&#34;&#34;&#34;
class ConvNetEncoder(nn.Module):
    def __init__(self, config):
        super(ConvNetEncoder, self).__init__()

        self.bsize = config[&#39;bsize&#39;]
        self.word_emb_dim = config[&#39;word_emb_dim&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.pool_type = config[&#39;pool_type&#39;]

        self.convnet1 = nn.Sequential(
            nn.Conv1d(self.word_emb_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )
        self.convnet2 = nn.Sequential(
            nn.Conv1d(2*self.enc_lstm_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )
        self.convnet3 = nn.Sequential(
            nn.Conv1d(2*self.enc_lstm_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )
        self.convnet4 = nn.Sequential(
            nn.Conv1d(2*self.enc_lstm_dim, 2*self.enc_lstm_dim, kernel_size=3,
                      stride=1, padding=1),
            nn.ReLU(inplace=True),
            )



    def forward(self, sent_tuple):
        # sent_len: [max_len, ..., min_len] (batch)
        # sent: Variable(seqlen x batch x worddim)

        sent, sent_len = sent_tuple

        sent = sent.transpose(0,1).transpose(1,2).contiguous()
        # batch, nhid, seqlen)

        sent = self.convnet1(sent)
        u1 = torch.max(sent, 2)[0]

        sent = self.convnet2(sent)
        u2 = torch.max(sent, 2)[0]

        sent = self.convnet3(sent)
        u3 = torch.max(sent, 2)[0]

        sent = self.convnet4(sent)
        u4 = torch.max(sent, 2)[0]

        emb = torch.cat((u1, u2, u3, u4), 1)

        return emb


&#34;&#34;&#34;
Main module for Natural Language Inference
&#34;&#34;&#34;


class NLINet(nn.Module):
    def __init__(self, config):
        super(NLINet, self).__init__()

        # classifier
        self.nonlinear_fc = config[&#39;nonlinear_fc&#39;]
        self.fc_dim = config[&#39;fc_dim&#39;]
        self.n_classes = config[&#39;n_classes&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.encoder_type = config[&#39;encoder_type&#39;]
        self.dpout_fc = config[&#39;dpout_fc&#39;]

        self.encoder = eval(self.encoder_type)(config)
        self.inputdim = 4*2*self.enc_lstm_dim
        self.inputdim = 4*self.inputdim if self.encoder_type in \
                        [&#34;ConvNetEncoder&#34;, &#34;InnerAttentionMILAEncoder&#34;] else self.inputdim
        self.inputdim = self.inputdim/2 if self.encoder_type == &#34;LSTMEncoder&#34; \
                                        else self.inputdim
        if self.nonlinear_fc:
            self.classifier = nn.Sequential(
                nn.Dropout(p=self.dpout_fc),
                nn.Linear(self.inputdim, self.fc_dim),
                nn.Tanh(),
                nn.Dropout(p=self.dpout_fc),
                nn.Linear(self.fc_dim, self.fc_dim),
                nn.Tanh(),
                nn.Dropout(p=self.dpout_fc),
                nn.Linear(self.fc_dim, self.n_classes),
                )
        else:
            self.classifier = nn.Sequential(
                nn.Linear(self.inputdim, self.fc_dim),
                nn.Linear(self.fc_dim, self.fc_dim),
                nn.Linear(self.fc_dim, self.n_classes)
                )

    def forward(self, s1, s2):
        # s1 : (s1, s1_len)
        u = self.encoder(s1)
        v = self.encoder(s2)

        features = torch.cat((u, v, torch.abs(u-v), u*v), 1)
        output = self.classifier(features)
        return output

    def encode(self, s1):
        emb = self.encoder(s1)
        return emb


&#34;&#34;&#34;
Main module for Classification
&#34;&#34;&#34;


class ClassificationNet(nn.Module):
    def __init__(self, config):
        super(ClassificationNet, self).__init__()

        # classifier
        self.nonlinear_fc = config[&#39;nonlinear_fc&#39;]
        self.fc_dim = config[&#39;fc_dim&#39;]
        self.n_classes = config[&#39;n_classes&#39;]
        self.enc_lstm_dim = config[&#39;enc_lstm_dim&#39;]
        self.encoder_type = config[&#39;encoder_type&#39;]
        self.dpout_fc = config[&#39;dpout_fc&#39;]

        self.encoder = eval(self.encoder_type)(config)
        self.inputdim = 2*self.enc_lstm_dim
        self.inputdim = 4*self.inputdim if self.encoder_type == &#34;ConvNetEncoder&#34; else self.inputdim
        self.inputdim = self.enc_lstm_dim if self.encoder_type ==&#34;LSTMEncoder&#34; else self.inputdim
        self.classifier = nn.Sequential(
            nn.Linear(self.inputdim, 512),
            nn.Linear(512, self.n_classes),
        )

    def forward(self, s1):
        # s1 : (s1, s1_len)
        u = self.encoder(s1)

        output = self.classifier(u)
        return output

    def encode(self, s1):
        emb = self.encoder(s1)
        return emb
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>data.py  encoder  models.py  mutils.py	  README.md  train_nli.py
dataset  LICENSE  MultiNLI   __pycache__  SNLI	     Untitled.ipynb
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="k">import</span> <span class="n">InferSent</span>
<span class="n">V</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s1">&#39;encoder/infersent</span><span class="si">%s</span><span class="s1">.pkl&#39;</span> <span class="o">%</span> <span class="n">V</span>
<span class="n">params_model</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;bsize&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;word_emb_dim&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span> <span class="s1">&#39;enc_lstm_dim&#39;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
                <span class="s1">&#39;pool_type&#39;</span><span class="p">:</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;dpout_model&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;version&#39;</span><span class="p">:</span> <span class="n">V</span><span class="p">}</span>
<span class="n">infersent</span> <span class="o">=</span> <span class="n">InferSent</span><span class="p">(</span><span class="n">params_model</span><span class="p">)</span>
<span class="n">infersent</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">infersent</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[12]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>models.InferSent</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#train_nli.py LSTM</span>
<span class="o">!</span> python train_nli.py
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
togrep : []

Namespace(batch_size=64, decay=0.99, dpout_fc=0.0, dpout_model=0.0, enc_lstm_dim=2048, encoder_type=&#39;LSTMEncoder&#39;, fc_dim=512, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=3, n_enc_layers=1, n_epochs=20, nlipath=&#39;/home/dc/InferSent/dataset/SNLI&#39;, nonlinear_fc=0, optimizer=&#39;sgd,lr=0.1&#39;, outputdir=&#39;savedir/&#39;, outputmodelname=&#39;model.pickle&#39;, pool_type=&#39;max&#39;, seed=1234)
** TRAIN DATA : Found 549367 pairs of train sentences.
** DEV DATA : Found 9842 pairs of dev sentences.
** TEST DATA : Found 9824 pairs of test sentences.
Found 38957(/43479) words with glove vectors
Vocab size : 38957
self.inputdim:8192, self.fc_dim:512
&lt;class &#39;int&#39;&gt; &lt;class &#39;int&#39;&gt;
NLINet(
  (encoder): LSTMEncoder(
    (enc_lstm): LSTM(300, 2048)
  )
  (classifier): Sequential(
    (0): Linear(in_features=8192, out_features=512, bias=True)
    (1): Linear(in_features=512, out_features=512, bias=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
  )
)

TRAINING : Epoch 1
Learning rate : 0.1
&lt;class &#39;torch.Tensor&#39;&gt; tensor(2128) 2128
6336 ; loss 1.1 accuracy:33.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4314) 4314
12736 ; loss 1.1 accuracy:33.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6426) 6426
19136 ; loss 1.1 accuracy:33.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(8534) 8534
25536 ; loss 1.1 accuracy:33.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10663) 10663
31936 ; loss 1.1 accuracy:33.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12849) 12849
38336 ; loss 1.1 accuracy:33.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(14938) 14938
44736 ; loss 1.1 accuracy:33.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(17105) 17105
51136 ; loss 1.1 accuracy:33.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19240) 19240
57536 ; loss 1.1 accuracy:33.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(21390) 21390
63936 ; loss 1.1 accuracy:33.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(23523) 23523
70336 ; loss 1.1 accuracy:33.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25652) 25652
76736 ; loss 1.1 accuracy:33.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(27776) 27776
83136 ; loss 1.1 accuracy:33.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(29911) 29911
89536 ; loss 1.1 accuracy:33.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32096) 32096
95936 ; loss 1.1 accuracy:33.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34277) 34277
102336 ; loss 1.1 accuracy:33.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(36428) 36428
108736 ; loss 1.1 accuracy:33.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38579) 38579
115136 ; loss 1.1 accuracy:33.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(40720) 40720
121536 ; loss 1.1 accuracy:33.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(42929) 42929
127936 ; loss 1.1 accuracy:33.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45142) 45142
134336 ; loss 1.1 accuracy:33.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(47326) 47326
140736 ; loss 1.1 accuracy:33.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(49435) 49435
147136 ; loss 1.1 accuracy:33.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51699) 51699
153536 ; loss 1.1 accuracy:33.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(53891) 53891
159936 ; loss 1.1 accuracy:33.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(56120) 56120
166336 ; loss 1.1 accuracy:33.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58372) 58372
172736 ; loss 1.1 accuracy:33.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(60748) 60748
179136 ; loss 1.1 accuracy:33.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(63008) 63008
185536 ; loss 1.1 accuracy:33.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(65258) 65258
191936 ; loss 1.1 accuracy:33.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67577) 67577
198336 ; loss 1.1 accuracy:34.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(69896) 69896
204736 ; loss 1.1 accuracy:34.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(72154) 72154
211136 ; loss 1.1 accuracy:34.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74305) 74305
217536 ; loss 1.1 accuracy:34.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76466) 76466
223936 ; loss 1.1 accuracy:34.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(78613) 78613
230336 ; loss 1.1 accuracy:34.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80805) 80805
236736 ; loss 1.1 accuracy:34.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82944) 82944
243136 ; loss 1.1 accuracy:34.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(85336) 85336
249536 ; loss 1.1 accuracy:34.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87730) 87730
255936 ; loss 1.1 accuracy:34.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90122) 90122
262336 ; loss 1.1 accuracy:34.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92441) 92441
268736 ; loss 1.1 accuracy:34.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(94666) 94666
275136 ; loss 1.1 accuracy:34.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96818) 96818
281536 ; loss 1.1 accuracy:34.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98961) 98961
287936 ; loss 1.1 accuracy:34.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101315) 101315
294336 ; loss 1.1 accuracy:34.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103627) 103627
300736 ; loss 1.1 accuracy:34.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105804) 105804
307136 ; loss 1.1 accuracy:34.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(107935) 107935
313536 ; loss 1.1 accuracy:34.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(110062) 110062
319936 ; loss 1.1 accuracy:34.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112386) 112386
326336 ; loss 1.1 accuracy:34.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114789) 114789
332736 ; loss 1.1 accuracy:34.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117288) 117288
339136 ; loss 1.1 accuracy:34.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119560) 119560
345536 ; loss 1.1 accuracy:34.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121799) 121799
351936 ; loss 1.1 accuracy:34.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124218) 124218
358336 ; loss 1.1 accuracy:34.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(126649) 126649
364736 ; loss 1.1 accuracy:34.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128903) 128903
371136 ; loss 1.1 accuracy:34.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131166) 131166
377536 ; loss 1.1 accuracy:34.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133369) 133369
383936 ; loss 1.1 accuracy:34.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135725) 135725
390336 ; loss 1.1 accuracy:34.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137911) 137911
396736 ; loss 1.1 accuracy:34.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140384) 140384
403136 ; loss 1.1 accuracy:34.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(143016) 143016
409536 ; loss 1.1 accuracy:34.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145525) 145525
415936 ; loss 1.1 accuracy:34.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148171) 148171
422336 ; loss 1.1 accuracy:35.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150575) 150575
428736 ; loss 1.1 accuracy:35.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(153129) 153129
435136 ; loss 1.1 accuracy:35.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155641) 155641
441536 ; loss 1.1 accuracy:35.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(158016) 158016
447936 ; loss 1.1 accuracy:35.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160316) 160316
454336 ; loss 1.1 accuracy:35.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(162898) 162898
460736 ; loss 1.1 accuracy:35.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165507) 165507
467136 ; loss 1.1 accuracy:35.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167747) 167747
473536 ; loss 1.1 accuracy:35.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169931) 169931
479936 ; loss 1.1 accuracy:35.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172205) 172205
486336 ; loss 1.1 accuracy:35.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174410) 174410
492736 ; loss 1.1 accuracy:35.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(176590) 176590
499136 ; loss 1.1 accuracy:35.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178831) 178831
505536 ; loss 1.1 accuracy:35.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(181025) 181025
511936 ; loss 1.1 accuracy:35.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183199) 183199
518336 ; loss 1.1 accuracy:35.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185302) 185302
524736 ; loss 1.1 accuracy:35.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187596) 187596
531136 ; loss 1.1 accuracy:35.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(189927) 189927
537536 ; loss 1.1 accuracy:35.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192313) 192313
543936 ; loss 1.1 accuracy:35.35 ;
results : epoch 1 ; mean accuracy train : 35.36

VALIDATION : Epoch 1
togrep : results : epoch 1 ; mean accuracy valid :              34.71
saving model at epoch 1

TRAINING : Epoch 2
Learning rate : 0.099
&lt;class &#39;torch.Tensor&#39;&gt; tensor(2192) 2192
6336 ; loss 1.1 accuracy:34.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4429) 4429
12736 ; loss 1.1 accuracy:34.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6994) 6994
19136 ; loss 1.1 accuracy:36.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9537) 9537
25536 ; loss 1.1 accuracy:37.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12005) 12005
31936 ; loss 1.1 accuracy:37.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(14411) 14411
38336 ; loss 1.1 accuracy:37.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(16756) 16756
44736 ; loss 1.1 accuracy:37.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19043) 19043
51136 ; loss 1.1 accuracy:37.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(21303) 21303
57536 ; loss 1.1 accuracy:36.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(23875) 23875
63936 ; loss 1.1 accuracy:37.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(26392) 26392
70336 ; loss 1.1 accuracy:37.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28667) 28667
76736 ; loss 1.1 accuracy:37.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(30791) 30791
83136 ; loss 1.1 accuracy:37.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32938) 32938
89536 ; loss 1.1 accuracy:36.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35163) 35163
95936 ; loss 1.1 accuracy:36.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37407) 37407
102336 ; loss 1.1 accuracy:36.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(40018) 40018
108736 ; loss 1.1 accuracy:36.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(42503) 42503
115136 ; loss 1.1 accuracy:36.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45049) 45049
121536 ; loss 1.1 accuracy:37.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(47518) 47518
127936 ; loss 1.1 accuracy:37.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50079) 50079
134336 ; loss 1.1 accuracy:37.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(52622) 52622
140736 ; loss 1.1 accuracy:37.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55176) 55176
147136 ; loss 1.1 accuracy:37.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58083) 58083
153536 ; loss 1.1 accuracy:37.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(60750) 60750
159936 ; loss 1.1 accuracy:37.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(63594) 63594
166336 ; loss 1.1 accuracy:38.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66040) 66040
172736 ; loss 1.1 accuracy:38.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(68478) 68478
179136 ; loss 1.1 accuracy:38.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(71037) 71037
185536 ; loss 1.1 accuracy:38.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73575) 73575
191936 ; loss 1.1 accuracy:38.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76273) 76273
198336 ; loss 1.1 accuracy:38.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(78877) 78877
204736 ; loss 1.1 accuracy:38.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(81324) 81324
211136 ; loss 1.1 accuracy:38.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83619) 83619
217536 ; loss 1.1 accuracy:38.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86033) 86033
223936 ; loss 1.1 accuracy:38.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(88862) 88862
230336 ; loss 1.1 accuracy:38.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(91532) 91532
236736 ; loss 1.1 accuracy:38.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(94380) 94380
243136 ; loss 1.1 accuracy:38.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(97336) 97336
249536 ; loss 1.1 accuracy:39.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(100166) 100166
255936 ; loss 1.1 accuracy:39.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(102988) 102988
262336 ; loss 1.1 accuracy:39.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105886) 105886
268736 ; loss 1.1 accuracy:39.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108779) 108779
275136 ; loss 1.1 accuracy:39.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(111669) 111669
281536 ; loss 1.1 accuracy:39.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114457) 114457
287936 ; loss 1.1 accuracy:39.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(116917) 116917
294336 ; loss 1.1 accuracy:39.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119729) 119729
300736 ; loss 1.1 accuracy:39.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122332) 122332
307136 ; loss 1.1 accuracy:39.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124764) 124764
313536 ; loss 1.1 accuracy:39.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(127290) 127290
319936 ; loss 1.1 accuracy:39.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(130050) 130050
326336 ; loss 1.1 accuracy:39.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133056) 133056
332736 ; loss 1.1 accuracy:39.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(136013) 136013
339136 ; loss 1.1 accuracy:40.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(139025) 139025
345536 ; loss 1.1 accuracy:40.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(142026) 142026
351936 ; loss 1.1 accuracy:40.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144948) 144948
358336 ; loss 1.1 accuracy:40.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(147721) 147721
364736 ; loss 1.1 accuracy:40.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150472) 150472
371136 ; loss 1.1 accuracy:40.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(153506) 153506
377536 ; loss 1.1 accuracy:40.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156260) 156260
383936 ; loss 1.1 accuracy:40.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(159104) 159104
390336 ; loss 1.1 accuracy:40.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161817) 161817
396736 ; loss 1.1 accuracy:40.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164526) 164526
403136 ; loss 1.1 accuracy:40.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167166) 167166
409536 ; loss 1.1 accuracy:40.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170001) 170001
415936 ; loss 1.1 accuracy:40.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172799) 172799
422336 ; loss 1.1 accuracy:40.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(175666) 175666
428736 ; loss 1.1 accuracy:40.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178536) 178536
435136 ; loss 1.1 accuracy:41.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180945) 180945
441536 ; loss 1.1 accuracy:40.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183173) 183173
447936 ; loss 1.1 accuracy:40.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185534) 185534
454336 ; loss 1.1 accuracy:40.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187975) 187975
460736 ; loss 1.1 accuracy:40.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190559) 190559
467136 ; loss 1.1 accuracy:40.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193202) 193202
473536 ; loss 1.1 accuracy:40.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195972) 195972
479936 ; loss 1.1 accuracy:40.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(198706) 198706
486336 ; loss 1.1 accuracy:40.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(201711) 201711
492736 ; loss 1.1 accuracy:40.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204665) 204665
499136 ; loss 1.1 accuracy:41.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(207109) 207109
505536 ; loss 1.1 accuracy:40.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209526) 209526
511936 ; loss 1.1 accuracy:40.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(211989) 211989
518336 ; loss 1.1 accuracy:40.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(214395) 214395
524736 ; loss 1.1 accuracy:40.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216934) 216934
531136 ; loss 1.1 accuracy:40.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219314) 219314
537536 ; loss 1.1 accuracy:40.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(221743) 221743
543936 ; loss 1.1 accuracy:40.76 ;
results : epoch 2 ; mean accuracy train : 40.73

VALIDATION : Epoch 2
togrep : results : epoch 2 ; mean accuracy valid :              42.33
saving model at epoch 2

TRAINING : Epoch 3
Learning rate : 0.09801
&lt;class &#39;torch.Tensor&#39;&gt; tensor(2640) 2640
6336 ; loss 1.1 accuracy:41.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5483) 5483
12736 ; loss 1.1 accuracy:42.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(8497) 8497
19136 ; loss 1.1 accuracy:44.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(11467) 11467
25536 ; loss 1.1 accuracy:44.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(14352) 14352
31936 ; loss 1.1 accuracy:44.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(17142) 17142
38336 ; loss 1.1 accuracy:44.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19950) 19950
44736 ; loss 1.1 accuracy:44.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22693) 22693
51136 ; loss 1.1 accuracy:44.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25239) 25239
57536 ; loss 1.1 accuracy:43.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(27697) 27697
63936 ; loss 1.1 accuracy:43.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(30112) 30112
70336 ; loss 1.1 accuracy:42.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32707) 32707
76736 ; loss 1.1 accuracy:42.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35317) 35317
83136 ; loss 1.1 accuracy:42.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38143) 38143
89536 ; loss 1.1 accuracy:42.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(40928) 40928
95936 ; loss 1.1 accuracy:42.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(43663) 43663
102336 ; loss 1.1 accuracy:42.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46535) 46535
108736 ; loss 1.1 accuracy:42.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(49436) 49436
115136 ; loss 1.1 accuracy:42.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(52537) 52537
121536 ; loss 1.1 accuracy:43.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55684) 55684
127936 ; loss 1.1 accuracy:43.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58654) 58654
134336 ; loss 1.1 accuracy:43.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61660) 61660
140736 ; loss 1.1 accuracy:43.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64768) 64768
147136 ; loss 1.1 accuracy:44.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67767) 67767
153536 ; loss 1.1 accuracy:44.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70872) 70872
159936 ; loss 1.1 accuracy:44.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73897) 73897
166336 ; loss 1.1 accuracy:44.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76760) 76760
172736 ; loss 1.1 accuracy:44.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79449) 79449
179136 ; loss 1.1 accuracy:44.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82067) 82067
185536 ; loss 1.1 accuracy:44.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(84372) 84372
191936 ; loss 1.1 accuracy:43.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86779) 86779
198336 ; loss 1.1 accuracy:43.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(89672) 89672
204736 ; loss 1.1 accuracy:43.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92753) 92753
211136 ; loss 1.1 accuracy:43.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95815) 95815
217536 ; loss 1.1 accuracy:44.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98686) 98686
223936 ; loss 1.1 accuracy:44.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101521) 101521
230336 ; loss 1.1 accuracy:44.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(104164) 104164
236736 ; loss 1.1 accuracy:43.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106664) 106664
243136 ; loss 1.1 accuracy:43.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109494) 109494
249536 ; loss 1.1 accuracy:43.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112417) 112417
255936 ; loss 1.1 accuracy:43.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(115382) 115382
262336 ; loss 1.1 accuracy:43.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(118232) 118232
268736 ; loss 1.1 accuracy:43.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121204) 121204
275136 ; loss 1.1 accuracy:44.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124371) 124371
281536 ; loss 1.1 accuracy:44.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(127488) 127488
287936 ; loss 1.1 accuracy:44.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(130486) 130486
294336 ; loss 1.1 accuracy:44.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133235) 133235
300736 ; loss 1.1 accuracy:44.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(136026) 136026
307136 ; loss 1.1 accuracy:44.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138664) 138664
313536 ; loss 1.1 accuracy:44.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141261) 141261
319936 ; loss 1.1 accuracy:44.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144105) 144105
326336 ; loss 1.1 accuracy:44.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(147068) 147068
332736 ; loss 1.1 accuracy:44.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(149917) 149917
339136 ; loss 1.1 accuracy:44.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(152790) 152790
345536 ; loss 1.1 accuracy:44.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155821) 155821
351936 ; loss 1.1 accuracy:44.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(158802) 158802
358336 ; loss 1.1 accuracy:44.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161626) 161626
364736 ; loss 1.1 accuracy:44.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164532) 164532
371136 ; loss 1.1 accuracy:44.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167516) 167516
377536 ; loss 1.1 accuracy:44.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170632) 170632
383936 ; loss 1.1 accuracy:44.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(173744) 173744
390336 ; loss 1.1 accuracy:44.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(176974) 176974
396736 ; loss 1.1 accuracy:44.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180095) 180095
403136 ; loss 1.1 accuracy:44.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183085) 183085
409536 ; loss 1.1 accuracy:44.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186099) 186099
415936 ; loss 1.1 accuracy:44.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(188999) 188999
422336 ; loss 1.1 accuracy:44.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191974) 191974
428736 ; loss 1.1 accuracy:44.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(194951) 194951
435136 ; loss 1.1 accuracy:44.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197993) 197993
441536 ; loss 1.1 accuracy:44.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(201083) 201083
447936 ; loss 1.1 accuracy:44.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204006) 204006
454336 ; loss 1.1 accuracy:44.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206802) 206802
460736 ; loss 1.1 accuracy:44.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209593) 209593
467136 ; loss 1.1 accuracy:44.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212379) 212379
473536 ; loss 1.1 accuracy:44.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215187) 215187
479936 ; loss 1.1 accuracy:44.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(218058) 218058
486336 ; loss 1.1 accuracy:44.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(221190) 221190
492736 ; loss 1.1 accuracy:44.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(224281) 224281
499136 ; loss 1.1 accuracy:44.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227403) 227403
505536 ; loss 1.1 accuracy:44.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230389) 230389
511936 ; loss 1.1 accuracy:45.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(233431) 233431
518336 ; loss 1.1 accuracy:45.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(236588) 236588
524736 ; loss 1.1 accuracy:45.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239764) 239764
531136 ; loss 1.1 accuracy:45.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(242894) 242894
537536 ; loss 1.1 accuracy:45.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(246020) 246020
543936 ; loss 1.1 accuracy:45.22 ;
results : epoch 3 ; mean accuracy train : 45.26

VALIDATION : Epoch 3
togrep : results : epoch 3 ; mean accuracy valid :              50.13
saving model at epoch 3

TRAINING : Epoch 4
Learning rate : 0.0970299
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3188) 3188
6336 ; loss 1.1 accuracy:49.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6353) 6353
12736 ; loss 1.1 accuracy:49.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9498) 9498
19136 ; loss 1.1 accuracy:49.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12636) 12636
25536 ; loss 1.1 accuracy:49.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15805) 15805
31936 ; loss 1.1 accuracy:49.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(18774) 18774
38336 ; loss 1.1 accuracy:48.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(21754) 21754
44736 ; loss 1.1 accuracy:48.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(24842) 24842
51136 ; loss 1.1 accuracy:48.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(27954) 27954
57536 ; loss 1.1 accuracy:48.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31094) 31094
63936 ; loss 1.1 accuracy:48.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34153) 34153
70336 ; loss 1.1 accuracy:48.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37257) 37257
76736 ; loss 1.1 accuracy:48.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(40311) 40311
83136 ; loss 1.1 accuracy:48.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(43260) 43260
89536 ; loss 1.1 accuracy:48.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46233) 46233
95936 ; loss 1.1 accuracy:48.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(49225) 49225
102336 ; loss 1.1 accuracy:48.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(52255) 52255
108736 ; loss 1.1 accuracy:48.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55342) 55342
115136 ; loss 1.1 accuracy:48.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58471) 58471
121536 ; loss 1.1 accuracy:48.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61693) 61693
127936 ; loss 1.1 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64907) 64907
134336 ; loss 1.1 accuracy:48.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(68092) 68092
140736 ; loss 1.1 accuracy:48.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(71113) 71113
147136 ; loss 1.1 accuracy:48.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74028) 74028
153536 ; loss 1.1 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76961) 76961
159936 ; loss 1.1 accuracy:48.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80069) 80069
166336 ; loss 1.1 accuracy:48.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83320) 83320
172736 ; loss 1.1 accuracy:48.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86380) 86380
179136 ; loss 1.1 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(89548) 89548
185536 ; loss 1.1 accuracy:48.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92540) 92540
191936 ; loss 1.1 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95740) 95740
198336 ; loss 1.1 accuracy:48.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98797) 98797
204736 ; loss 1.1 accuracy:48.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101808) 101808
211136 ; loss 1.1 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(104632) 104632
217536 ; loss 1.1 accuracy:48.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(107503) 107503
223936 ; loss 1.1 accuracy:47.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(110515) 110515
230336 ; loss 1.1 accuracy:47.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(113681) 113681
236736 ; loss 1.1 accuracy:48.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(116839) 116839
243136 ; loss 1.1 accuracy:48.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119944) 119944
249536 ; loss 1.1 accuracy:48.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122919) 122919
255936 ; loss 1.1 accuracy:48.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(125788) 125788
262336 ; loss 1.1 accuracy:47.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128987) 128987
268736 ; loss 1.1 accuracy:47.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(132127) 132127
275136 ; loss 1.1 accuracy:48.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135252) 135252
281536 ; loss 1.1 accuracy:48.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138437) 138437
287936 ; loss 1.1 accuracy:48.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141618) 141618
294336 ; loss 1.1 accuracy:48.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144766) 144766
300736 ; loss 1.1 accuracy:48.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(147928) 147928
307136 ; loss 1.1 accuracy:48.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150996) 150996
313536 ; loss 1.1 accuracy:48.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154096) 154096
319936 ; loss 1.1 accuracy:48.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(157203) 157203
326336 ; loss 1.09 accuracy:48.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160382) 160382
332736 ; loss 1.09 accuracy:48.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163490) 163490
339136 ; loss 1.09 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166585) 166585
345536 ; loss 1.09 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169795) 169795
351936 ; loss 1.09 accuracy:48.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172911) 172911
358336 ; loss 1.09 accuracy:48.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(175963) 175963
364736 ; loss 1.09 accuracy:48.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(179037) 179037
371136 ; loss 1.09 accuracy:48.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182062) 182062
377536 ; loss 1.09 accuracy:48.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185123) 185123
383936 ; loss 1.09 accuracy:48.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(188067) 188067
390336 ; loss 1.09 accuracy:48.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191064) 191064
396736 ; loss 1.09 accuracy:48.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193993) 193993
403136 ; loss 1.09 accuracy:48.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197036) 197036
409536 ; loss 1.09 accuracy:48.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200203) 200203
415936 ; loss 1.09 accuracy:48.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203230) 203230
422336 ; loss 1.09 accuracy:48.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206415) 206415
428736 ; loss 1.09 accuracy:48.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209560) 209560
435136 ; loss 1.09 accuracy:48.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212759) 212759
441536 ; loss 1.09 accuracy:48.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215938) 215938
447936 ; loss 1.09 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219044) 219044
454336 ; loss 1.09 accuracy:48.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222128) 222128
460736 ; loss 1.09 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225139) 225139
467136 ; loss 1.09 accuracy:48.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(228080) 228080
473536 ; loss 1.09 accuracy:48.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(231156) 231156
479936 ; loss 1.09 accuracy:48.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234434) 234434
486336 ; loss 1.09 accuracy:48.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(237620) 237620
492736 ; loss 1.09 accuracy:48.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(240774) 240774
499136 ; loss 1.09 accuracy:48.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243900) 243900
505536 ; loss 1.09 accuracy:48.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(247022) 247022
511936 ; loss 1.09 accuracy:48.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249945) 249945
518336 ; loss 1.09 accuracy:48.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252853) 252853
524736 ; loss 1.09 accuracy:48.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(255604) 255604
531136 ; loss 1.09 accuracy:48.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(258284) 258284
537536 ; loss 1.09 accuracy:48.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261111) 261111
543936 ; loss 1.09 accuracy:48.0 ;
results : epoch 4 ; mean accuracy train : 47.99

VALIDATION : Epoch 4
togrep : results : epoch 4 ; mean accuracy valid :              48.23
Shrinking lr by : 5. New lr = 0.01940598

TRAINING : Epoch 5
Learning rate : 0.0192119202
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3065) 3065
6336 ; loss 1.09 accuracy:47.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6115) 6115
12736 ; loss 1.09 accuracy:47.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9225) 9225
19136 ; loss 1.09 accuracy:48.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12361) 12361
25536 ; loss 1.09 accuracy:48.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15500) 15500
31936 ; loss 1.09 accuracy:48.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(18599) 18599
38336 ; loss 1.09 accuracy:48.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(21781) 21781
44736 ; loss 1.09 accuracy:48.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(24871) 24871
51136 ; loss 1.09 accuracy:48.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28055) 28055
57536 ; loss 1.09 accuracy:48.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31222) 31222
63936 ; loss 1.09 accuracy:48.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34480) 34480
70336 ; loss 1.09 accuracy:48.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37681) 37681
76736 ; loss 1.09 accuracy:49.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(40921) 40921
83136 ; loss 1.09 accuracy:49.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44150) 44150
89536 ; loss 1.09 accuracy:49.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(47448) 47448
95936 ; loss 1.09 accuracy:49.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50691) 50691
102336 ; loss 1.09 accuracy:49.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(53945) 53945
108736 ; loss 1.09 accuracy:49.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57230) 57230
115136 ; loss 1.09 accuracy:49.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(60413) 60413
121536 ; loss 1.09 accuracy:49.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(63613) 63613
127936 ; loss 1.09 accuracy:49.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66797) 66797
134336 ; loss 1.09 accuracy:49.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70009) 70009
140736 ; loss 1.09 accuracy:49.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73172) 73172
147136 ; loss 1.09 accuracy:49.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76339) 76339
153536 ; loss 1.09 accuracy:49.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79507) 79507
159936 ; loss 1.09 accuracy:49.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82666) 82666
166336 ; loss 1.09 accuracy:49.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(85801) 85801
172736 ; loss 1.09 accuracy:49.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(89015) 89015
179136 ; loss 1.09 accuracy:49.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92173) 92173
185536 ; loss 1.09 accuracy:49.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95340) 95340
191936 ; loss 1.09 accuracy:49.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98489) 98489
198336 ; loss 1.09 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101695) 101695
204736 ; loss 1.09 accuracy:49.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(104896) 104896
211136 ; loss 1.09 accuracy:49.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108099) 108099
217536 ; loss 1.09 accuracy:49.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(111332) 111332
223936 ; loss 1.09 accuracy:49.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114586) 114586
230336 ; loss 1.09 accuracy:49.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117845) 117845
236736 ; loss 1.09 accuracy:49.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121088) 121088
243136 ; loss 1.09 accuracy:49.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124318) 124318
249536 ; loss 1.09 accuracy:49.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(127615) 127615
255936 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(130783) 130783
262336 ; loss 1.09 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134047) 134047
268736 ; loss 1.09 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137283) 137283
275136 ; loss 1.09 accuracy:49.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140387) 140387
281536 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(143591) 143591
287936 ; loss 1.09 accuracy:49.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146745) 146745
294336 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(149872) 149872
300736 ; loss 1.09 accuracy:49.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(152994) 152994
307136 ; loss 1.09 accuracy:49.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156107) 156107
313536 ; loss 1.09 accuracy:49.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(159238) 159238
319936 ; loss 1.09 accuracy:49.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(162430) 162430
326336 ; loss 1.09 accuracy:49.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165676) 165676
332736 ; loss 1.09 accuracy:49.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(168827) 168827
339136 ; loss 1.09 accuracy:49.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172065) 172065
345536 ; loss 1.09 accuracy:49.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(175294) 175294
351936 ; loss 1.09 accuracy:49.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178537) 178537
358336 ; loss 1.09 accuracy:49.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(181730) 181730
364736 ; loss 1.09 accuracy:49.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(184934) 184934
371136 ; loss 1.09 accuracy:49.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(188113) 188113
377536 ; loss 1.09 accuracy:49.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191281) 191281
383936 ; loss 1.09 accuracy:49.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(194581) 194581
390336 ; loss 1.09 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197809) 197809
396736 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200998) 200998
403136 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204226) 204226
409536 ; loss 1.09 accuracy:49.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(207345) 207345
415936 ; loss 1.09 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(210534) 210534
422336 ; loss 1.09 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(213698) 213698
428736 ; loss 1.09 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216868) 216868
435136 ; loss 1.09 accuracy:49.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(220093) 220093
441536 ; loss 1.09 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223299) 223299
447936 ; loss 1.09 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(226578) 226578
454336 ; loss 1.09 accuracy:49.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(229762) 229762
460736 ; loss 1.09 accuracy:49.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(232997) 232997
467136 ; loss 1.09 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(236190) 236190
473536 ; loss 1.09 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239403) 239403
479936 ; loss 1.09 accuracy:49.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(242574) 242574
486336 ; loss 1.09 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(245749) 245749
492736 ; loss 1.09 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(248928) 248928
499136 ; loss 1.09 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252106) 252106
505536 ; loss 1.09 accuracy:49.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(255245) 255245
511936 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(258484) 258484
518336 ; loss 1.09 accuracy:49.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261699) 261699
524736 ; loss 1.09 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264817) 264817
531136 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268010) 268010
537536 ; loss 1.09 accuracy:49.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(271193) 271193
543936 ; loss 1.09 accuracy:49.85 ;
results : epoch 5 ; mean accuracy train : 49.85

VALIDATION : Epoch 5
togrep : results : epoch 5 ; mean accuracy valid :              49.93
Shrinking lr by : 5. New lr = 0.0038423840399999997

TRAINING : Epoch 6
Learning rate : 0.0038039601995999996
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3255) 3255
6336 ; loss 1.09 accuracy:50.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6409) 6409
12736 ; loss 1.09 accuracy:50.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9642) 9642
19136 ; loss 1.09 accuracy:50.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12850) 12850
25536 ; loss 1.09 accuracy:50.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(16133) 16133
31936 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19372) 19372
38336 ; loss 1.09 accuracy:50.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22578) 22578
44736 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25773) 25773
51136 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28957) 28957
57536 ; loss 1.09 accuracy:50.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32121) 32121
63936 ; loss 1.09 accuracy:50.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35309) 35309
70336 ; loss 1.09 accuracy:50.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38527) 38527
76736 ; loss 1.09 accuracy:50.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41742) 41742
83136 ; loss 1.09 accuracy:50.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44979) 44979
89536 ; loss 1.09 accuracy:50.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48174) 48174
95936 ; loss 1.09 accuracy:50.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51359) 51359
102336 ; loss 1.09 accuracy:50.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54504) 54504
108736 ; loss 1.09 accuracy:50.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57721) 57721
115136 ; loss 1.09 accuracy:50.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(60922) 60922
121536 ; loss 1.09 accuracy:50.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64135) 64135
127936 ; loss 1.09 accuracy:50.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67338) 67338
134336 ; loss 1.09 accuracy:50.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70514) 70514
140736 ; loss 1.09 accuracy:50.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73680) 73680
147136 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76870) 76870
153536 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80073) 80073
159936 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83302) 83302
166336 ; loss 1.09 accuracy:50.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86564) 86564
172736 ; loss 1.09 accuracy:50.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(89713) 89713
179136 ; loss 1.09 accuracy:50.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92882) 92882
185536 ; loss 1.09 accuracy:50.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96045) 96045
191936 ; loss 1.09 accuracy:50.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(99250) 99250
198336 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(102440) 102440
204736 ; loss 1.09 accuracy:50.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105612) 105612
211136 ; loss 1.09 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108776) 108776
217536 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(111985) 111985
223936 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(115162) 115162
230336 ; loss 1.09 accuracy:49.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(118438) 118438
236736 ; loss 1.09 accuracy:50.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121646) 121646
243136 ; loss 1.09 accuracy:50.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124880) 124880
249536 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128121) 128121
255936 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131364) 131364
262336 ; loss 1.09 accuracy:50.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134519) 134519
268736 ; loss 1.09 accuracy:50.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137707) 137707
275136 ; loss 1.09 accuracy:50.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140884) 140884
281536 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144086) 144086
287936 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(147274) 147274
294336 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150513) 150513
300736 ; loss 1.09 accuracy:50.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(153750) 153750
307136 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156855) 156855
313536 ; loss 1.09 accuracy:50.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160010) 160010
319936 ; loss 1.09 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163174) 163174
326336 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166330) 166330
332736 ; loss 1.09 accuracy:49.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169580) 169580
339136 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172788) 172788
345536 ; loss 1.09 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(176009) 176009
351936 ; loss 1.09 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(179165) 179165
358336 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182338) 182338
364736 ; loss 1.09 accuracy:49.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185508) 185508
371136 ; loss 1.09 accuracy:49.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(188762) 188762
377536 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191979) 191979
383936 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195137) 195137
390336 ; loss 1.09 accuracy:49.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(198370) 198370
396736 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(201576) 201576
403136 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204798) 204798
409536 ; loss 1.09 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(208004) 208004
415936 ; loss 1.09 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(211244) 211244
422336 ; loss 1.09 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(214424) 214424
428736 ; loss 1.09 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(217643) 217643
435136 ; loss 1.09 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(220805) 220805
441536 ; loss 1.09 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223970) 223970
447936 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227156) 227156
454336 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230371) 230371
460736 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(233576) 233576
467136 ; loss 1.09 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(236848) 236848
473536 ; loss 1.09 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(240071) 240071
479936 ; loss 1.09 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243310) 243310
486336 ; loss 1.09 accuracy:50.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(246571) 246571
492736 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249799) 249799
499136 ; loss 1.09 accuracy:50.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(253030) 253030
505536 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(256275) 256275
511936 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(259453) 259453
518336 ; loss 1.09 accuracy:50.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(262556) 262556
524736 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(265722) 265722
531136 ; loss 1.09 accuracy:50.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268948) 268948
537536 ; loss 1.09 accuracy:50.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(272139) 272139
543936 ; loss 1.09 accuracy:50.03 ;
results : epoch 6 ; mean accuracy train : 50.04

VALIDATION : Epoch 6
togrep : results : epoch 6 ; mean accuracy valid :              50.7
saving model at epoch 6

TRAINING : Epoch 7
Learning rate : 0.0037659205976039996
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3289) 3289
6336 ; loss 1.09 accuracy:51.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6510) 6510
12736 ; loss 1.09 accuracy:50.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9695) 9695
19136 ; loss 1.09 accuracy:50.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12861) 12861
25536 ; loss 1.09 accuracy:50.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(16067) 16067
31936 ; loss 1.09 accuracy:50.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19261) 19261
38336 ; loss 1.09 accuracy:50.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22507) 22507
44736 ; loss 1.09 accuracy:50.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25706) 25706
51136 ; loss 1.09 accuracy:50.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28872) 28872
57536 ; loss 1.09 accuracy:50.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32077) 32077
63936 ; loss 1.09 accuracy:50.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35320) 35320
70336 ; loss 1.09 accuracy:50.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38510) 38510
76736 ; loss 1.09 accuracy:50.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41745) 41745
83136 ; loss 1.09 accuracy:50.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44958) 44958
89536 ; loss 1.09 accuracy:50.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48228) 48228
95936 ; loss 1.09 accuracy:50.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51422) 51422
102336 ; loss 1.09 accuracy:50.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54728) 54728
108736 ; loss 1.09 accuracy:50.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57920) 57920
115136 ; loss 1.09 accuracy:50.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61224) 61224
121536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64427) 64427
127936 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67655) 67655
134336 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70907) 70907
140736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74159) 74159
147136 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77347) 77347
153536 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80596) 80596
159936 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83831) 83831
166336 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87061) 87061
172736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90336) 90336
179136 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93557) 93557
185536 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96810) 96810
191936 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(100003) 100003
198336 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103224) 103224
204736 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106438) 106438
211136 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109683) 109683
217536 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112925) 112925
223936 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(116101) 116101
230336 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119299) 119299
236736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122499) 122499
243136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(125687) 125687
249536 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128982) 128982
255936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(132245) 132245
262336 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135418) 135418
268736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138563) 138563
275136 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141831) 141831
281536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145028) 145028
287936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148190) 148190
294336 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151408) 151408
300736 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154622) 154622
307136 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(157760) 157760
313536 ; loss 1.09 accuracy:50.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161037) 161037
319936 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164289) 164289
326336 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167568) 167568
332736 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170807) 170807
339136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174022) 174022
345536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177273) 177273
351936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180520) 180520
358336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183710) 183710
364736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186879) 186879
371136 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190058) 190058
377536 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193210) 193210
383936 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196485) 196485
390336 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(199689) 199689
396736 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(202895) 202895
403136 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206138) 206138
409536 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209415) 209415
415936 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212662) 212662
422336 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215931) 215931
428736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219182) 219182
435136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222427) 222427
441536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225619) 225619
447936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(228795) 228795
454336 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(232065) 232065
460736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235319) 235319
467136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(238518) 238518
473536 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(241723) 241723
479936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244973) 244973
486336 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(248195) 248195
492736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(251430) 251430
499136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254685) 254685
505536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257926) 257926
511936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261118) 261118
518336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264453) 264453
524736 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267702) 267702
531136 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270948) 270948
537536 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(274180) 274180
543936 ; loss 1.09 accuracy:50.4 ;
results : epoch 7 ; mean accuracy train : 50.41

VALIDATION : Epoch 7
togrep : results : epoch 7 ; mean accuracy valid :              50.57
Shrinking lr by : 5. New lr = 0.0007531841195207999

TRAINING : Epoch 8
Learning rate : 0.0007456522783255919
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3225) 3225
6336 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6424) 6424
12736 ; loss 1.09 accuracy:50.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9653) 9653
19136 ; loss 1.09 accuracy:50.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12832) 12832
25536 ; loss 1.09 accuracy:50.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(16040) 16040
31936 ; loss 1.09 accuracy:50.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19237) 19237
38336 ; loss 1.09 accuracy:50.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22513) 22513
44736 ; loss 1.09 accuracy:50.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25729) 25729
51136 ; loss 1.09 accuracy:50.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28982) 28982
57536 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32181) 32181
63936 ; loss 1.09 accuracy:50.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35358) 35358
70336 ; loss 1.09 accuracy:50.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38600) 38600
76736 ; loss 1.09 accuracy:50.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41867) 41867
83136 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45125) 45125
89536 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48385) 48385
95936 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51547) 51547
102336 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54809) 54809
108736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58060) 58060
115136 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61355) 61355
121536 ; loss 1.09 accuracy:50.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64587) 64587
127936 ; loss 1.09 accuracy:50.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67773) 67773
134336 ; loss 1.09 accuracy:50.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70993) 70993
140736 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74225) 74225
147136 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77434) 77434
153536 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80681) 80681
159936 ; loss 1.09 accuracy:50.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83899) 83899
166336 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87156) 87156
172736 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90381) 90381
179136 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93617) 93617
185536 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96813) 96813
191936 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(100030) 100030
198336 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103268) 103268
204736 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106475) 106475
211136 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109728) 109728
217536 ; loss 1.09 accuracy:50.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112936) 112936
223936 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(116170) 116170
230336 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119393) 119393
236736 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122560) 122560
243136 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(125757) 125757
249536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(129046) 129046
255936 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(132220) 132220
262336 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135364) 135364
268736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138585) 138585
275136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141867) 141867
281536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145043) 145043
287936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148227) 148227
294336 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151487) 151487
300736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154709) 154709
307136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(158001) 158001
313536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161180) 161180
319936 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164418) 164418
326336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167630) 167630
332736 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170833) 170833
339136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174082) 174082
345536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177275) 177275
351936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180498) 180498
358336 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183767) 183767
364736 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187043) 187043
371136 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190294) 190294
377536 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193545) 193545
383936 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196751) 196751
390336 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(199992) 199992
396736 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203246) 203246
403136 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206430) 206430
409536 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209637) 209637
415936 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212875) 212875
422336 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216049) 216049
428736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219258) 219258
435136 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222535) 222535
441536 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225769) 225769
447936 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(228939) 228939
454336 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(232119) 232119
460736 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235325) 235325
467136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(238561) 238561
473536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(241767) 241767
479936 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244984) 244984
486336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(248271) 248271
492736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(251390) 251390
499136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254594) 254594
505536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257826) 257826
511936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261124) 261124
518336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264370) 264370
524736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267599) 267599
531136 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270800) 270800
537536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(274029) 274029
543936 ; loss 1.09 accuracy:50.37 ;
results : epoch 8 ; mean accuracy train : 50.36

VALIDATION : Epoch 8
togrep : results : epoch 8 ; mean accuracy valid :              50.61
Shrinking lr by : 5. New lr = 0.00014913045566511838

TRAINING : Epoch 9
Learning rate : 0.00014763915110846718
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3159) 3159
6336 ; loss 1.09 accuracy:49.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6351) 6351
12736 ; loss 1.09 accuracy:49.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9577) 9577
19136 ; loss 1.09 accuracy:49.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12822) 12822
25536 ; loss 1.09 accuracy:50.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(16027) 16027
31936 ; loss 1.09 accuracy:50.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19222) 19222
38336 ; loss 1.09 accuracy:50.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22460) 22460
44736 ; loss 1.09 accuracy:50.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25692) 25692
51136 ; loss 1.09 accuracy:50.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28888) 28888
57536 ; loss 1.09 accuracy:50.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32082) 32082
63936 ; loss 1.09 accuracy:50.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35340) 35340
70336 ; loss 1.09 accuracy:50.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38566) 38566
76736 ; loss 1.09 accuracy:50.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41825) 41825
83136 ; loss 1.09 accuracy:50.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44976) 44976
89536 ; loss 1.09 accuracy:50.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48186) 48186
95936 ; loss 1.09 accuracy:50.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51480) 51480
102336 ; loss 1.09 accuracy:50.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54633) 54633
108736 ; loss 1.09 accuracy:50.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57849) 57849
115136 ; loss 1.09 accuracy:50.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61138) 61138
121536 ; loss 1.09 accuracy:50.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64374) 64374
127936 ; loss 1.09 accuracy:50.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67593) 67593
134336 ; loss 1.09 accuracy:50.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70859) 70859
140736 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74171) 74171
147136 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77329) 77329
153536 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80515) 80515
159936 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83750) 83750
166336 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87037) 87037
172736 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90336) 90336
179136 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93540) 93540
185536 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96735) 96735
191936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(99915) 99915
198336 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103140) 103140
204736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106341) 106341
211136 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109588) 109588
217536 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112843) 112843
223936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(116064) 116064
230336 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119258) 119258
236736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122469) 122469
243136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(125663) 125663
249536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128845) 128845
255936 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(132016) 132016
262336 ; loss 1.09 accuracy:50.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135275) 135275
268736 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138485) 138485
275136 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141726) 141726
281536 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144924) 144924
287936 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148151) 148151
294336 ; loss 1.09 accuracy:50.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151345) 151345
300736 ; loss 1.09 accuracy:50.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154608) 154608
307136 ; loss 1.09 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(157888) 157888
313536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161096) 161096
319936 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164376) 164376
326336 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167587) 167587
332736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170762) 170762
339136 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(173996) 173996
345536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177240) 177240
351936 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180495) 180495
358336 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183775) 183775
364736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186974) 186974
371136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190221) 190221
377536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193468) 193468
383936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196690) 196690
390336 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(199925) 199925
396736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203101) 203101
403136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206313) 206313
409536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209530) 209530
415936 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212745) 212745
422336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216005) 216005
428736 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219276) 219276
435136 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222485) 222485
441536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225698) 225698
447936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(228882) 228882
454336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(232078) 232078
460736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235202) 235202
467136 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(238488) 238488
473536 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(241658) 241658
479936 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244929) 244929
486336 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(248185) 248185
492736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(251439) 251439
499136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254685) 254685
505536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257868) 257868
511936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261127) 261127
518336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264357) 264357
524736 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267590) 267590
531136 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270790) 270790
537536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(273938) 273938
543936 ; loss 1.09 accuracy:50.36 ;
results : epoch 9 ; mean accuracy train : 50.36

VALIDATION : Epoch 9
togrep : results : epoch 9 ; mean accuracy valid :              50.58
Shrinking lr by : 5. New lr = 2.9527830221693436e-05

TRAINING : Epoch 10
Learning rate : 2.9232551919476502e-05
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3232) 3232
6336 ; loss 1.09 accuracy:50.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6446) 6446
12736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9684) 9684
19136 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12840) 12840
25536 ; loss 1.09 accuracy:50.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(16066) 16066
31936 ; loss 1.09 accuracy:50.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19306) 19306
38336 ; loss 1.09 accuracy:50.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22515) 22515
44736 ; loss 1.09 accuracy:50.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25746) 25746
51136 ; loss 1.09 accuracy:50.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28952) 28952
57536 ; loss 1.09 accuracy:50.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32198) 32198
63936 ; loss 1.09 accuracy:50.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35381) 35381
70336 ; loss 1.09 accuracy:50.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38577) 38577
76736 ; loss 1.09 accuracy:50.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41883) 41883
83136 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45173) 45173
89536 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48458) 48458
95936 ; loss 1.09 accuracy:50.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51613) 51613
102336 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54875) 54875
108736 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58059) 58059
115136 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61245) 61245
121536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64524) 64524
127936 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67747) 67747
134336 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70954) 70954
140736 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74240) 74240
147136 ; loss 1.09 accuracy:50.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77378) 77378
153536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80610) 80610
159936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83931) 83931
166336 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87130) 87130
172736 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90397) 90397
179136 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93549) 93549
185536 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96802) 96802
191936 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(99979) 99979
198336 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103176) 103176
204736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106426) 106426
211136 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109702) 109702
217536 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112892) 112892
223936 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(116035) 116035
230336 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119214) 119214
236736 ; loss 1.09 accuracy:50.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122467) 122467
243136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(125678) 125678
249536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128958) 128958
255936 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(132217) 132217
262336 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135437) 135437
268736 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138704) 138704
275136 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141872) 141872
281536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145140) 145140
287936 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148340) 148340
294336 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151497) 151497
300736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154789) 154789
307136 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(157984) 157984
313536 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161220) 161220
319936 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164455) 164455
326336 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167748) 167748
332736 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170971) 170971
339136 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174232) 174232
345536 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177540) 177540
351936 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180776) 180776
358336 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(184042) 184042
364736 ; loss 1.09 accuracy:50.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187206) 187206
371136 ; loss 1.09 accuracy:50.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190375) 190375
377536 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193671) 193671
383936 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196901) 196901
390336 ; loss 1.09 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200104) 200104
396736 ; loss 1.09 accuracy:50.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203308) 203308
403136 ; loss 1.09 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206477) 206477
409536 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209707) 209707
415936 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212917) 212917
422336 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216115) 216115
428736 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219381) 219381
435136 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222632) 222632
441536 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225832) 225832
447936 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(229078) 229078
454336 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(232249) 232249
460736 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235494) 235494
467136 ; loss 1.09 accuracy:50.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(238672) 238672
473536 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(241903) 241903
479936 ; loss 1.09 accuracy:50.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(245082) 245082
486336 ; loss 1.09 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(248260) 248260
492736 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(251483) 251483
499136 ; loss 1.09 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254678) 254678
505536 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257862) 257862
511936 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261125) 261125
518336 ; loss 1.09 accuracy:50.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264288) 264288
524736 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267497) 267497
531136 ; loss 1.09 accuracy:50.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270704) 270704
537536 ; loss 1.09 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(273958) 273958
543936 ; loss 1.09 accuracy:50.36 ;
results : epoch 10 ; mean accuracy train : 50.36

VALIDATION : Epoch 10
togrep : results : epoch 10 ; mean accuracy valid :              50.58
Shrinking lr by : 5. New lr = 5.8465103838953e-06

TEST : Epoch 11

VALIDATION : Epoch 1000000.0
finalgrep : accuracy valid : 50.58
finalgrep : accuracy test : 51.77
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#InferSent</span>
<span class="o">!</span> python train_nli.py
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
togrep : []

Namespace(batch_size=64, decay=0.99, dpout_fc=0.0, dpout_model=0.0, enc_lstm_dim=2048, encoder_type=&#39;InferSent&#39;, fc_dim=512, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=3, n_enc_layers=1, n_epochs=20, nlipath=&#39;/home/dc/InferSent/dataset/SNLI&#39;, nonlinear_fc=0, optimizer=&#39;sgd,lr=0.1&#39;, outputdir=&#39;savedir/&#39;, outputmodelname=&#39;infersent.pickle&#39;, pool_type=&#39;max&#39;, seed=1234)
** TRAIN DATA : Found 549367 pairs of train sentences.
** DEV DATA : Found 9842 pairs of dev sentences.
** TEST DATA : Found 9824 pairs of test sentences.
Found 38957(/43479) words with glove vectors
Vocab size : 38957
self.inputdim:16384, self.fc_dim:512
&lt;class &#39;int&#39;&gt; &lt;class &#39;int&#39;&gt;
NLINet(
  (encoder): InferSent(
    (enc_lstm): LSTM(300, 2048, bidirectional=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=16384, out_features=512, bias=True)
    (1): Linear(in_features=512, out_features=512, bias=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
  )
)

TRAINING : Epoch 1
Learning rate : 0.1
&lt;class &#39;torch.Tensor&#39;&gt; tensor(2128) 2128
6336 ; loss 1.1 accuracy:33.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4267) 4267
12736 ; loss 1.1 accuracy:33.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6449) 6449
19136 ; loss 1.1 accuracy:33.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(8578) 8578
25536 ; loss 1.1 accuracy:33.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10807) 10807
31936 ; loss 1.1 accuracy:33.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12943) 12943
38336 ; loss 1.1 accuracy:33.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15143) 15143
44736 ; loss 1.1 accuracy:33.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(17330) 17330
51136 ; loss 1.1 accuracy:33.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19503) 19503
57536 ; loss 1.1 accuracy:33.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(21620) 21620
63936 ; loss 1.1 accuracy:33.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(23764) 23764
70336 ; loss 1.1 accuracy:33.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25949) 25949
76736 ; loss 1.1 accuracy:33.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28096) 28096
83136 ; loss 1.1 accuracy:33.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(30411) 30411
89536 ; loss 1.1 accuracy:33.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32665) 32665
95936 ; loss 1.1 accuracy:34.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34849) 34849
102336 ; loss 1.1 accuracy:34.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37001) 37001
108736 ; loss 1.1 accuracy:34.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(39162) 39162
115136 ; loss 1.1 accuracy:33.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41274) 41274
121536 ; loss 1.1 accuracy:33.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(43602) 43602
127936 ; loss 1.1 accuracy:34.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46113) 46113
134336 ; loss 1.1 accuracy:34.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48473) 48473
140736 ; loss 1.1 accuracy:34.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50885) 50885
147136 ; loss 1.1 accuracy:34.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(53362) 53362
153536 ; loss 1.1 accuracy:34.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55791) 55791
159936 ; loss 1.1 accuracy:34.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58274) 58274
166336 ; loss 1.1 accuracy:35.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(60747) 60747
172736 ; loss 1.1 accuracy:35.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(63240) 63240
179136 ; loss 1.1 accuracy:35.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(65765) 65765
185536 ; loss 1.1 accuracy:35.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(68177) 68177
191936 ; loss 1.1 accuracy:35.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70575) 70575
198336 ; loss 1.1 accuracy:35.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73221) 73221
204736 ; loss 1.1 accuracy:35.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76069) 76069
211136 ; loss 1.1 accuracy:36.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(78921) 78921
217536 ; loss 1.1 accuracy:36.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(81694) 81694
223936 ; loss 1.1 accuracy:36.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(84516) 84516
230336 ; loss 1.1 accuracy:36.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87569) 87569
236736 ; loss 1.1 accuracy:36.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90631) 90631
243136 ; loss 1.1 accuracy:37.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93188) 93188
249536 ; loss 1.1 accuracy:37.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95786) 95786
255936 ; loss 1.1 accuracy:37.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98503) 98503
262336 ; loss 1.1 accuracy:37.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101414) 101414
268736 ; loss 1.1 accuracy:37.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(104336) 104336
275136 ; loss 1.1 accuracy:37.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(107108) 107108
281536 ; loss 1.1 accuracy:38.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109833) 109833
287936 ; loss 1.1 accuracy:38.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112374) 112374
294336 ; loss 1.1 accuracy:38.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114807) 114807
300736 ; loss 1.1 accuracy:38.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117090) 117090
307136 ; loss 1.1 accuracy:38.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119295) 119295
313536 ; loss 1.1 accuracy:38.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121646) 121646
319936 ; loss 1.1 accuracy:38.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124127) 124127
326336 ; loss 1.1 accuracy:38.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(126681) 126681
332736 ; loss 1.1 accuracy:38.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(129251) 129251
339136 ; loss 1.1 accuracy:38.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131608) 131608
345536 ; loss 1.1 accuracy:38.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134042) 134042
351936 ; loss 1.1 accuracy:38.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(136628) 136628
358336 ; loss 1.1 accuracy:38.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(139232) 139232
364736 ; loss 1.1 accuracy:38.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141658) 141658
371136 ; loss 1.1 accuracy:38.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144076) 144076
377536 ; loss 1.1 accuracy:38.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146393) 146393
383936 ; loss 1.1 accuracy:38.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148945) 148945
390336 ; loss 1.1 accuracy:38.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151592) 151592
396736 ; loss 1.1 accuracy:38.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154559) 154559
403136 ; loss 1.1 accuracy:38.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(157706) 157706
409536 ; loss 1.1 accuracy:38.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160944) 160944
415936 ; loss 1.1 accuracy:38.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163828) 163828
422336 ; loss 1.1 accuracy:38.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166540) 166540
428736 ; loss 1.1 accuracy:38.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169321) 169321
435136 ; loss 1.1 accuracy:38.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172103) 172103
441536 ; loss 1.1 accuracy:38.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174837) 174837
447936 ; loss 1.1 accuracy:39.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177597) 177597
454336 ; loss 1.1 accuracy:39.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180412) 180412
460736 ; loss 1.1 accuracy:39.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183392) 183392
467136 ; loss 1.1 accuracy:39.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186292) 186292
473536 ; loss 1.1 accuracy:39.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(189244) 189244
479936 ; loss 1.1 accuracy:39.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192337) 192337
486336 ; loss 1.1 accuracy:39.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195262) 195262
492736 ; loss 1.1 accuracy:39.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(198325) 198325
499136 ; loss 1.09 accuracy:39.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(201443) 201443
505536 ; loss 1.1 accuracy:39.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204490) 204490
511936 ; loss 1.1 accuracy:39.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(207432) 207432
518336 ; loss 1.09 accuracy:40.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(210472) 210472
524736 ; loss 1.1 accuracy:40.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(213616) 213616
531136 ; loss 1.09 accuracy:40.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216633) 216633
537536 ; loss 1.1 accuracy:40.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219641) 219641
543936 ; loss 1.09 accuracy:40.38 ;
results : epoch 1 ; mean accuracy train : 40.44

VALIDATION : Epoch 1
togrep : results : epoch 1 ; mean accuracy valid :              48.02
saving model at epoch 1

TRAINING : Epoch 2
Learning rate : 0.099
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3062) 3062
6336 ; loss 1.09 accuracy:47.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6182) 6182
12736 ; loss 1.09 accuracy:48.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9210) 9210
19136 ; loss 1.09 accuracy:47.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12111) 12111
25536 ; loss 1.09 accuracy:47.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15325) 15325
31936 ; loss 1.09 accuracy:47.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(18545) 18545
38336 ; loss 1.09 accuracy:48.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(21858) 21858
44736 ; loss 1.09 accuracy:48.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25116) 25116
51136 ; loss 1.09 accuracy:49.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28424) 28424
57536 ; loss 1.09 accuracy:49.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31699) 31699
63936 ; loss 1.09 accuracy:49.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34743) 34743
70336 ; loss 1.09 accuracy:49.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37406) 37406
76736 ; loss 1.09 accuracy:48.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(39982) 39982
83136 ; loss 1.09 accuracy:48.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(42810) 42810
89536 ; loss 1.09 accuracy:47.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45709) 45709
95936 ; loss 1.09 accuracy:47.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48689) 48689
102336 ; loss 1.09 accuracy:47.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51906) 51906
108736 ; loss 1.09 accuracy:47.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55055) 55055
115136 ; loss 1.09 accuracy:47.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58307) 58307
121536 ; loss 1.09 accuracy:47.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61421) 61421
127936 ; loss 1.09 accuracy:47.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64736) 64736
134336 ; loss 1.09 accuracy:48.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67966) 67966
140736 ; loss 1.09 accuracy:48.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(71117) 71117
147136 ; loss 1.09 accuracy:48.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74079) 74079
153536 ; loss 1.09 accuracy:48.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77217) 77217
159936 ; loss 1.09 accuracy:48.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80336) 80336
166336 ; loss 1.09 accuracy:48.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83548) 83548
172736 ; loss 1.09 accuracy:48.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86784) 86784
179136 ; loss 1.09 accuracy:48.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90083) 90083
185536 ; loss 1.09 accuracy:48.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93337) 93337
191936 ; loss 1.09 accuracy:48.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96662) 96662
198336 ; loss 1.09 accuracy:48.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(99995) 99995
204736 ; loss 1.09 accuracy:48.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103305) 103305
211136 ; loss 1.09 accuracy:48.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106505) 106505
217536 ; loss 1.09 accuracy:48.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109710) 109710
223936 ; loss 1.09 accuracy:48.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112832) 112832
230336 ; loss 1.09 accuracy:48.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(115983) 115983
236736 ; loss 1.09 accuracy:48.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(118958) 118958
243136 ; loss 1.09 accuracy:48.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122018) 122018
249536 ; loss 1.09 accuracy:48.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(125059) 125059
255936 ; loss 1.09 accuracy:48.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128006) 128006
262336 ; loss 1.09 accuracy:48.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131035) 131035
268736 ; loss 1.09 accuracy:48.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134128) 134128
275136 ; loss 1.09 accuracy:48.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137249) 137249
281536 ; loss 1.09 accuracy:48.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140402) 140402
287936 ; loss 1.09 accuracy:48.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(143585) 143585
294336 ; loss 1.09 accuracy:48.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146817) 146817
300736 ; loss 1.09 accuracy:48.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150001) 150001
307136 ; loss 1.09 accuracy:48.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(153064) 153064
313536 ; loss 1.09 accuracy:48.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156151) 156151
319936 ; loss 1.09 accuracy:48.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(159286) 159286
326336 ; loss 1.09 accuracy:48.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(162415) 162415
332736 ; loss 1.09 accuracy:48.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165601) 165601
339136 ; loss 1.09 accuracy:48.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(168699) 168699
345536 ; loss 1.09 accuracy:48.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(171843) 171843
351936 ; loss 1.09 accuracy:48.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(175096) 175096
358336 ; loss 1.09 accuracy:48.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178347) 178347
364736 ; loss 1.09 accuracy:48.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(181588) 181588
371136 ; loss 1.09 accuracy:48.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(184825) 184825
377536 ; loss 1.09 accuracy:48.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187892) 187892
383936 ; loss 1.09 accuracy:48.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190954) 190954
390336 ; loss 1.09 accuracy:48.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(194006) 194006
396736 ; loss 1.09 accuracy:48.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197023) 197023
403136 ; loss 1.09 accuracy:48.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200035) 200035
409536 ; loss 1.09 accuracy:48.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203078) 203078
415936 ; loss 1.09 accuracy:48.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206117) 206117
422336 ; loss 1.09 accuracy:48.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209244) 209244
428736 ; loss 1.09 accuracy:48.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212330) 212330
435136 ; loss 1.09 accuracy:48.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215265) 215265
441536 ; loss 1.09 accuracy:48.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(218098) 218098
447936 ; loss 1.09 accuracy:48.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(220896) 220896
454336 ; loss 1.09 accuracy:48.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223861) 223861
460736 ; loss 1.09 accuracy:48.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(226844) 226844
467136 ; loss 1.09 accuracy:48.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(229900) 229900
473536 ; loss 1.09 accuracy:48.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(233048) 233048
479936 ; loss 1.09 accuracy:48.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(236235) 236235
486336 ; loss 1.09 accuracy:48.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239496) 239496
492736 ; loss 1.09 accuracy:48.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(242784) 242784
499136 ; loss 1.09 accuracy:48.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(246062) 246062
505536 ; loss 1.09 accuracy:48.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249193) 249193
511936 ; loss 1.09 accuracy:48.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252320) 252320
518336 ; loss 1.09 accuracy:48.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(255499) 255499
524736 ; loss 1.09 accuracy:48.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(258676) 258676
531136 ; loss 1.09 accuracy:48.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261788) 261788
537536 ; loss 1.09 accuracy:48.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264912) 264912
543936 ; loss 1.09 accuracy:48.7 ;
results : epoch 2 ; mean accuracy train : 48.7

VALIDATION : Epoch 2
togrep : results : epoch 2 ; mean accuracy valid :              49.88
saving model at epoch 2

TRAINING : Epoch 3
Learning rate : 0.09801
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3173) 3173
6336 ; loss 1.09 accuracy:49.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6310) 6310
12736 ; loss 1.09 accuracy:49.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9487) 9487
19136 ; loss 1.09 accuracy:49.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12662) 12662
25536 ; loss 1.09 accuracy:49.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15838) 15838
31936 ; loss 1.09 accuracy:49.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19009) 19009
38336 ; loss 1.09 accuracy:49.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22310) 22310
44736 ; loss 1.09 accuracy:49.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25576) 25576
51136 ; loss 1.09 accuracy:49.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28822) 28822
57536 ; loss 1.08 accuracy:50.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32156) 32156
63936 ; loss 1.08 accuracy:50.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35409) 35409
70336 ; loss 1.08 accuracy:50.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38724) 38724
76736 ; loss 1.08 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(42019) 42019
83136 ; loss 1.08 accuracy:50.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45293) 45293
89536 ; loss 1.08 accuracy:50.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48430) 48430
95936 ; loss 1.08 accuracy:50.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51652) 51652
102336 ; loss 1.08 accuracy:50.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54756) 54756
108736 ; loss 1.08 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57907) 57907
115136 ; loss 1.08 accuracy:50.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61114) 61114
121536 ; loss 1.08 accuracy:50.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64186) 64186
127936 ; loss 1.08 accuracy:50.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67305) 67305
134336 ; loss 1.08 accuracy:50.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70418) 70418
140736 ; loss 1.08 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73589) 73589
147136 ; loss 1.08 accuracy:49.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76740) 76740
153536 ; loss 1.08 accuracy:49.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79883) 79883
159936 ; loss 1.08 accuracy:49.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82989) 82989
166336 ; loss 1.08 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86083) 86083
172736 ; loss 1.08 accuracy:49.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(89154) 89154
179136 ; loss 1.08 accuracy:49.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92193) 92193
185536 ; loss 1.08 accuracy:49.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95217) 95217
191936 ; loss 1.08 accuracy:49.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98279) 98279
198336 ; loss 1.08 accuracy:49.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101504) 101504
204736 ; loss 1.08 accuracy:49.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(104683) 104683
211136 ; loss 1.08 accuracy:49.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(107962) 107962
217536 ; loss 1.08 accuracy:49.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(111208) 111208
223936 ; loss 1.08 accuracy:49.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114377) 114377
230336 ; loss 1.08 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117577) 117577
236736 ; loss 1.08 accuracy:49.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(120692) 120692
243136 ; loss 1.08 accuracy:49.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(123857) 123857
249536 ; loss 1.08 accuracy:49.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(127140) 127140
255936 ; loss 1.08 accuracy:49.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(130251) 130251
262336 ; loss 1.08 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133455) 133455
268736 ; loss 1.08 accuracy:49.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(136615) 136615
275136 ; loss 1.08 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(139780) 139780
281536 ; loss 1.08 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(142895) 142895
287936 ; loss 1.08 accuracy:49.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146093) 146093
294336 ; loss 1.08 accuracy:49.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(149312) 149312
300736 ; loss 1.07 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(152453) 152453
307136 ; loss 1.07 accuracy:49.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155448) 155448
313536 ; loss 1.07 accuracy:49.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(158537) 158537
319936 ; loss 1.07 accuracy:49.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161688) 161688
326336 ; loss 1.07 accuracy:49.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164873) 164873
332736 ; loss 1.07 accuracy:49.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167945) 167945
339136 ; loss 1.07 accuracy:49.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(171026) 171026
345536 ; loss 1.07 accuracy:49.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174204) 174204
351936 ; loss 1.07 accuracy:49.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177309) 177309
358336 ; loss 1.07 accuracy:49.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180464) 180464
364736 ; loss 1.07 accuracy:49.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183524) 183524
371136 ; loss 1.07 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186561) 186561
377536 ; loss 1.07 accuracy:49.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(189762) 189762
383936 ; loss 1.07 accuracy:49.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192917) 192917
390336 ; loss 1.07 accuracy:49.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196087) 196087
396736 ; loss 1.07 accuracy:49.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(199333) 199333
403136 ; loss 1.07 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(202526) 202526
409536 ; loss 1.07 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(205701) 205701
415936 ; loss 1.07 accuracy:49.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(208822) 208822
422336 ; loss 1.07 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212004) 212004
428736 ; loss 1.07 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215171) 215171
435136 ; loss 1.07 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(218319) 218319
441536 ; loss 1.07 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(221388) 221388
447936 ; loss 1.07 accuracy:49.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(224520) 224520
454336 ; loss 1.07 accuracy:49.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227792) 227792
460736 ; loss 1.06 accuracy:49.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(231013) 231013
467136 ; loss 1.06 accuracy:49.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234179) 234179
473536 ; loss 1.07 accuracy:49.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(237370) 237370
479936 ; loss 1.06 accuracy:49.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(240480) 240480
486336 ; loss 1.06 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243671) 243671
492736 ; loss 1.06 accuracy:49.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(246794) 246794
499136 ; loss 1.06 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249955) 249955
505536 ; loss 1.06 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(253092) 253092
511936 ; loss 1.06 accuracy:49.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(256233) 256233
518336 ; loss 1.06 accuracy:49.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(259386) 259386
524736 ; loss 1.06 accuracy:49.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(262600) 262600
531136 ; loss 1.05 accuracy:49.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(265723) 265723
537536 ; loss 1.06 accuracy:49.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268873) 268873
543936 ; loss 1.06 accuracy:49.43 ;
results : epoch 3 ; mean accuracy train : 49.43

VALIDATION : Epoch 3
togrep : results : epoch 3 ; mean accuracy valid :              50.15
saving model at epoch 3

TRAINING : Epoch 4
Learning rate : 0.0970299
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3196) 3196
6336 ; loss 1.06 accuracy:49.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6304) 6304
12736 ; loss 1.06 accuracy:49.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9517) 9517
19136 ; loss 1.05 accuracy:49.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12687) 12687
25536 ; loss 1.05 accuracy:49.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15801) 15801
31936 ; loss 1.06 accuracy:49.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(18963) 18963
38336 ; loss 1.05 accuracy:49.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22160) 22160
44736 ; loss 1.05 accuracy:49.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25353) 25353
51136 ; loss 1.05 accuracy:49.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28591) 28591
57536 ; loss 1.05 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31837) 31837
63936 ; loss 1.05 accuracy:49.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34998) 34998
70336 ; loss 1.05 accuracy:49.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38233) 38233
76736 ; loss 1.05 accuracy:49.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41384) 41384
83136 ; loss 1.05 accuracy:49.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44479) 44479
89536 ; loss 1.05 accuracy:49.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(47668) 47668
95936 ; loss 1.05 accuracy:49.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50957) 50957
102336 ; loss 1.05 accuracy:49.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54174) 54174
108736 ; loss 1.04 accuracy:49.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57361) 57361
115136 ; loss 1.04 accuracy:49.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(60483) 60483
121536 ; loss 1.05 accuracy:49.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(63656) 63656
127936 ; loss 1.04 accuracy:49.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66874) 66874
134336 ; loss 1.04 accuracy:49.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70092) 70092
140736 ; loss 1.04 accuracy:49.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73275) 73275
147136 ; loss 1.05 accuracy:49.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76440) 76440
153536 ; loss 1.04 accuracy:49.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79659) 79659
159936 ; loss 1.04 accuracy:49.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82846) 82846
166336 ; loss 1.04 accuracy:49.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86015) 86015
172736 ; loss 1.04 accuracy:49.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(89250) 89250
179136 ; loss 1.04 accuracy:49.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92412) 92412
185536 ; loss 1.04 accuracy:49.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95607) 95607
191936 ; loss 1.04 accuracy:49.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98873) 98873
198336 ; loss 1.03 accuracy:49.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(102125) 102125
204736 ; loss 1.03 accuracy:49.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105344) 105344
211136 ; loss 1.03 accuracy:49.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108596) 108596
217536 ; loss 1.03 accuracy:49.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(111833) 111833
223936 ; loss 1.03 accuracy:49.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(115051) 115051
230336 ; loss 1.03 accuracy:49.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(118399) 118399
236736 ; loss 1.02 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121624) 121624
243136 ; loss 1.03 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124795) 124795
249536 ; loss 1.03 accuracy:50.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(127937) 127937
255936 ; loss 1.03 accuracy:49.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131229) 131229
262336 ; loss 1.02 accuracy:50.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134496) 134496
268736 ; loss 1.02 accuracy:50.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137757) 137757
275136 ; loss 1.02 accuracy:50.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141034) 141034
281536 ; loss 1.02 accuracy:50.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144281) 144281
287936 ; loss 1.03 accuracy:50.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(147531) 147531
294336 ; loss 1.03 accuracy:50.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150779) 150779
300736 ; loss 1.02 accuracy:50.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(153985) 153985
307136 ; loss 1.02 accuracy:50.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(157224) 157224
313536 ; loss 1.02 accuracy:50.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160525) 160525
319936 ; loss 1.02 accuracy:50.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163790) 163790
326336 ; loss 1.02 accuracy:50.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167127) 167127
332736 ; loss 1.02 accuracy:50.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170407) 170407
339136 ; loss 1.02 accuracy:50.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(173667) 173667
345536 ; loss 1.01 accuracy:50.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(176948) 176948
351936 ; loss 1.02 accuracy:50.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180320) 180320
358336 ; loss 1.01 accuracy:50.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183597) 183597
364736 ; loss 1.01 accuracy:50.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186915) 186915
371136 ; loss 1.01 accuracy:50.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190231) 190231
377536 ; loss 1.01 accuracy:50.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193512) 193512
383936 ; loss 1.01 accuracy:50.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196842) 196842
390336 ; loss 1.01 accuracy:50.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200166) 200166
396736 ; loss 1.01 accuracy:50.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203452) 203452
403136 ; loss 1.01 accuracy:50.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206854) 206854
409536 ; loss 1.01 accuracy:50.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(210204) 210204
415936 ; loss 1.01 accuracy:50.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(213545) 213545
422336 ; loss 1.01 accuracy:50.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216962) 216962
428736 ; loss 1.0 accuracy:50.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(220346) 220346
435136 ; loss 1.0 accuracy:50.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223693) 223693
441536 ; loss 1.0 accuracy:50.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227017) 227017
447936 ; loss 1.0 accuracy:50.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230364) 230364
454336 ; loss 1.0 accuracy:50.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(233712) 233712
460736 ; loss 0.99 accuracy:50.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(237087) 237087
467136 ; loss 0.99 accuracy:50.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(240353) 240353
473536 ; loss 1.0 accuracy:50.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243710) 243710
479936 ; loss 1.0 accuracy:50.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(247224) 247224
486336 ; loss 0.99 accuracy:50.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(250546) 250546
492736 ; loss 1.0 accuracy:50.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(253907) 253907
499136 ; loss 0.99 accuracy:50.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257293) 257293
505536 ; loss 0.99 accuracy:50.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(260736) 260736
511936 ; loss 0.99 accuracy:50.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264101) 264101
518336 ; loss 1.0 accuracy:50.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267572) 267572
524736 ; loss 0.98 accuracy:50.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270950) 270950
531136 ; loss 0.99 accuracy:51.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(274342) 274342
537536 ; loss 0.99 accuracy:51.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(277814) 277814
543936 ; loss 0.98 accuracy:51.07 ;
results : epoch 4 ; mean accuracy train : 51.1

VALIDATION : Epoch 4
togrep : results : epoch 4 ; mean accuracy valid :              54.04
saving model at epoch 4

TRAINING : Epoch 5
Learning rate : 0.096059601
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3396) 3396
6336 ; loss 0.98 accuracy:53.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(6769) 6769
12736 ; loss 0.99 accuracy:52.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10228) 10228
19136 ; loss 0.98 accuracy:53.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(13685) 13685
25536 ; loss 0.98 accuracy:53.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(17111) 17111
31936 ; loss 0.98 accuracy:53.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20601) 20601
38336 ; loss 0.98 accuracy:53.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(24054) 24054
44736 ; loss 0.98 accuracy:53.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(27514) 27514
51136 ; loss 0.98 accuracy:53.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31001) 31001
57536 ; loss 0.98 accuracy:53.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34405) 34405
63936 ; loss 0.98 accuracy:53.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37916) 37916
70336 ; loss 0.97 accuracy:53.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41428) 41428
76736 ; loss 0.97 accuracy:53.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44943) 44943
83136 ; loss 0.97 accuracy:54.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48458) 48458
89536 ; loss 0.97 accuracy:54.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51939) 51939
95936 ; loss 0.97 accuracy:54.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55442) 55442
102336 ; loss 0.97 accuracy:54.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58955) 58955
108736 ; loss 0.96 accuracy:54.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(62512) 62512
115136 ; loss 0.96 accuracy:54.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66035) 66035
121536 ; loss 0.96 accuracy:54.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(69536) 69536
127936 ; loss 0.96 accuracy:54.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(72998) 72998
134336 ; loss 0.97 accuracy:54.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76614) 76614
140736 ; loss 0.96 accuracy:54.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80122) 80122
147136 ; loss 0.96 accuracy:54.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83674) 83674
153536 ; loss 0.96 accuracy:54.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87243) 87243
159936 ; loss 0.96 accuracy:54.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90882) 90882
166336 ; loss 0.95 accuracy:54.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(94432) 94432
172736 ; loss 0.96 accuracy:54.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98012) 98012
179136 ; loss 0.95 accuracy:54.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101557) 101557
185536 ; loss 0.95 accuracy:54.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105152) 105152
191936 ; loss 0.95 accuracy:54.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108650) 108650
198336 ; loss 0.96 accuracy:54.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112277) 112277
204736 ; loss 0.95 accuracy:54.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(115903) 115903
211136 ; loss 0.95 accuracy:54.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119531) 119531
217536 ; loss 0.95 accuracy:54.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(123159) 123159
223936 ; loss 0.94 accuracy:54.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(126766) 126766
230336 ; loss 0.95 accuracy:55.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(130400) 130400
236736 ; loss 0.94 accuracy:55.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134045) 134045
243136 ; loss 0.94 accuracy:55.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137691) 137691
249536 ; loss 0.94 accuracy:55.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141343) 141343
255936 ; loss 0.94 accuracy:55.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144967) 144967
262336 ; loss 0.94 accuracy:55.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148611) 148611
268736 ; loss 0.94 accuracy:55.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(152278) 152278
275136 ; loss 0.94 accuracy:55.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155895) 155895
281536 ; loss 0.94 accuracy:55.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(159595) 159595
287936 ; loss 0.93 accuracy:55.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163237) 163237
294336 ; loss 0.94 accuracy:55.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166886) 166886
300736 ; loss 0.93 accuracy:55.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170631) 170631
307136 ; loss 0.93 accuracy:55.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174274) 174274
313536 ; loss 0.94 accuracy:55.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178000) 178000
319936 ; loss 0.93 accuracy:55.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(181695) 181695
326336 ; loss 0.93 accuracy:55.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185353) 185353
332736 ; loss 0.93 accuracy:55.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(189031) 189031
339136 ; loss 0.93 accuracy:55.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192711) 192711
345536 ; loss 0.93 accuracy:55.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196459) 196459
351936 ; loss 0.93 accuracy:55.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200190) 200190
358336 ; loss 0.92 accuracy:55.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203936) 203936
364736 ; loss 0.92 accuracy:55.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(207641) 207641
371136 ; loss 0.92 accuracy:55.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(211415) 211415
377536 ; loss 0.92 accuracy:55.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215071) 215071
383936 ; loss 0.93 accuracy:56.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(218851) 218851
390336 ; loss 0.92 accuracy:56.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222572) 222572
396736 ; loss 0.91 accuracy:56.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(226367) 226367
403136 ; loss 0.91 accuracy:56.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230119) 230119
409536 ; loss 0.92 accuracy:56.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(233888) 233888
415936 ; loss 0.92 accuracy:56.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(237669) 237669
422336 ; loss 0.91 accuracy:56.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(241443) 241443
428736 ; loss 0.92 accuracy:56.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(245168) 245168
435136 ; loss 0.92 accuracy:56.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249017) 249017
441536 ; loss 0.9 accuracy:56.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252877) 252877
447936 ; loss 0.91 accuracy:56.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(256739) 256739
454336 ; loss 0.9 accuracy:56.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(260509) 260509
460736 ; loss 0.91 accuracy:56.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264318) 264318
467136 ; loss 0.9 accuracy:56.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268165) 268165
473536 ; loss 0.9 accuracy:56.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(272032) 272032
479936 ; loss 0.9 accuracy:56.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(275837) 275837
486336 ; loss 0.9 accuracy:56.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(279664) 279664
492736 ; loss 0.9 accuracy:56.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(283439) 283439
499136 ; loss 0.9 accuracy:56.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(287229) 287229
505536 ; loss 0.9 accuracy:56.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(291091) 291091
511936 ; loss 0.89 accuracy:56.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(294893) 294893
518336 ; loss 0.9 accuracy:56.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(298797) 298797
524736 ; loss 0.88 accuracy:56.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(302630) 302630
531136 ; loss 0.89 accuracy:56.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(306480) 306480
537536 ; loss 0.88 accuracy:57.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(310374) 310374
543936 ; loss 0.89 accuracy:57.05 ;
results : epoch 5 ; mean accuracy train : 57.09

VALIDATION : Epoch 5
togrep : results : epoch 5 ; mean accuracy valid :              62.32
saving model at epoch 5

TRAINING : Epoch 6
Learning rate : 0.09509900499
&lt;class &#39;torch.Tensor&#39;&gt; tensor(3925) 3925
6336 ; loss 0.88 accuracy:61.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(7739) 7739
12736 ; loss 0.89 accuracy:60.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(11593) 11593
19136 ; loss 0.88 accuracy:60.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15497) 15497
25536 ; loss 0.88 accuracy:60.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19476) 19476
31936 ; loss 0.88 accuracy:60.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(23347) 23347
38336 ; loss 0.88 accuracy:60.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(27251) 27251
44736 ; loss 0.88 accuracy:60.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31153) 31153
51136 ; loss 0.88 accuracy:60.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35000) 35000
57536 ; loss 0.88 accuracy:60.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38918) 38918
63936 ; loss 0.88 accuracy:60.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(42844) 42844
70336 ; loss 0.87 accuracy:60.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46745) 46745
76736 ; loss 0.87 accuracy:60.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50686) 50686
83136 ; loss 0.87 accuracy:60.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54646) 54646
89536 ; loss 0.87 accuracy:60.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58530) 58530
95936 ; loss 0.88 accuracy:60.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(62418) 62418
102336 ; loss 0.87 accuracy:60.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66308) 66308
108736 ; loss 0.87 accuracy:60.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70295) 70295
115136 ; loss 0.86 accuracy:61.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74291) 74291
121536 ; loss 0.86 accuracy:61.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(78257) 78257
127936 ; loss 0.86 accuracy:61.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82248) 82248
134336 ; loss 0.86 accuracy:61.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86230) 86230
140736 ; loss 0.85 accuracy:61.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90153) 90153
147136 ; loss 0.87 accuracy:61.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(94157) 94157
153536 ; loss 0.86 accuracy:61.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98090) 98090
159936 ; loss 0.85 accuracy:61.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(102117) 102117
166336 ; loss 0.85 accuracy:61.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106108) 106108
172736 ; loss 0.85 accuracy:61.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(110064) 110064
179136 ; loss 0.86 accuracy:61.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114040) 114040
185536 ; loss 0.85 accuracy:61.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117992) 117992
191936 ; loss 0.86 accuracy:61.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121970) 121970
198336 ; loss 0.86 accuracy:61.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(125961) 125961
204736 ; loss 0.84 accuracy:61.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(129940) 129940
211136 ; loss 0.85 accuracy:61.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133963) 133963
217536 ; loss 0.84 accuracy:61.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137983) 137983
223936 ; loss 0.84 accuracy:61.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141963) 141963
230336 ; loss 0.85 accuracy:61.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146011) 146011
236736 ; loss 0.84 accuracy:61.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150009) 150009
243136 ; loss 0.85 accuracy:61.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154085) 154085
249536 ; loss 0.83 accuracy:61.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(158141) 158141
255936 ; loss 0.84 accuracy:61.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(162227) 162227
262336 ; loss 0.83 accuracy:61.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166267) 166267
268736 ; loss 0.83 accuracy:61.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170267) 170267
275136 ; loss 0.84 accuracy:61.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174265) 174265
281536 ; loss 0.85 accuracy:61.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178373) 178373
287936 ; loss 0.83 accuracy:61.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182425) 182425
294336 ; loss 0.83 accuracy:61.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186502) 186502
300736 ; loss 0.83 accuracy:62.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190603) 190603
307136 ; loss 0.83 accuracy:62.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(194632) 194632
313536 ; loss 0.84 accuracy:62.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(198699) 198699
319936 ; loss 0.83 accuracy:62.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(202812) 202812
326336 ; loss 0.82 accuracy:62.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206860) 206860
332736 ; loss 0.83 accuracy:62.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(210914) 210914
339136 ; loss 0.83 accuracy:62.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215014) 215014
345536 ; loss 0.82 accuracy:62.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219152) 219152
351936 ; loss 0.82 accuracy:62.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223213) 223213
358336 ; loss 0.82 accuracy:62.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227282) 227282
364736 ; loss 0.83 accuracy:62.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(231362) 231362
371136 ; loss 0.82 accuracy:62.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235479) 235479
377536 ; loss 0.82 accuracy:62.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239618) 239618
383936 ; loss 0.82 accuracy:62.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243679) 243679
390336 ; loss 0.83 accuracy:62.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(247831) 247831
396736 ; loss 0.81 accuracy:62.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(251992) 251992
403136 ; loss 0.81 accuracy:62.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(256089) 256089
409536 ; loss 0.82 accuracy:62.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(260247) 260247
415936 ; loss 0.81 accuracy:62.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264413) 264413
422336 ; loss 0.8 accuracy:62.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268588) 268588
428736 ; loss 0.81 accuracy:62.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(272695) 272695
435136 ; loss 0.81 accuracy:62.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(276817) 276817
441536 ; loss 0.8 accuracy:62.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(280989) 280989
447936 ; loss 0.81 accuracy:62.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(285166) 285166
454336 ; loss 0.8 accuracy:62.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(289293) 289293
460736 ; loss 0.8 accuracy:62.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(293430) 293430
467136 ; loss 0.81 accuracy:62.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(297647) 297647
473536 ; loss 0.8 accuracy:62.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(301839) 301839
479936 ; loss 0.8 accuracy:62.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(306008) 306008
486336 ; loss 0.8 accuracy:62.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(310169) 310169
492736 ; loss 0.8 accuracy:62.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(314302) 314302
499136 ; loss 0.8 accuracy:62.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(318474) 318474
505536 ; loss 0.79 accuracy:62.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(322633) 322633
511936 ; loss 0.8 accuracy:63.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326833) 326833
518336 ; loss 0.8 accuracy:63.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(330955) 330955
524736 ; loss 0.8 accuracy:63.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(335104) 335104
531136 ; loss 0.8 accuracy:63.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(339325) 339325
537536 ; loss 0.79 accuracy:63.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(343472) 343472
543936 ; loss 0.8 accuracy:63.14 ;
results : epoch 6 ; mean accuracy train : 63.17

VALIDATION : Epoch 6
togrep : results : epoch 6 ; mean accuracy valid :              67.12
saving model at epoch 6

TRAINING : Epoch 7
Learning rate : 0.0941480149401
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4215) 4215
6336 ; loss 0.79 accuracy:65.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(8408) 8408
12736 ; loss 0.79 accuracy:65.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(12575) 12575
19136 ; loss 0.79 accuracy:65.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(16724) 16724
25536 ; loss 0.8 accuracy:65.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20955) 20955
31936 ; loss 0.79 accuracy:65.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25162) 25162
38336 ; loss 0.79 accuracy:65.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(29374) 29374
44736 ; loss 0.79 accuracy:65.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(33570) 33570
51136 ; loss 0.79 accuracy:65.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37799) 37799
57536 ; loss 0.79 accuracy:65.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(42013) 42013
63936 ; loss 0.79 accuracy:65.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46257) 46257
70336 ; loss 0.79 accuracy:65.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50381) 50381
76736 ; loss 0.8 accuracy:65.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54639) 54639
83136 ; loss 0.78 accuracy:65.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58894) 58894
89536 ; loss 0.78 accuracy:65.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(63140) 63140
95936 ; loss 0.77 accuracy:65.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67421) 67421
102336 ; loss 0.77 accuracy:65.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(71709) 71709
108736 ; loss 0.76 accuracy:65.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(75949) 75949
115136 ; loss 0.78 accuracy:65.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80245) 80245
121536 ; loss 0.78 accuracy:65.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(84454) 84454
127936 ; loss 0.78 accuracy:65.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(88685) 88685
134336 ; loss 0.79 accuracy:65.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92960) 92960
140736 ; loss 0.78 accuracy:66.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(97197) 97197
147136 ; loss 0.78 accuracy:66.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101454) 101454
153536 ; loss 0.78 accuracy:66.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105714) 105714
159936 ; loss 0.76 accuracy:66.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109983) 109983
166336 ; loss 0.77 accuracy:66.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114235) 114235
172736 ; loss 0.77 accuracy:66.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(118529) 118529
179136 ; loss 0.76 accuracy:66.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122774) 122774
185536 ; loss 0.78 accuracy:66.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(127052) 127052
191936 ; loss 0.77 accuracy:66.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131254) 131254
198336 ; loss 0.78 accuracy:66.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135582) 135582
204736 ; loss 0.76 accuracy:66.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(139868) 139868
211136 ; loss 0.77 accuracy:66.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(144192) 144192
217536 ; loss 0.76 accuracy:66.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148462) 148462
223936 ; loss 0.77 accuracy:66.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(152726) 152726
230336 ; loss 0.78 accuracy:66.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156996) 156996
236736 ; loss 0.77 accuracy:66.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161303) 161303
243136 ; loss 0.77 accuracy:66.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165608) 165608
249536 ; loss 0.76 accuracy:66.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169914) 169914
255936 ; loss 0.76 accuracy:66.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174260) 174260
262336 ; loss 0.75 accuracy:66.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178573) 178573
268736 ; loss 0.76 accuracy:66.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182883) 182883
275136 ; loss 0.76 accuracy:66.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187212) 187212
281536 ; loss 0.75 accuracy:66.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191470) 191470
287936 ; loss 0.76 accuracy:66.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195833) 195833
294336 ; loss 0.76 accuracy:66.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200132) 200132
300736 ; loss 0.77 accuracy:66.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204458) 204458
307136 ; loss 0.76 accuracy:66.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(208679) 208679
313536 ; loss 0.78 accuracy:66.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(213003) 213003
319936 ; loss 0.75 accuracy:66.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(217327) 217327
326336 ; loss 0.75 accuracy:66.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(221697) 221697
332736 ; loss 0.75 accuracy:66.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(226034) 226034
339136 ; loss 0.75 accuracy:66.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230392) 230392
345536 ; loss 0.75 accuracy:66.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234708) 234708
351936 ; loss 0.76 accuracy:66.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239012) 239012
358336 ; loss 0.76 accuracy:66.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243287) 243287
364736 ; loss 0.77 accuracy:66.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(247632) 247632
371136 ; loss 0.75 accuracy:66.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(251969) 251969
377536 ; loss 0.75 accuracy:66.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(256315) 256315
383936 ; loss 0.76 accuracy:66.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(260633) 260633
390336 ; loss 0.74 accuracy:66.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264993) 264993
396736 ; loss 0.75 accuracy:66.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(269330) 269330
403136 ; loss 0.76 accuracy:66.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(273717) 273717
409536 ; loss 0.74 accuracy:66.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(278064) 278064
415936 ; loss 0.74 accuracy:66.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(282423) 282423
422336 ; loss 0.74 accuracy:66.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(286780) 286780
428736 ; loss 0.75 accuracy:66.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(291109) 291109
435136 ; loss 0.75 accuracy:66.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(295473) 295473
441536 ; loss 0.74 accuracy:66.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(299815) 299815
447936 ; loss 0.75 accuracy:66.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(304103) 304103
454336 ; loss 0.75 accuracy:66.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(308514) 308514
460736 ; loss 0.73 accuracy:66.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(312914) 312914
467136 ; loss 0.74 accuracy:66.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(317247) 317247
473536 ; loss 0.74 accuracy:66.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(321648) 321648
479936 ; loss 0.73 accuracy:67.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326012) 326012
486336 ; loss 0.73 accuracy:67.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(330393) 330393
492736 ; loss 0.74 accuracy:67.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(334754) 334754
499136 ; loss 0.73 accuracy:67.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(339124) 339124
505536 ; loss 0.75 accuracy:67.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(343541) 343541
511936 ; loss 0.74 accuracy:67.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(347925) 347925
518336 ; loss 0.73 accuracy:67.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(352342) 352342
524736 ; loss 0.73 accuracy:67.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(356782) 356782
531136 ; loss 0.72 accuracy:67.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(361214) 361214
537536 ; loss 0.72 accuracy:67.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(365556) 365556
543936 ; loss 0.75 accuracy:67.2 ;
results : epoch 7 ; mean accuracy train : 67.22

VALIDATION : Epoch 7
togrep : results : epoch 7 ; mean accuracy valid :              70.34
saving model at epoch 7

TRAINING : Epoch 8
Learning rate : 0.093206534790699
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4438) 4438
6336 ; loss 0.73 accuracy:69.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(8873) 8873
12736 ; loss 0.73 accuracy:69.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(13246) 13246
19136 ; loss 0.74 accuracy:68.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(17635) 17635
25536 ; loss 0.74 accuracy:68.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22031) 22031
31936 ; loss 0.73 accuracy:68.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(26440) 26440
38336 ; loss 0.73 accuracy:68.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(30845) 30845
44736 ; loss 0.73 accuracy:68.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35312) 35312
51136 ; loss 0.72 accuracy:68.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(39737) 39737
57536 ; loss 0.74 accuracy:68.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44129) 44129
63936 ; loss 0.73 accuracy:68.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48560) 48560
70336 ; loss 0.74 accuracy:68.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(52936) 52936
76736 ; loss 0.73 accuracy:68.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57332) 57332
83136 ; loss 0.73 accuracy:68.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61783) 61783
89536 ; loss 0.71 accuracy:68.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66257) 66257
95936 ; loss 0.72 accuracy:69.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70629) 70629
102336 ; loss 0.74 accuracy:68.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(75098) 75098
108736 ; loss 0.71 accuracy:69.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79592) 79592
115136 ; loss 0.71 accuracy:69.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(84038) 84038
121536 ; loss 0.71 accuracy:69.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(88513) 88513
127936 ; loss 0.71 accuracy:69.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92984) 92984
134336 ; loss 0.71 accuracy:69.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(97464) 97464
140736 ; loss 0.71 accuracy:69.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101862) 101862
147136 ; loss 0.73 accuracy:69.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(106324) 106324
153536 ; loss 0.71 accuracy:69.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(110827) 110827
159936 ; loss 0.71 accuracy:69.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(115299) 115299
166336 ; loss 0.72 accuracy:69.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119775) 119775
172736 ; loss 0.71 accuracy:69.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124255) 124255
179136 ; loss 0.7 accuracy:69.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128733) 128733
185536 ; loss 0.71 accuracy:69.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133205) 133205
191936 ; loss 0.72 accuracy:69.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137654) 137654
198336 ; loss 0.71 accuracy:69.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(142098) 142098
204736 ; loss 0.71 accuracy:69.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146551) 146551
211136 ; loss 0.71 accuracy:69.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150981) 150981
217536 ; loss 0.72 accuracy:69.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155444) 155444
223936 ; loss 0.71 accuracy:69.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(159929) 159929
230336 ; loss 0.71 accuracy:69.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164380) 164380
236736 ; loss 0.72 accuracy:69.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(168850) 168850
243136 ; loss 0.71 accuracy:69.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(173322) 173322
249536 ; loss 0.71 accuracy:69.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177821) 177821
255936 ; loss 0.7 accuracy:69.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182340) 182340
262336 ; loss 0.7 accuracy:69.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186829) 186829
268736 ; loss 0.7 accuracy:69.5 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191333) 191333
275136 ; loss 0.7 accuracy:69.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195826) 195826
281536 ; loss 0.7 accuracy:69.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200301) 200301
287936 ; loss 0.72 accuracy:69.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204792) 204792
294336 ; loss 0.72 accuracy:69.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209253) 209253
300736 ; loss 0.71 accuracy:69.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(213756) 213756
307136 ; loss 0.7 accuracy:69.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(218258) 218258
313536 ; loss 0.7 accuracy:69.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222695) 222695
319936 ; loss 0.72 accuracy:69.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227165) 227165
326336 ; loss 0.71 accuracy:69.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(231609) 231609
332736 ; loss 0.71 accuracy:69.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(236090) 236090
339136 ; loss 0.71 accuracy:69.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(240595) 240595
345536 ; loss 0.7 accuracy:69.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244984) 244984
351936 ; loss 0.72 accuracy:69.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249514) 249514
358336 ; loss 0.7 accuracy:69.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254026) 254026
364736 ; loss 0.7 accuracy:69.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(258530) 258530
371136 ; loss 0.69 accuracy:69.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(263041) 263041
377536 ; loss 0.7 accuracy:69.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267587) 267587
383936 ; loss 0.7 accuracy:69.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(272162) 272162
390336 ; loss 0.69 accuracy:69.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(276606) 276606
396736 ; loss 0.71 accuracy:69.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(281095) 281095
403136 ; loss 0.7 accuracy:69.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(285633) 285633
409536 ; loss 0.7 accuracy:69.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(290189) 290189
415936 ; loss 0.7 accuracy:69.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(294646) 294646
422336 ; loss 0.7 accuracy:69.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(299179) 299179
428736 ; loss 0.71 accuracy:69.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(303693) 303693
435136 ; loss 0.7 accuracy:69.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(308243) 308243
441536 ; loss 0.7 accuracy:69.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(312726) 312726
447936 ; loss 0.7 accuracy:69.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(317226) 317226
454336 ; loss 0.7 accuracy:69.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(321722) 321722
460736 ; loss 0.7 accuracy:69.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326234) 326234
467136 ; loss 0.7 accuracy:69.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(330712) 330712
473536 ; loss 0.7 accuracy:69.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(335257) 335257
479936 ; loss 0.69 accuracy:69.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(339751) 339751
486336 ; loss 0.71 accuracy:69.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(344232) 344232
492736 ; loss 0.69 accuracy:69.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(348787) 348787
499136 ; loss 0.69 accuracy:69.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(353289) 353289
505536 ; loss 0.7 accuracy:69.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(357846) 357846
511936 ; loss 0.69 accuracy:69.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(362412) 362412
518336 ; loss 0.69 accuracy:69.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(366938) 366938
524736 ; loss 0.7 accuracy:69.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(371616) 371616
531136 ; loss 0.68 accuracy:69.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(376203) 376203
537536 ; loss 0.68 accuracy:69.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(380790) 380790
543936 ; loss 0.68 accuracy:70.0 ;
results : epoch 8 ; mean accuracy train : 70.0

VALIDATION : Epoch 8
togrep : results : epoch 8 ; mean accuracy valid :              72.54
saving model at epoch 8

TRAINING : Epoch 9
Learning rate : 0.09227446944279201
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4601) 4601
6336 ; loss 0.68 accuracy:71.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9141) 9141
12736 ; loss 0.7 accuracy:71.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(13729) 13729
19136 ; loss 0.68 accuracy:71.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(18296) 18296
25536 ; loss 0.68 accuracy:71.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(22865) 22865
31936 ; loss 0.68 accuracy:71.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(27419) 27419
38336 ; loss 0.68 accuracy:71.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32016) 32016
44736 ; loss 0.68 accuracy:71.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(36577) 36577
51136 ; loss 0.69 accuracy:71.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41085) 41085
57536 ; loss 0.69 accuracy:71.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45635) 45635
63936 ; loss 0.68 accuracy:71.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50286) 50286
70336 ; loss 0.68 accuracy:71.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54831) 54831
76736 ; loss 0.69 accuracy:71.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(59421) 59421
83136 ; loss 0.69 accuracy:71.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64054) 64054
89536 ; loss 0.67 accuracy:71.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(68659) 68659
95936 ; loss 0.68 accuracy:71.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73252) 73252
102336 ; loss 0.68 accuracy:71.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77840) 77840
108736 ; loss 0.68 accuracy:71.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82387) 82387
115136 ; loss 0.68 accuracy:71.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86974) 86974
121536 ; loss 0.68 accuracy:71.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(91531) 91531
127936 ; loss 0.68 accuracy:71.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96161) 96161
134336 ; loss 0.68 accuracy:71.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(100771) 100771
140736 ; loss 0.67 accuracy:71.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105383) 105383
147136 ; loss 0.67 accuracy:71.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109994) 109994
153536 ; loss 0.67 accuracy:71.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114599) 114599
159936 ; loss 0.68 accuracy:71.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119181) 119181
166336 ; loss 0.67 accuracy:71.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(123771) 123771
172736 ; loss 0.67 accuracy:71.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128368) 128368
179136 ; loss 0.67 accuracy:71.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(132923) 132923
185536 ; loss 0.68 accuracy:71.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137502) 137502
191936 ; loss 0.68 accuracy:71.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(142068) 142068
198336 ; loss 0.69 accuracy:71.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146694) 146694
204736 ; loss 0.67 accuracy:71.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151297) 151297
211136 ; loss 0.67 accuracy:71.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155893) 155893
217536 ; loss 0.68 accuracy:71.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160484) 160484
223936 ; loss 0.68 accuracy:71.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165112) 165112
230336 ; loss 0.67 accuracy:71.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169730) 169730
236736 ; loss 0.67 accuracy:71.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174332) 174332
243136 ; loss 0.67 accuracy:71.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178917) 178917
249536 ; loss 0.68 accuracy:71.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183539) 183539
255936 ; loss 0.66 accuracy:71.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(188171) 188171
262336 ; loss 0.67 accuracy:71.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192807) 192807
268736 ; loss 0.67 accuracy:71.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197383) 197383
275136 ; loss 0.68 accuracy:71.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(201964) 201964
281536 ; loss 0.67 accuracy:71.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206643) 206643
287936 ; loss 0.65 accuracy:71.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(211242) 211242
294336 ; loss 0.68 accuracy:71.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215842) 215842
300736 ; loss 0.68 accuracy:71.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(220409) 220409
307136 ; loss 0.67 accuracy:71.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225024) 225024
313536 ; loss 0.66 accuracy:71.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(229657) 229657
319936 ; loss 0.67 accuracy:71.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234230) 234230
326336 ; loss 0.68 accuracy:71.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(238865) 238865
332736 ; loss 0.67 accuracy:71.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243477) 243477
339136 ; loss 0.68 accuracy:71.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(248101) 248101
345536 ; loss 0.66 accuracy:71.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252710) 252710
351936 ; loss 0.66 accuracy:71.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257308) 257308
358336 ; loss 0.67 accuracy:71.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(261964) 261964
364736 ; loss 0.66 accuracy:71.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(266706) 266706
371136 ; loss 0.64 accuracy:71.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(271300) 271300
377536 ; loss 0.66 accuracy:71.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(275965) 275965
383936 ; loss 0.66 accuracy:71.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(280549) 280549
390336 ; loss 0.66 accuracy:71.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(285164) 285164
396736 ; loss 0.67 accuracy:71.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(289830) 289830
403136 ; loss 0.66 accuracy:71.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(294456) 294456
409536 ; loss 0.66 accuracy:71.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(299082) 299082
415936 ; loss 0.66 accuracy:71.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(303741) 303741
422336 ; loss 0.66 accuracy:71.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(308415) 308415
428736 ; loss 0.65 accuracy:71.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(313078) 313078
435136 ; loss 0.66 accuracy:71.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(317729) 317729
441536 ; loss 0.65 accuracy:71.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(322325) 322325
447936 ; loss 0.68 accuracy:71.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326900) 326900
454336 ; loss 0.67 accuracy:71.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(331521) 331521
460736 ; loss 0.66 accuracy:71.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(336177) 336177
467136 ; loss 0.66 accuracy:71.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(340820) 340820
473536 ; loss 0.66 accuracy:71.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(345458) 345458
479936 ; loss 0.66 accuracy:71.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(350185) 350185
486336 ; loss 0.64 accuracy:72.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(354835) 354835
492736 ; loss 0.66 accuracy:72.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(359508) 359508
499136 ; loss 0.65 accuracy:72.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(364129) 364129
505536 ; loss 0.66 accuracy:72.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(368850) 368850
511936 ; loss 0.65 accuracy:72.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(373546) 373546
518336 ; loss 0.65 accuracy:72.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(378152) 378152
524736 ; loss 0.67 accuracy:72.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(382852) 382852
531136 ; loss 0.65 accuracy:72.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(387465) 387465
537536 ; loss 0.67 accuracy:72.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(392130) 392130
543936 ; loss 0.65 accuracy:72.08 ;
results : epoch 9 ; mean accuracy train : 72.1

VALIDATION : Epoch 9
togrep : results : epoch 9 ; mean accuracy valid :              73.95
saving model at epoch 9

TRAINING : Epoch 10
Learning rate : 0.09135172474836409
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4722) 4722
6336 ; loss 0.64 accuracy:73.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9422) 9422
12736 ; loss 0.64 accuracy:73.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(14111) 14111
19136 ; loss 0.65 accuracy:73.49 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(18791) 18791
25536 ; loss 0.66 accuracy:73.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(23486) 23486
31936 ; loss 0.65 accuracy:73.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28212) 28212
38336 ; loss 0.64 accuracy:73.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(32919) 32919
44736 ; loss 0.64 accuracy:73.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(37598) 37598
51136 ; loss 0.65 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(42298) 42298
57536 ; loss 0.65 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46964) 46964
63936 ; loss 0.64 accuracy:73.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51659) 51659
70336 ; loss 0.65 accuracy:73.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(56370) 56370
76736 ; loss 0.64 accuracy:73.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61134) 61134
83136 ; loss 0.63 accuracy:73.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(65829) 65829
89536 ; loss 0.64 accuracy:73.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70527) 70527
95936 ; loss 0.64 accuracy:73.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(75175) 75175
102336 ; loss 0.66 accuracy:73.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79880) 79880
108736 ; loss 0.64 accuracy:73.42 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(84515) 84515
115136 ; loss 0.66 accuracy:73.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(89201) 89201
121536 ; loss 0.66 accuracy:73.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93931) 93931
127936 ; loss 0.64 accuracy:73.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98629) 98629
134336 ; loss 0.65 accuracy:73.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103346) 103346
140736 ; loss 0.65 accuracy:73.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108049) 108049
147136 ; loss 0.64 accuracy:73.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112716) 112716
153536 ; loss 0.66 accuracy:73.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117453) 117453
159936 ; loss 0.64 accuracy:73.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122154) 122154
166336 ; loss 0.64 accuracy:73.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(126901) 126901
172736 ; loss 0.64 accuracy:73.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131578) 131578
179136 ; loss 0.64 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(136165) 136165
185536 ; loss 0.67 accuracy:73.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140882) 140882
191936 ; loss 0.64 accuracy:73.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145629) 145629
198336 ; loss 0.63 accuracy:73.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150312) 150312
204736 ; loss 0.66 accuracy:73.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155023) 155023
211136 ; loss 0.64 accuracy:73.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(159783) 159783
217536 ; loss 0.63 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164480) 164480
223936 ; loss 0.64 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169132) 169132
230336 ; loss 0.65 accuracy:73.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(173888) 173888
236736 ; loss 0.64 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178593) 178593
243136 ; loss 0.63 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183269) 183269
249536 ; loss 0.65 accuracy:73.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(188018) 188018
255936 ; loss 0.63 accuracy:73.44 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192733) 192733
262336 ; loss 0.64 accuracy:73.45 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197462) 197462
268736 ; loss 0.64 accuracy:73.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(202200) 202200
275136 ; loss 0.63 accuracy:73.47 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206909) 206909
281536 ; loss 0.64 accuracy:73.48 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(211696) 211696
287936 ; loss 0.63 accuracy:73.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216456) 216456
294336 ; loss 0.63 accuracy:73.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(221150) 221150
300736 ; loss 0.66 accuracy:73.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225921) 225921
307136 ; loss 0.63 accuracy:73.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230574) 230574
313536 ; loss 0.64 accuracy:73.52 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235342) 235342
319936 ; loss 0.63 accuracy:73.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(240038) 240038
326336 ; loss 0.64 accuracy:73.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244852) 244852
332736 ; loss 0.62 accuracy:73.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249664) 249664
339136 ; loss 0.61 accuracy:73.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254371) 254371
345536 ; loss 0.64 accuracy:73.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(259067) 259067
351936 ; loss 0.63 accuracy:73.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(263762) 263762
358336 ; loss 0.63 accuracy:73.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268518) 268518
364736 ; loss 0.63 accuracy:73.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(273234) 273234
371136 ; loss 0.64 accuracy:73.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(277972) 277972
377536 ; loss 0.63 accuracy:73.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(282728) 282728
383936 ; loss 0.62 accuracy:73.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(287426) 287426
390336 ; loss 0.64 accuracy:73.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(292205) 292205
396736 ; loss 0.63 accuracy:73.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(296925) 296925
403136 ; loss 0.64 accuracy:73.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(301706) 301706
409536 ; loss 0.62 accuracy:73.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(306477) 306477
415936 ; loss 0.63 accuracy:73.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(311231) 311231
422336 ; loss 0.62 accuracy:73.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(315994) 315994
428736 ; loss 0.62 accuracy:73.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(320708) 320708
435136 ; loss 0.64 accuracy:73.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(325454) 325454
441536 ; loss 0.63 accuracy:73.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(330236) 330236
447936 ; loss 0.62 accuracy:73.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(335020) 335020
454336 ; loss 0.62 accuracy:73.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(339764) 339764
460736 ; loss 0.63 accuracy:73.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(344523) 344523
467136 ; loss 0.62 accuracy:73.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(349233) 349233
473536 ; loss 0.64 accuracy:73.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(353985) 353985
479936 ; loss 0.62 accuracy:73.75 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(358793) 358793
486336 ; loss 0.62 accuracy:73.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(363478) 363478
492736 ; loss 0.66 accuracy:73.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(368246) 368246
499136 ; loss 0.62 accuracy:73.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(373061) 373061
505536 ; loss 0.62 accuracy:73.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(377843) 377843
511936 ; loss 0.63 accuracy:73.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(382594) 382594
518336 ; loss 0.63 accuracy:73.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(387343) 387343
524736 ; loss 0.64 accuracy:73.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(392094) 392094
531136 ; loss 0.63 accuracy:73.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(396867) 396867
537536 ; loss 0.63 accuracy:73.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(401648) 401648
543936 ; loss 0.62 accuracy:73.83 ;
results : epoch 10 ; mean accuracy train : 73.84

VALIDATION : Epoch 10
togrep : results : epoch 10 ; mean accuracy valid :              75.02
saving model at epoch 10

TRAINING : Epoch 11
Learning rate : 0.09043820750088044
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4782) 4782
6336 ; loss 0.62 accuracy:74.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9565) 9565
12736 ; loss 0.62 accuracy:74.73 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(14353) 14353
19136 ; loss 0.62 accuracy:74.76 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19164) 19164
25536 ; loss 0.62 accuracy:74.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(23955) 23955
31936 ; loss 0.61 accuracy:74.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(28759) 28759
38336 ; loss 0.62 accuracy:74.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(33541) 33541
44736 ; loss 0.62 accuracy:74.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38352) 38352
51136 ; loss 0.61 accuracy:74.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(43140) 43140
57536 ; loss 0.62 accuracy:74.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(47892) 47892
63936 ; loss 0.62 accuracy:74.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(52700) 52700
70336 ; loss 0.61 accuracy:74.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57428) 57428
76736 ; loss 0.63 accuracy:74.78 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(62222) 62222
83136 ; loss 0.61 accuracy:74.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67025) 67025
89536 ; loss 0.61 accuracy:74.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(71826) 71826
95936 ; loss 0.61 accuracy:74.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76628) 76628
102336 ; loss 0.61 accuracy:74.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(81436) 81436
108736 ; loss 0.62 accuracy:74.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86218) 86218
115136 ; loss 0.62 accuracy:74.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90952) 90952
121536 ; loss 0.63 accuracy:74.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95793) 95793
127936 ; loss 0.6 accuracy:74.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(100639) 100639
134336 ; loss 0.61 accuracy:74.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105380) 105380
140736 ; loss 0.63 accuracy:74.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(110200) 110200
147136 ; loss 0.61 accuracy:74.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114959) 114959
153536 ; loss 0.62 accuracy:74.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119728) 119728
159936 ; loss 0.62 accuracy:74.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124593) 124593
166336 ; loss 0.6 accuracy:74.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(129409) 129409
172736 ; loss 0.6 accuracy:74.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134200) 134200
179136 ; loss 0.6 accuracy:74.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138978) 138978
185536 ; loss 0.62 accuracy:74.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(143802) 143802
191936 ; loss 0.61 accuracy:74.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148633) 148633
198336 ; loss 0.6 accuracy:74.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(153461) 153461
204736 ; loss 0.6 accuracy:74.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(158240) 158240
211136 ; loss 0.61 accuracy:74.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163049) 163049
217536 ; loss 0.6 accuracy:74.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(167820) 167820
223936 ; loss 0.62 accuracy:74.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172638) 172638
230336 ; loss 0.61 accuracy:74.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177409) 177409
236736 ; loss 0.62 accuracy:74.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182198) 182198
243136 ; loss 0.61 accuracy:74.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187029) 187029
249536 ; loss 0.6 accuracy:74.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191841) 191841
255936 ; loss 0.61 accuracy:74.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196665) 196665
262336 ; loss 0.61 accuracy:74.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(201463) 201463
268736 ; loss 0.61 accuracy:74.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206289) 206289
275136 ; loss 0.61 accuracy:74.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(211100) 211100
281536 ; loss 0.6 accuracy:74.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215912) 215912
287936 ; loss 0.6 accuracy:74.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(220749) 220749
294336 ; loss 0.61 accuracy:74.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225548) 225548
300736 ; loss 0.61 accuracy:74.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230305) 230305
307136 ; loss 0.63 accuracy:74.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235144) 235144
313536 ; loss 0.6 accuracy:74.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239932) 239932
319936 ; loss 0.61 accuracy:74.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244737) 244737
326336 ; loss 0.62 accuracy:74.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249540) 249540
332736 ; loss 0.61 accuracy:74.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254367) 254367
339136 ; loss 0.61 accuracy:74.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(259144) 259144
345536 ; loss 0.62 accuracy:74.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264002) 264002
351936 ; loss 0.6 accuracy:75.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268807) 268807
358336 ; loss 0.61 accuracy:75.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(273638) 273638
364736 ; loss 0.6 accuracy:75.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(278545) 278545
371136 ; loss 0.6 accuracy:75.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(283313) 283313
377536 ; loss 0.62 accuracy:75.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(288139) 288139
383936 ; loss 0.6 accuracy:75.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(292950) 292950
390336 ; loss 0.61 accuracy:75.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(297778) 297778
396736 ; loss 0.61 accuracy:75.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(302618) 302618
403136 ; loss 0.6 accuracy:75.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(307464) 307464
409536 ; loss 0.61 accuracy:75.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(312312) 312312
415936 ; loss 0.6 accuracy:75.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(317124) 317124
422336 ; loss 0.61 accuracy:75.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(321927) 321927
428736 ; loss 0.61 accuracy:75.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326775) 326775
435136 ; loss 0.59 accuracy:75.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(331600) 331600
441536 ; loss 0.61 accuracy:75.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(336457) 336457
447936 ; loss 0.6 accuracy:75.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(341346) 341346
454336 ; loss 0.59 accuracy:75.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(346194) 346194
460736 ; loss 0.6 accuracy:75.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(351009) 351009
467136 ; loss 0.61 accuracy:75.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(355864) 355864
473536 ; loss 0.6 accuracy:75.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(360758) 360758
479936 ; loss 0.58 accuracy:75.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(365575) 365575
486336 ; loss 0.61 accuracy:75.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(370385) 370385
492736 ; loss 0.61 accuracy:75.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(375214) 375214
499136 ; loss 0.59 accuracy:75.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(380019) 380019
505536 ; loss 0.61 accuracy:75.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(384852) 384852
511936 ; loss 0.6 accuracy:75.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(389734) 389734
518336 ; loss 0.59 accuracy:75.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(394555) 394555
524736 ; loss 0.61 accuracy:75.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(399397) 399397
531136 ; loss 0.6 accuracy:75.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(404274) 404274
537536 ; loss 0.6 accuracy:75.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(409095) 409095
543936 ; loss 0.6 accuracy:75.2 ;
results : epoch 11 ; mean accuracy train : 75.21

VALIDATION : Epoch 11
togrep : results : epoch 11 ; mean accuracy valid :              76.97
saving model at epoch 11

TRAINING : Epoch 12
Learning rate : 0.08953382542587164
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4843) 4843
6336 ; loss 0.6 accuracy:75.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9753) 9753
12736 ; loss 0.58 accuracy:76.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(14599) 14599
19136 ; loss 0.59 accuracy:76.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19456) 19456
25536 ; loss 0.59 accuracy:76.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(24336) 24336
31936 ; loss 0.59 accuracy:76.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(29196) 29196
38336 ; loss 0.6 accuracy:76.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34099) 34099
44736 ; loss 0.59 accuracy:76.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(38995) 38995
51136 ; loss 0.58 accuracy:76.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(43904) 43904
57536 ; loss 0.58 accuracy:76.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(48767) 48767
63936 ; loss 0.6 accuracy:76.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(53675) 53675
70336 ; loss 0.58 accuracy:76.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(58489) 58489
76736 ; loss 0.6 accuracy:76.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(63377) 63377
83136 ; loss 0.59 accuracy:76.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(68332) 68332
89536 ; loss 0.58 accuracy:76.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(73192) 73192
95936 ; loss 0.6 accuracy:76.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(78038) 78038
102336 ; loss 0.61 accuracy:76.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82962) 82962
108736 ; loss 0.58 accuracy:76.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87866) 87866
115136 ; loss 0.58 accuracy:76.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92696) 92696
121536 ; loss 0.6 accuracy:76.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(97552) 97552
127936 ; loss 0.58 accuracy:76.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(102411) 102411
134336 ; loss 0.59 accuracy:76.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(107239) 107239
140736 ; loss 0.59 accuracy:76.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112136) 112136
147136 ; loss 0.58 accuracy:76.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117014) 117014
153536 ; loss 0.59 accuracy:76.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121864) 121864
159936 ; loss 0.6 accuracy:76.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(126735) 126735
166336 ; loss 0.6 accuracy:76.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131524) 131524
172736 ; loss 0.6 accuracy:76.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(136375) 136375
179136 ; loss 0.6 accuracy:76.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141273) 141273
185536 ; loss 0.58 accuracy:76.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146227) 146227
191936 ; loss 0.58 accuracy:76.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151124) 151124
198336 ; loss 0.58 accuracy:76.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156006) 156006
204736 ; loss 0.58 accuracy:76.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160904) 160904
211136 ; loss 0.58 accuracy:76.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165802) 165802
217536 ; loss 0.59 accuracy:76.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170673) 170673
223936 ; loss 0.59 accuracy:76.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(175533) 175533
230336 ; loss 0.6 accuracy:76.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180451) 180451
236736 ; loss 0.58 accuracy:76.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185352) 185352
243136 ; loss 0.59 accuracy:76.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190249) 190249
249536 ; loss 0.58 accuracy:76.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195084) 195084
255936 ; loss 0.6 accuracy:76.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200019) 200019
262336 ; loss 0.57 accuracy:76.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(204963) 204963
268736 ; loss 0.57 accuracy:76.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209899) 209899
275136 ; loss 0.57 accuracy:76.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(214827) 214827
281536 ; loss 0.58 accuracy:76.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219754) 219754
287936 ; loss 0.57 accuracy:76.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(224658) 224658
294336 ; loss 0.58 accuracy:76.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(229558) 229558
300736 ; loss 0.57 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234439) 234439
307136 ; loss 0.59 accuracy:76.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239276) 239276
313536 ; loss 0.59 accuracy:76.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244090) 244090
319936 ; loss 0.59 accuracy:76.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(248925) 248925
326336 ; loss 0.59 accuracy:76.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(253804) 253804
332736 ; loss 0.58 accuracy:76.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(258667) 258667
339136 ; loss 0.59 accuracy:76.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(263549) 263549
345536 ; loss 0.59 accuracy:76.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(268477) 268477
351936 ; loss 0.58 accuracy:76.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(273332) 273332
358336 ; loss 0.58 accuracy:76.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(278250) 278250
364736 ; loss 0.59 accuracy:76.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(283156) 283156
371136 ; loss 0.59 accuracy:76.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(288021) 288021
377536 ; loss 0.59 accuracy:76.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(292918) 292918
383936 ; loss 0.58 accuracy:76.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(297799) 297799
390336 ; loss 0.59 accuracy:76.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(302668) 302668
396736 ; loss 0.58 accuracy:76.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(307621) 307621
403136 ; loss 0.57 accuracy:76.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(312598) 312598
409536 ; loss 0.56 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(317474) 317474
415936 ; loss 0.59 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(322385) 322385
422336 ; loss 0.57 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(327271) 327271
428736 ; loss 0.58 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(332160) 332160
435136 ; loss 0.59 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(337081) 337081
441536 ; loss 0.58 accuracy:76.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(341977) 341977
447936 ; loss 0.58 accuracy:76.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(346811) 346811
454336 ; loss 0.59 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(351662) 351662
460736 ; loss 0.6 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(356577) 356577
467136 ; loss 0.57 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(361440) 361440
473536 ; loss 0.58 accuracy:76.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(366400) 366400
479936 ; loss 0.56 accuracy:76.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(371343) 371343
486336 ; loss 0.57 accuracy:76.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(376186) 376186
492736 ; loss 0.6 accuracy:76.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(381114) 381114
499136 ; loss 0.58 accuracy:76.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(386028) 386028
505536 ; loss 0.58 accuracy:76.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(390909) 390909
511936 ; loss 0.58 accuracy:76.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(395822) 395822
518336 ; loss 0.59 accuracy:76.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(400783) 400783
524736 ; loss 0.56 accuracy:76.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(405695) 405695
531136 ; loss 0.58 accuracy:76.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(410598) 410598
537536 ; loss 0.57 accuracy:76.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(415509) 415509
543936 ; loss 0.57 accuracy:76.38 ;
results : epoch 12 ; mean accuracy train : 76.38

VALIDATION : Epoch 12
togrep : results : epoch 12 ; mean accuracy valid :              77.48
saving model at epoch 12

TRAINING : Epoch 13
Learning rate : 0.08863848717161292
&lt;class &#39;torch.Tensor&#39;&gt; tensor(4943) 4943
6336 ; loss 0.57 accuracy:77.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9854) 9854
12736 ; loss 0.59 accuracy:76.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(14812) 14812
19136 ; loss 0.57 accuracy:77.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(19702) 19702
25536 ; loss 0.58 accuracy:76.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(24663) 24663
31936 ; loss 0.56 accuracy:77.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(29614) 29614
38336 ; loss 0.56 accuracy:77.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34524) 34524
44736 ; loss 0.57 accuracy:77.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(39469) 39469
51136 ; loss 0.56 accuracy:77.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44415) 44415
57536 ; loss 0.57 accuracy:77.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(49368) 49368
63936 ; loss 0.56 accuracy:77.14 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54354) 54354
70336 ; loss 0.55 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(59274) 59274
76736 ; loss 0.57 accuracy:77.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64239) 64239
83136 ; loss 0.57 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(69144) 69144
89536 ; loss 0.57 accuracy:77.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74084) 74084
95936 ; loss 0.58 accuracy:77.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79021) 79021
102336 ; loss 0.57 accuracy:77.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83985) 83985
108736 ; loss 0.57 accuracy:77.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(88849) 88849
115136 ; loss 0.58 accuracy:77.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93725) 93725
121536 ; loss 0.58 accuracy:77.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98687) 98687
127936 ; loss 0.56 accuracy:77.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103665) 103665
134336 ; loss 0.56 accuracy:77.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108598) 108598
140736 ; loss 0.57 accuracy:77.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(113629) 113629
147136 ; loss 0.54 accuracy:77.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(118600) 118600
153536 ; loss 0.57 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(123511) 123511
159936 ; loss 0.58 accuracy:77.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128464) 128464
166336 ; loss 0.56 accuracy:77.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133392) 133392
172736 ; loss 0.57 accuracy:77.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138344) 138344
179136 ; loss 0.56 accuracy:77.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(143321) 143321
185536 ; loss 0.56 accuracy:77.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(148291) 148291
191936 ; loss 0.56 accuracy:77.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(153204) 153204
198336 ; loss 0.56 accuracy:77.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(158119) 158119
204736 ; loss 0.58 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163088) 163088
211136 ; loss 0.56 accuracy:77.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(168043) 168043
217536 ; loss 0.56 accuracy:77.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(172952) 172952
223936 ; loss 0.58 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177861) 177861
230336 ; loss 0.59 accuracy:77.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182818) 182818
236736 ; loss 0.57 accuracy:77.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187743) 187743
243136 ; loss 0.57 accuracy:77.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192718) 192718
249536 ; loss 0.56 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197686) 197686
255936 ; loss 0.55 accuracy:77.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(202593) 202593
262336 ; loss 0.57 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(207547) 207547
268736 ; loss 0.56 accuracy:77.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212533) 212533
275136 ; loss 0.55 accuracy:77.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(217504) 217504
281536 ; loss 0.55 accuracy:77.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222415) 222415
287936 ; loss 0.56 accuracy:77.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227410) 227410
294336 ; loss 0.56 accuracy:77.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(232374) 232374
300736 ; loss 0.57 accuracy:77.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(237323) 237323
307136 ; loss 0.56 accuracy:77.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(242347) 242347
313536 ; loss 0.55 accuracy:77.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(247316) 247316
319936 ; loss 0.56 accuracy:77.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252257) 252257
326336 ; loss 0.57 accuracy:77.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257263) 257263
332736 ; loss 0.56 accuracy:77.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(262217) 262217
339136 ; loss 0.55 accuracy:77.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267171) 267171
345536 ; loss 0.56 accuracy:77.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(272112) 272112
351936 ; loss 0.58 accuracy:77.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(277068) 277068
358336 ; loss 0.56 accuracy:77.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(282079) 282079
364736 ; loss 0.55 accuracy:77.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(287073) 287073
371136 ; loss 0.56 accuracy:77.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(291961) 291961
377536 ; loss 0.57 accuracy:77.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(296927) 296927
383936 ; loss 0.56 accuracy:77.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(301879) 301879
390336 ; loss 0.56 accuracy:77.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(306820) 306820
396736 ; loss 0.57 accuracy:77.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(311776) 311776
403136 ; loss 0.56 accuracy:77.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(316679) 316679
409536 ; loss 0.57 accuracy:77.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(321663) 321663
415936 ; loss 0.55 accuracy:77.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326646) 326646
422336 ; loss 0.56 accuracy:77.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(331605) 331605
428736 ; loss 0.56 accuracy:77.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(336576) 336576
435136 ; loss 0.56 accuracy:77.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(341592) 341592
441536 ; loss 0.55 accuracy:77.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(346565) 346565
447936 ; loss 0.56 accuracy:77.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(351515) 351515
454336 ; loss 0.56 accuracy:77.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(356419) 356419
460736 ; loss 0.58 accuracy:77.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(361399) 361399
467136 ; loss 0.57 accuracy:77.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(366265) 366265
473536 ; loss 0.58 accuracy:77.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(371252) 371252
479936 ; loss 0.54 accuracy:77.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(376231) 376231
486336 ; loss 0.56 accuracy:77.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(381227) 381227
492736 ; loss 0.56 accuracy:77.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(386171) 386171
499136 ; loss 0.57 accuracy:77.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(391142) 391142
505536 ; loss 0.55 accuracy:77.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(396132) 396132
511936 ; loss 0.56 accuracy:77.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(401121) 401121
518336 ; loss 0.56 accuracy:77.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(406102) 406102
524736 ; loss 0.55 accuracy:77.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(411081) 411081
531136 ; loss 0.55 accuracy:77.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(416055) 416055
537536 ; loss 0.55 accuracy:77.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(421058) 421058
543936 ; loss 0.55 accuracy:77.4 ;
results : epoch 13 ; mean accuracy train : 77.4

VALIDATION : Epoch 13
togrep : results : epoch 13 ; mean accuracy valid :              78.25
saving model at epoch 13

TRAINING : Epoch 14
Learning rate : 0.08775210229989679
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5000) 5000
6336 ; loss 0.55 accuracy:78.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(9981) 9981
12736 ; loss 0.56 accuracy:77.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15022) 15022
19136 ; loss 0.54 accuracy:78.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20055) 20055
25536 ; loss 0.54 accuracy:78.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25032) 25032
31936 ; loss 0.55 accuracy:78.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(29998) 29998
38336 ; loss 0.57 accuracy:78.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(34974) 34974
44736 ; loss 0.56 accuracy:78.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(39969) 39969
51136 ; loss 0.55 accuracy:78.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(44907) 44907
57536 ; loss 0.56 accuracy:77.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(49936) 49936
63936 ; loss 0.54 accuracy:78.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(54919) 54919
70336 ; loss 0.55 accuracy:78.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(59900) 59900
76736 ; loss 0.55 accuracy:77.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(64863) 64863
83136 ; loss 0.57 accuracy:77.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(69898) 69898
89536 ; loss 0.54 accuracy:78.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(74914) 74914
95936 ; loss 0.54 accuracy:78.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(79934) 79934
102336 ; loss 0.55 accuracy:78.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(84988) 84988
108736 ; loss 0.53 accuracy:78.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90006) 90006
115136 ; loss 0.54 accuracy:78.13 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95038) 95038
121536 ; loss 0.54 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(100042) 100042
127936 ; loss 0.54 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(104995) 104995
134336 ; loss 0.56 accuracy:78.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109934) 109934
140736 ; loss 0.56 accuracy:78.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114988) 114988
147136 ; loss 0.54 accuracy:78.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119992) 119992
153536 ; loss 0.55 accuracy:78.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124988) 124988
159936 ; loss 0.54 accuracy:78.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(129992) 129992
166336 ; loss 0.55 accuracy:78.12 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135040) 135040
172736 ; loss 0.53 accuracy:78.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140086) 140086
179136 ; loss 0.54 accuracy:78.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145141) 145141
185536 ; loss 0.53 accuracy:78.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150144) 150144
191936 ; loss 0.55 accuracy:78.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155178) 155178
198336 ; loss 0.54 accuracy:78.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160136) 160136
204736 ; loss 0.55 accuracy:78.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165124) 165124
211136 ; loss 0.55 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(170082) 170082
217536 ; loss 0.56 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(175067) 175067
223936 ; loss 0.56 accuracy:78.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180077) 180077
230336 ; loss 0.53 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185082) 185082
236736 ; loss 0.55 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190106) 190106
243136 ; loss 0.54 accuracy:78.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195097) 195097
249536 ; loss 0.55 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200143) 200143
255936 ; loss 0.53 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(205122) 205122
262336 ; loss 0.56 accuracy:78.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(210151) 210151
268736 ; loss 0.54 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(215140) 215140
275136 ; loss 0.55 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(220097) 220097
281536 ; loss 0.56 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(225152) 225152
287936 ; loss 0.53 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(230175) 230175
294336 ; loss 0.54 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(235149) 235149
300736 ; loss 0.55 accuracy:78.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(240106) 240106
307136 ; loss 0.56 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(245071) 245071
313536 ; loss 0.56 accuracy:78.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(250089) 250089
319936 ; loss 0.53 accuracy:78.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(255087) 255087
326336 ; loss 0.55 accuracy:78.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(260090) 260090
332736 ; loss 0.54 accuracy:78.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(265111) 265111
339136 ; loss 0.54 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270093) 270093
345536 ; loss 0.56 accuracy:78.15 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(275117) 275117
351936 ; loss 0.54 accuracy:78.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(280167) 280167
358336 ; loss 0.53 accuracy:78.17 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(285207) 285207
364736 ; loss 0.55 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(290225) 290225
371136 ; loss 0.56 accuracy:78.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(295193) 295193
377536 ; loss 0.55 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(300207) 300207
383936 ; loss 0.55 accuracy:78.18 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(305264) 305264
390336 ; loss 0.54 accuracy:78.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(310267) 310267
396736 ; loss 0.55 accuracy:78.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(315277) 315277
403136 ; loss 0.54 accuracy:78.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(320294) 320294
409536 ; loss 0.54 accuracy:78.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(325344) 325344
415936 ; loss 0.53 accuracy:78.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(330364) 330364
422336 ; loss 0.55 accuracy:78.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(335404) 335404
428736 ; loss 0.54 accuracy:78.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(340352) 340352
435136 ; loss 0.56 accuracy:78.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(345370) 345370
441536 ; loss 0.54 accuracy:78.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(350376) 350376
447936 ; loss 0.55 accuracy:78.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(355424) 355424
454336 ; loss 0.54 accuracy:78.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(360451) 360451
460736 ; loss 0.54 accuracy:78.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(365460) 365460
467136 ; loss 0.54 accuracy:78.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(370466) 370466
473536 ; loss 0.55 accuracy:78.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(375465) 375465
479936 ; loss 0.55 accuracy:78.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(380506) 380506
486336 ; loss 0.53 accuracy:78.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(385526) 385526
492736 ; loss 0.53 accuracy:78.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(390512) 390512
499136 ; loss 0.54 accuracy:78.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(395557) 395557
505536 ; loss 0.54 accuracy:78.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(400542) 400542
511936 ; loss 0.55 accuracy:78.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(405578) 405578
518336 ; loss 0.53 accuracy:78.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(410607) 410607
524736 ; loss 0.55 accuracy:78.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(415684) 415684
531136 ; loss 0.54 accuracy:78.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(420717) 420717
537536 ; loss 0.53 accuracy:78.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(425723) 425723
543936 ; loss 0.55 accuracy:78.26 ;
results : epoch 14 ; mean accuracy train : 78.26

VALIDATION : Epoch 14
togrep : results : epoch 14 ; mean accuracy valid :              79.31
saving model at epoch 14

TRAINING : Epoch 15
Learning rate : 0.08687458127689782
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5050) 5050
6336 ; loss 0.52 accuracy:78.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10073) 10073
12736 ; loss 0.54 accuracy:78.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15156) 15156
19136 ; loss 0.53 accuracy:78.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20222) 20222
25536 ; loss 0.53 accuracy:78.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25229) 25229
31936 ; loss 0.54 accuracy:78.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(30279) 30279
38336 ; loss 0.54 accuracy:78.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35343) 35343
44736 ; loss 0.52 accuracy:78.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(40359) 40359
51136 ; loss 0.53 accuracy:78.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45438) 45438
57536 ; loss 0.52 accuracy:78.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50460) 50460
63936 ; loss 0.54 accuracy:78.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55489) 55489
70336 ; loss 0.54 accuracy:78.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(60531) 60531
76736 ; loss 0.54 accuracy:78.82 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(65535) 65535
83136 ; loss 0.53 accuracy:78.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(70594) 70594
89536 ; loss 0.52 accuracy:78.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(75588) 75588
95936 ; loss 0.54 accuracy:78.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(80657) 80657
102336 ; loss 0.53 accuracy:78.77 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(85736) 85736
108736 ; loss 0.52 accuracy:78.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(90782) 90782
115136 ; loss 0.53 accuracy:78.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(95837) 95837
121536 ; loss 0.53 accuracy:78.81 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(100856) 100856
127936 ; loss 0.53 accuracy:78.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(105895) 105895
134336 ; loss 0.53 accuracy:78.79 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(110944) 110944
140736 ; loss 0.53 accuracy:78.8 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(116034) 116034
147136 ; loss 0.52 accuracy:78.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(121076) 121076
153536 ; loss 0.53 accuracy:78.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(126135) 126135
159936 ; loss 0.53 accuracy:78.83 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(131243) 131243
166336 ; loss 0.52 accuracy:78.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(136298) 136298
172736 ; loss 0.53 accuracy:78.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(141377) 141377
179136 ; loss 0.53 accuracy:78.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(146449) 146449
185536 ; loss 0.52 accuracy:78.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151494) 151494
191936 ; loss 0.54 accuracy:78.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156530) 156530
198336 ; loss 0.54 accuracy:78.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161548) 161548
204736 ; loss 0.55 accuracy:78.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166586) 166586
211136 ; loss 0.54 accuracy:78.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(171632) 171632
217536 ; loss 0.52 accuracy:78.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(176649) 176649
223936 ; loss 0.53 accuracy:78.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(181656) 181656
230336 ; loss 0.53 accuracy:78.84 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186803) 186803
236736 ; loss 0.51 accuracy:78.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191807) 191807
243136 ; loss 0.55 accuracy:78.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(196872) 196872
249536 ; loss 0.52 accuracy:78.88 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(201903) 201903
255936 ; loss 0.54 accuracy:78.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(206922) 206922
262336 ; loss 0.55 accuracy:78.86 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212061) 212061
268736 ; loss 0.51 accuracy:78.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(217128) 217128
275136 ; loss 0.53 accuracy:78.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(222189) 222189
281536 ; loss 0.53 accuracy:78.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(227232) 227232
287936 ; loss 0.53 accuracy:78.9 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(232334) 232334
294336 ; loss 0.52 accuracy:78.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(237350) 237350
300736 ; loss 0.54 accuracy:78.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(242454) 242454
307136 ; loss 0.52 accuracy:78.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(247536) 247536
313536 ; loss 0.52 accuracy:78.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252645) 252645
319936 ; loss 0.51 accuracy:78.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257721) 257721
326336 ; loss 0.53 accuracy:78.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(262797) 262797
332736 ; loss 0.52 accuracy:78.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267875) 267875
339136 ; loss 0.52 accuracy:78.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(272894) 272894
345536 ; loss 0.54 accuracy:78.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(277911) 277911
351936 ; loss 0.54 accuracy:78.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(282967) 282967
358336 ; loss 0.53 accuracy:78.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(288013) 288013
364736 ; loss 0.54 accuracy:78.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(293074) 293074
371136 ; loss 0.53 accuracy:78.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(298111) 298111
377536 ; loss 0.53 accuracy:78.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(303183) 303183
383936 ; loss 0.53 accuracy:78.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(308281) 308281
390336 ; loss 0.52 accuracy:78.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(313325) 313325
396736 ; loss 0.53 accuracy:78.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(318422) 318422
403136 ; loss 0.52 accuracy:78.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(323500) 323500
409536 ; loss 0.52 accuracy:78.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(328529) 328529
415936 ; loss 0.55 accuracy:78.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(333600) 333600
422336 ; loss 0.53 accuracy:78.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(338619) 338619
428736 ; loss 0.53 accuracy:78.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(343740) 343740
435136 ; loss 0.52 accuracy:78.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(348843) 348843
441536 ; loss 0.52 accuracy:79.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(353879) 353879
447936 ; loss 0.53 accuracy:78.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(358975) 358975
454336 ; loss 0.52 accuracy:79.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(364069) 364069
460736 ; loss 0.52 accuracy:79.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(369118) 369118
467136 ; loss 0.53 accuracy:79.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(374211) 374211
473536 ; loss 0.52 accuracy:79.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(379295) 379295
479936 ; loss 0.52 accuracy:79.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(384424) 384424
486336 ; loss 0.52 accuracy:79.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(389478) 389478
492736 ; loss 0.53 accuracy:79.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(394520) 394520
499136 ; loss 0.52 accuracy:79.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(399566) 399566
505536 ; loss 0.53 accuracy:79.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(404608) 404608
511936 ; loss 0.53 accuracy:79.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(409617) 409617
518336 ; loss 0.54 accuracy:79.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(414703) 414703
524736 ; loss 0.53 accuracy:79.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(419840) 419840
531136 ; loss 0.51 accuracy:79.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(424904) 424904
537536 ; loss 0.53 accuracy:79.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(430028) 430028
543936 ; loss 0.52 accuracy:79.05 ;
results : epoch 15 ; mean accuracy train : 79.05

VALIDATION : Epoch 15
togrep : results : epoch 15 ; mean accuracy valid :              79.98
saving model at epoch 15

TRAINING : Epoch 16
Learning rate : 0.08600583546412884
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5082) 5082
6336 ; loss 0.52 accuracy:79.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10177) 10177
12736 ; loss 0.51 accuracy:79.51 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15235) 15235
19136 ; loss 0.52 accuracy:79.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20281) 20281
25536 ; loss 0.53 accuracy:79.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25400) 25400
31936 ; loss 0.51 accuracy:79.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(30482) 30482
38336 ; loss 0.52 accuracy:79.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35586) 35586
44736 ; loss 0.52 accuracy:79.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(40719) 40719
51136 ; loss 0.52 accuracy:79.53 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(45831) 45831
57536 ; loss 0.51 accuracy:79.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(50935) 50935
63936 ; loss 0.52 accuracy:79.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(55995) 55995
70336 ; loss 0.52 accuracy:79.54 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61098) 61098
76736 ; loss 0.51 accuracy:79.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66209) 66209
83136 ; loss 0.51 accuracy:79.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(71314) 71314
89536 ; loss 0.51 accuracy:79.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(76436) 76436
95936 ; loss 0.51 accuracy:79.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(81539) 81539
102336 ; loss 0.52 accuracy:79.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(86642) 86642
108736 ; loss 0.52 accuracy:79.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(91735) 91735
115136 ; loss 0.52 accuracy:79.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(96822) 96822
121536 ; loss 0.52 accuracy:79.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(101915) 101915
127936 ; loss 0.51 accuracy:79.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(107006) 107006
134336 ; loss 0.52 accuracy:79.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(112092) 112092
140736 ; loss 0.51 accuracy:79.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(117219) 117219
147136 ; loss 0.51 accuracy:79.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(122338) 122338
153536 ; loss 0.51 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(127437) 127437
159936 ; loss 0.51 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(132470) 132470
166336 ; loss 0.52 accuracy:79.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(137511) 137511
172736 ; loss 0.52 accuracy:79.58 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(142641) 142641
179136 ; loss 0.5 accuracy:79.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(147684) 147684
185536 ; loss 0.53 accuracy:79.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(152768) 152768
191936 ; loss 0.52 accuracy:79.57 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(157907) 157907
198336 ; loss 0.5 accuracy:79.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(163013) 163013
204736 ; loss 0.52 accuracy:79.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(168102) 168102
211136 ; loss 0.52 accuracy:79.59 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(173219) 173219
217536 ; loss 0.51 accuracy:79.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(178311) 178311
223936 ; loss 0.52 accuracy:79.6 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(183446) 183446
230336 ; loss 0.52 accuracy:79.62 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(188570) 188570
236736 ; loss 0.51 accuracy:79.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(193655) 193655
243136 ; loss 0.52 accuracy:79.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(198784) 198784
249536 ; loss 0.51 accuracy:79.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203914) 203914
255936 ; loss 0.51 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(209018) 209018
262336 ; loss 0.51 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(214125) 214125
268736 ; loss 0.52 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(219221) 219221
275136 ; loss 0.51 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(224322) 224322
281536 ; loss 0.51 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(229450) 229450
287936 ; loss 0.51 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234568) 234568
294336 ; loss 0.51 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239667) 239667
300736 ; loss 0.52 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244782) 244782
307136 ; loss 0.51 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249874) 249874
313536 ; loss 0.52 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254942) 254942
319936 ; loss 0.52 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(259998) 259998
326336 ; loss 0.52 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(265082) 265082
332736 ; loss 0.52 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270130) 270130
339136 ; loss 0.53 accuracy:79.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(275206) 275206
345536 ; loss 0.53 accuracy:79.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(280325) 280325
351936 ; loss 0.51 accuracy:79.64 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(285455) 285455
358336 ; loss 0.5 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(290557) 290557
364736 ; loss 0.52 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(295683) 295683
371136 ; loss 0.5 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(300750) 300750
377536 ; loss 0.52 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(305871) 305871
383936 ; loss 0.5 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(310982) 310982
390336 ; loss 0.52 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(316069) 316069
396736 ; loss 0.51 accuracy:79.65 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(321175) 321175
403136 ; loss 0.5 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326335) 326335
409536 ; loss 0.49 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(331438) 331438
415936 ; loss 0.5 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(336520) 336520
422336 ; loss 0.52 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(341648) 341648
428736 ; loss 0.51 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(346767) 346767
435136 ; loss 0.51 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(351824) 351824
441536 ; loss 0.51 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(356895) 356895
447936 ; loss 0.52 accuracy:79.66 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(362022) 362022
454336 ; loss 0.51 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(367116) 367116
460736 ; loss 0.52 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(372216) 372216
467136 ; loss 0.51 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(377317) 377317
473536 ; loss 0.51 accuracy:79.67 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(382459) 382459
479936 ; loss 0.5 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(387577) 387577
486336 ; loss 0.51 accuracy:79.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(392705) 392705
492736 ; loss 0.51 accuracy:79.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(397821) 397821
499136 ; loss 0.51 accuracy:79.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(402924) 402924
505536 ; loss 0.5 accuracy:79.69 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(408041) 408041
511936 ; loss 0.51 accuracy:79.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(413185) 413185
518336 ; loss 0.5 accuracy:79.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(418275) 418275
524736 ; loss 0.52 accuracy:79.7 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(423485) 423485
531136 ; loss 0.5 accuracy:79.72 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(428543) 428543
537536 ; loss 0.52 accuracy:79.71 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(433687) 433687
543936 ; loss 0.5 accuracy:79.72 ;
results : epoch 16 ; mean accuracy train : 79.72

VALIDATION : Epoch 16
togrep : results : epoch 16 ; mean accuracy valid :              80.3
saving model at epoch 16

TRAINING : Epoch 17
Learning rate : 0.08514577710948755
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5095) 5095
6336 ; loss 0.51 accuracy:79.61 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10249) 10249
12736 ; loss 0.5 accuracy:80.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15352) 15352
19136 ; loss 0.51 accuracy:79.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20460) 20460
25536 ; loss 0.5 accuracy:79.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25599) 25599
31936 ; loss 0.51 accuracy:80.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(30760) 30760
38336 ; loss 0.5 accuracy:80.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(35959) 35959
44736 ; loss 0.48 accuracy:80.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41096) 41096
51136 ; loss 0.51 accuracy:80.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46222) 46222
57536 ; loss 0.51 accuracy:80.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51327) 51327
63936 ; loss 0.5 accuracy:80.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(56457) 56457
70336 ; loss 0.51 accuracy:80.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(61653) 61653
76736 ; loss 0.49 accuracy:80.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(66798) 66798
83136 ; loss 0.5 accuracy:80.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(71978) 71978
89536 ; loss 0.49 accuracy:80.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77150) 77150
95936 ; loss 0.49 accuracy:80.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82269) 82269
102336 ; loss 0.5 accuracy:80.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(87404) 87404
108736 ; loss 0.5 accuracy:80.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(92569) 92569
115136 ; loss 0.5 accuracy:80.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(97733) 97733
121536 ; loss 0.48 accuracy:80.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(102866) 102866
127936 ; loss 0.5 accuracy:80.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(107975) 107975
134336 ; loss 0.51 accuracy:80.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(113041) 113041
140736 ; loss 0.52 accuracy:80.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(118213) 118213
147136 ; loss 0.49 accuracy:80.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(123394) 123394
153536 ; loss 0.5 accuracy:80.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(128540) 128540
159936 ; loss 0.5 accuracy:80.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(133656) 133656
166336 ; loss 0.5 accuracy:80.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(138776) 138776
172736 ; loss 0.51 accuracy:80.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(143925) 143925
179136 ; loss 0.5 accuracy:80.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(149030) 149030
185536 ; loss 0.53 accuracy:80.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(154159) 154159
191936 ; loss 0.5 accuracy:80.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(159363) 159363
198336 ; loss 0.48 accuracy:80.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(164519) 164519
204736 ; loss 0.5 accuracy:80.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(169644) 169644
211136 ; loss 0.51 accuracy:80.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(174830) 174830
217536 ; loss 0.48 accuracy:80.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(180051) 180051
223936 ; loss 0.49 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(185259) 185259
230336 ; loss 0.48 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(190379) 190379
236736 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(195513) 195513
243136 ; loss 0.5 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(200678) 200678
249536 ; loss 0.49 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(205815) 205815
255936 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(210915) 210915
262336 ; loss 0.51 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(216092) 216092
268736 ; loss 0.49 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(221259) 221259
275136 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(226430) 226430
281536 ; loss 0.49 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(231592) 231592
287936 ; loss 0.49 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(236721) 236721
294336 ; loss 0.51 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(241873) 241873
300736 ; loss 0.5 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(246966) 246966
307136 ; loss 0.51 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(252122) 252122
313536 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(257322) 257322
319936 ; loss 0.48 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(262467) 262467
326336 ; loss 0.49 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(267575) 267575
332736 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(272679) 272679
339136 ; loss 0.51 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(277851) 277851
345536 ; loss 0.48 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(282992) 282992
351936 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(288061) 288061
358336 ; loss 0.53 accuracy:80.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(293154) 293154
364736 ; loss 0.51 accuracy:80.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(298308) 298308
371136 ; loss 0.5 accuracy:80.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(303478) 303478
377536 ; loss 0.49 accuracy:80.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(308616) 308616
383936 ; loss 0.5 accuracy:80.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(313746) 313746
390336 ; loss 0.5 accuracy:80.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(318884) 318884
396736 ; loss 0.5 accuracy:80.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(324049) 324049
403136 ; loss 0.49 accuracy:80.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(329217) 329217
409536 ; loss 0.5 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(334384) 334384
415936 ; loss 0.49 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(339536) 339536
422336 ; loss 0.49 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(344707) 344707
428736 ; loss 0.5 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(349815) 349815
435136 ; loss 0.51 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(354967) 354967
441536 ; loss 0.5 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(360101) 360101
447936 ; loss 0.5 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(365276) 365276
454336 ; loss 0.49 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(370451) 370451
460736 ; loss 0.5 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(375617) 375617
467136 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(380702) 380702
473536 ; loss 0.52 accuracy:80.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(385887) 385887
479936 ; loss 0.48 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(391023) 391023
486336 ; loss 0.5 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(396182) 396182
492736 ; loss 0.5 accuracy:80.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(401337) 401337
499136 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(406487) 406487
505536 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(411645) 411645
511936 ; loss 0.5 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(416845) 416845
518336 ; loss 0.48 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(422011) 422011
524736 ; loss 0.49 accuracy:80.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(427106) 427106
531136 ; loss 0.51 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(432210) 432210
537536 ; loss 0.51 accuracy:80.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(437317) 437317
543936 ; loss 0.5 accuracy:80.39 ;
results : epoch 17 ; mean accuracy train : 80.39

VALIDATION : Epoch 17
togrep : results : epoch 17 ; mean accuracy valid :              80.2
Shrinking lr by : 5. New lr = 0.01702915542189751

TRAINING : Epoch 18
Learning rate : 0.016858863867678535
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5113) 5113
6336 ; loss 0.5 accuracy:79.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10264) 10264
12736 ; loss 0.49 accuracy:80.19 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15467) 15467
19136 ; loss 0.48 accuracy:80.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20641) 20641
25536 ; loss 0.49 accuracy:80.63 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25837) 25837
31936 ; loss 0.49 accuracy:80.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31006) 31006
38336 ; loss 0.49 accuracy:80.74 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(36219) 36219
44736 ; loss 0.48 accuracy:80.85 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41416) 41416
51136 ; loss 0.49 accuracy:80.89 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46605) 46605
57536 ; loss 0.49 accuracy:80.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(51780) 51780
63936 ; loss 0.49 accuracy:80.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(56935) 56935
70336 ; loss 0.5 accuracy:80.87 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(62141) 62141
76736 ; loss 0.48 accuracy:80.91 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67339) 67339
83136 ; loss 0.48 accuracy:80.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(72504) 72504
89536 ; loss 0.49 accuracy:80.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77711) 77711
95936 ; loss 0.47 accuracy:80.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(82862) 82862
102336 ; loss 0.49 accuracy:80.92 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(88060) 88060
108736 ; loss 0.48 accuracy:80.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93277) 93277
115136 ; loss 0.48 accuracy:80.97 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98449) 98449
121536 ; loss 0.49 accuracy:80.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103605) 103605
127936 ; loss 0.49 accuracy:80.94 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(108808) 108808
134336 ; loss 0.47 accuracy:80.96 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(113972) 113972
140736 ; loss 0.5 accuracy:80.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119159) 119159
147136 ; loss 0.49 accuracy:80.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124334) 124334
153536 ; loss 0.48 accuracy:80.95 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(129585) 129585
159936 ; loss 0.47 accuracy:80.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(134785) 134785
166336 ; loss 0.48 accuracy:81.0 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(139935) 139935
172736 ; loss 0.5 accuracy:80.98 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145179) 145179
179136 ; loss 0.47 accuracy:81.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150361) 150361
185536 ; loss 0.49 accuracy:81.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155567) 155567
191936 ; loss 0.49 accuracy:81.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(160755) 160755
198336 ; loss 0.48 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(165948) 165948
204736 ; loss 0.48 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(171130) 171130
211136 ; loss 0.49 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(176320) 176320
217536 ; loss 0.49 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(181503) 181503
223936 ; loss 0.48 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(186704) 186704
230336 ; loss 0.48 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(191900) 191900
236736 ; loss 0.48 accuracy:81.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197025) 197025
243136 ; loss 0.5 accuracy:81.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(202211) 202211
249536 ; loss 0.48 accuracy:81.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(207418) 207418
255936 ; loss 0.48 accuracy:81.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(212610) 212610
262336 ; loss 0.48 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(217788) 217788
268736 ; loss 0.48 accuracy:81.02 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223000) 223000
275136 ; loss 0.47 accuracy:81.03 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(228249) 228249
281536 ; loss 0.47 accuracy:81.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(233477) 233477
287936 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(238664) 238664
294336 ; loss 0.49 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(243848) 243848
300736 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249023) 249023
307136 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(254198) 254198
313536 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(259381) 259381
319936 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(264585) 264585
326336 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(269748) 269748
332736 ; loss 0.5 accuracy:81.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(274941) 274941
339136 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(280126) 280126
345536 ; loss 0.49 accuracy:81.05 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(285271) 285271
351936 ; loss 0.5 accuracy:81.04 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(290503) 290503
358336 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(295709) 295709
364736 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(300932) 300932
371136 ; loss 0.47 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(306153) 306153
377536 ; loss 0.48 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(311334) 311334
383936 ; loss 0.48 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(316483) 316483
390336 ; loss 0.5 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(321668) 321668
396736 ; loss 0.49 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(326873) 326873
403136 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(332067) 332067
409536 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(337215) 337215
415936 ; loss 0.5 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(342407) 342407
422336 ; loss 0.48 accuracy:81.06 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(347627) 347627
428736 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(352836) 352836
435136 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(358038) 358038
441536 ; loss 0.48 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(363248) 363248
447936 ; loss 0.49 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(368445) 368445
454336 ; loss 0.48 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(373622) 373622
460736 ; loss 0.49 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(378814) 378814
467136 ; loss 0.48 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(383969) 383969
473536 ; loss 0.49 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(389150) 389150
479936 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(394344) 394344
486336 ; loss 0.48 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(399519) 399519
492736 ; loss 0.49 accuracy:81.07 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(404776) 404776
499136 ; loss 0.47 accuracy:81.08 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(409969) 409969
505536 ; loss 0.48 accuracy:81.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(415188) 415188
511936 ; loss 0.48 accuracy:81.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(420417) 420417
518336 ; loss 0.48 accuracy:81.1 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(425566) 425566
524736 ; loss 0.5 accuracy:81.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(430750) 430750
531136 ; loss 0.49 accuracy:81.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(435922) 435922
537536 ; loss 0.48 accuracy:81.09 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(441112) 441112
543936 ; loss 0.48 accuracy:81.09 ;
results : epoch 18 ; mean accuracy train : 81.08

VALIDATION : Epoch 18
togrep : results : epoch 18 ; mean accuracy valid :              81.02
saving model at epoch 18

TRAINING : Epoch 19
Learning rate : 0.01669027522900175
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5219) 5219
6336 ; loss 0.48 accuracy:81.55 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10455) 10455
12736 ; loss 0.46 accuracy:81.68 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15659) 15659
19136 ; loss 0.48 accuracy:81.56 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20822) 20822
25536 ; loss 0.49 accuracy:81.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(26019) 26019
31936 ; loss 0.48 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31256) 31256
38336 ; loss 0.48 accuracy:81.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(36493) 36493
44736 ; loss 0.48 accuracy:81.46 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41669) 41669
51136 ; loss 0.48 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46829) 46829
57536 ; loss 0.49 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(52067) 52067
63936 ; loss 0.48 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57213) 57213
70336 ; loss 0.49 accuracy:81.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(62431) 62431
76736 ; loss 0.47 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67632) 67632
83136 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(72800) 72800
89536 ; loss 0.49 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(77952) 77952
95936 ; loss 0.49 accuracy:81.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83107) 83107
102336 ; loss 0.48 accuracy:81.16 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(88346) 88346
108736 ; loss 0.48 accuracy:81.2 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93553) 93553
115136 ; loss 0.47 accuracy:81.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98765) 98765
121536 ; loss 0.49 accuracy:81.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(103948) 103948
127936 ; loss 0.49 accuracy:81.21 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109179) 109179
134336 ; loss 0.47 accuracy:81.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114390) 114390
140736 ; loss 0.48 accuracy:81.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119563) 119563
147136 ; loss 0.48 accuracy:81.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124759) 124759
153536 ; loss 0.48 accuracy:81.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(129976) 129976
159936 ; loss 0.48 accuracy:81.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135166) 135166
166336 ; loss 0.48 accuracy:81.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140369) 140369
172736 ; loss 0.49 accuracy:81.23 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145587) 145587
179136 ; loss 0.48 accuracy:81.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(150748) 150748
185536 ; loss 0.49 accuracy:81.22 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(155976) 155976
191936 ; loss 0.47 accuracy:81.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161220) 161220
198336 ; loss 0.48 accuracy:81.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166475) 166475
204736 ; loss 0.46 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(171709) 171709
211136 ; loss 0.48 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(176930) 176930
217536 ; loss 0.49 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182137) 182137
223936 ; loss 0.48 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187309) 187309
230336 ; loss 0.49 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192513) 192513
236736 ; loss 0.48 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197696) 197696
243136 ; loss 0.49 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(202923) 202923
249536 ; loss 0.47 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(208129) 208129
255936 ; loss 0.48 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(213313) 213313
262336 ; loss 0.49 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(218515) 218515
268736 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223718) 223718
275136 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(228942) 228942
281536 ; loss 0.47 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234166) 234166
287936 ; loss 0.48 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239388) 239388
294336 ; loss 0.47 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244626) 244626
300736 ; loss 0.47 accuracy:81.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249827) 249827
307136 ; loss 0.49 accuracy:81.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(255040) 255040
313536 ; loss 0.48 accuracy:81.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(260186) 260186
319936 ; loss 0.49 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(265313) 265313
326336 ; loss 0.5 accuracy:81.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270553) 270553
332736 ; loss 0.46 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(275768) 275768
339136 ; loss 0.48 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(280975) 280975
345536 ; loss 0.48 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(286186) 286186
351936 ; loss 0.47 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(291407) 291407
358336 ; loss 0.47 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(296566) 296566
364736 ; loss 0.49 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(301729) 301729
371136 ; loss 0.48 accuracy:81.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(306945) 306945
377536 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(312189) 312189
383936 ; loss 0.47 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(317375) 317375
390336 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(322551) 322551
396736 ; loss 0.5 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(327756) 327756
403136 ; loss 0.49 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(332977) 332977
409536 ; loss 0.49 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(338156) 338156
415936 ; loss 0.49 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(343360) 343360
422336 ; loss 0.47 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(348560) 348560
428736 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(353780) 353780
435136 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(358977) 358977
441536 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(364192) 364192
447936 ; loss 0.47 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(369383) 369383
454336 ; loss 0.48 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(374560) 374560
460736 ; loss 0.48 accuracy:81.28 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(379704) 379704
467136 ; loss 0.49 accuracy:81.27 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(384865) 384865
473536 ; loss 0.48 accuracy:81.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(390050) 390050
479936 ; loss 0.49 accuracy:81.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(395221) 395221
486336 ; loss 0.49 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(400386) 400386
492736 ; loss 0.5 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(405634) 405634
499136 ; loss 0.47 accuracy:81.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(410817) 410817
505536 ; loss 0.48 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(416013) 416013
511936 ; loss 0.48 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(421197) 421197
518336 ; loss 0.48 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(426436) 426436
524736 ; loss 0.47 accuracy:81.26 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(431615) 431615
531136 ; loss 0.48 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(436760) 436760
537536 ; loss 0.49 accuracy:81.24 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(441961) 441961
543936 ; loss 0.48 accuracy:81.24 ;
results : epoch 19 ; mean accuracy train : 81.24

VALIDATION : Epoch 19
togrep : results : epoch 19 ; mean accuracy valid :              81.26
saving model at epoch 19

TRAINING : Epoch 20
Learning rate : 0.01652337247671173
&lt;class &#39;torch.Tensor&#39;&gt; tensor(5141) 5141
6336 ; loss 0.49 accuracy:80.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(10382) 10382
12736 ; loss 0.47 accuracy:81.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(15554) 15554
19136 ; loss 0.49 accuracy:81.01 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(20718) 20718
25536 ; loss 0.48 accuracy:80.93 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(25916) 25916
31936 ; loss 0.48 accuracy:80.99 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(31147) 31147
38336 ; loss 0.48 accuracy:81.11 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(36398) 36398
44736 ; loss 0.47 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(41656) 41656
51136 ; loss 0.46 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(46800) 46800
57536 ; loss 0.48 accuracy:81.25 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(52032) 52032
63936 ; loss 0.48 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(57250) 57250
70336 ; loss 0.49 accuracy:81.32 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(62466) 62466
76736 ; loss 0.48 accuracy:81.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(67650) 67650
83136 ; loss 0.48 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(72877) 72877
89536 ; loss 0.48 accuracy:81.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(78041) 78041
95936 ; loss 0.49 accuracy:81.29 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(83258) 83258
102336 ; loss 0.47 accuracy:81.31 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(88454) 88454
108736 ; loss 0.49 accuracy:81.3 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(93700) 93700
115136 ; loss 0.47 accuracy:81.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(98930) 98930
121536 ; loss 0.47 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(104149) 104149
127936 ; loss 0.48 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(109317) 109317
134336 ; loss 0.49 accuracy:81.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(114542) 114542
140736 ; loss 0.47 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(119755) 119755
147136 ; loss 0.48 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(124946) 124946
153536 ; loss 0.48 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(130148) 130148
159936 ; loss 0.48 accuracy:81.34 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(135371) 135371
166336 ; loss 0.47 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(140615) 140615
172736 ; loss 0.47 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(145881) 145881
179136 ; loss 0.47 accuracy:81.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(151133) 151133
185536 ; loss 0.47 accuracy:81.43 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(156269) 156269
191936 ; loss 0.49 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(161468) 161468
198336 ; loss 0.48 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(166683) 166683
204736 ; loss 0.47 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(171859) 171859
211136 ; loss 0.48 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(177077) 177077
217536 ; loss 0.48 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(182294) 182294
223936 ; loss 0.48 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(187522) 187522
230336 ; loss 0.48 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(192786) 192786
236736 ; loss 0.47 accuracy:81.41 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(197915) 197915
243136 ; loss 0.49 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(203143) 203143
249536 ; loss 0.48 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(208363) 208363
255936 ; loss 0.48 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(213538) 213538
262336 ; loss 0.49 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(218717) 218717
268736 ; loss 0.49 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(223922) 223922
275136 ; loss 0.48 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(229154) 229154
281536 ; loss 0.47 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(234390) 234390
287936 ; loss 0.47 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(239580) 239580
294336 ; loss 0.48 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(244779) 244779
300736 ; loss 0.48 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(249988) 249988
307136 ; loss 0.47 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(255235) 255235
313536 ; loss 0.46 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(260439) 260439
319936 ; loss 0.48 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(265667) 265667
326336 ; loss 0.47 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(270910) 270910
332736 ; loss 0.48 accuracy:81.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(276100) 276100
339136 ; loss 0.48 accuracy:81.4 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(281289) 281289
345536 ; loss 0.49 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(286484) 286484
351936 ; loss 0.48 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(291695) 291695
358336 ; loss 0.47 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(296923) 296923
364736 ; loss 0.48 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(302112) 302112
371136 ; loss 0.49 accuracy:81.39 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(307291) 307291
377536 ; loss 0.5 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(312486) 312486
383936 ; loss 0.48 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(317689) 317689
390336 ; loss 0.47 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(322930) 322930
396736 ; loss 0.47 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(328116) 328116
403136 ; loss 0.48 accuracy:81.38 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(333294) 333294
409536 ; loss 0.49 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(338464) 338464
415936 ; loss 0.49 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(343674) 343674
422336 ; loss 0.47 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(348859) 348859
428736 ; loss 0.48 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(354072) 354072
435136 ; loss 0.48 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(359267) 359267
441536 ; loss 0.47 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(364499) 364499
447936 ; loss 0.48 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(369739) 369739
454336 ; loss 0.46 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(374954) 374954
460736 ; loss 0.48 accuracy:81.37 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(380133) 380133
467136 ; loss 0.47 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(385322) 385322
473536 ; loss 0.49 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(390548) 390548
479936 ; loss 0.48 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(395756) 395756
486336 ; loss 0.48 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(400957) 400957
492736 ; loss 0.48 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(406137) 406137
499136 ; loss 0.49 accuracy:81.36 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(411317) 411317
505536 ; loss 0.48 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(416536) 416536
511936 ; loss 0.47 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(421740) 421740
518336 ; loss 0.47 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(426925) 426925
524736 ; loss 0.48 accuracy:81.35 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(432050) 432050
531136 ; loss 0.5 accuracy:81.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(437242) 437242
537536 ; loss 0.48 accuracy:81.33 ;
&lt;class &#39;torch.Tensor&#39;&gt; tensor(442443) 442443
543936 ; loss 0.48 accuracy:81.33 ;
results : epoch 20 ; mean accuracy train : 81.33

VALIDATION : Epoch 20
togrep : results : epoch 20 ; mean accuracy valid :              81.1
Shrinking lr by : 5. New lr = 0.003304674495342346

TEST : Epoch 21

VALIDATION : Epoch 1000000.0
finalgrep : accuracy valid : 81.1
finalgrep : accuracy test : 81.15
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
