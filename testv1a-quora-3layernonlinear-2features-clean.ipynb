{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_dir='/home/dc/cs230_project/dataset')\n",
      "\n",
      "togrep : ['-f', '/run/user/1000/jupyter/kernel-c6ea98b6-4492-4d7a-9c9d-f67f0fc5ca58.json']\n",
      "\n",
      "Namespace(LSTM_num_layers=2, batch_size=128, data_dir='/home/dc/cs230_project/dataset', decay=0.99, dpout_fc=0.2, dpout_model=0.2, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=256, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=2, n_enc_layers=1, n_epochs=25, nonlinear_fc=1, optimizer='adam', outputdir='savedir/', outputmodelname='3layernonlinear_2fearurescleanadam.pickle', pool_type='max', seed=4, weight_decay=0.0005, word_emb_dim=300)\n",
      "loading clean\n",
      "quora checkpoint len(train[s1]):219666,len(train[s2]):219666,          len(train[label]):219666\n",
      "============\n",
      "len(valid['s1']):73223, len(valid[s2]):73223,           len(valid['label']):73223\n",
      "============\n",
      "len(test['s1']):73222,len(test['s2']):73222,           len(test['label']):73222\n",
      "Found 79019(/198506) words with glove vectors\n",
      "Vocab size : 79019\n",
      "checkpoint after formatting: len(train[s1]):219666 ,len(train[s2]):219666       ,len(train[label]):219666, len(valid[s2]):73223 ,len(valid[s2]):73223,       len(valid[label]):73223,len(test[s2]):73222, len(test[s2]):73222       ,len(valid[label]):73223,len(word_vec):79019\n",
      "NLINet(\n",
      "  (encoder): InferSent(\n",
      "    (enc_lstm): LSTM(300, 2048, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2)\n",
      "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2)\n",
      "    (12): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:451: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num epochs:25\n",
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.001\n",
      "<class 'torch.Tensor'> tensor(8015) 8015\n",
      "12672 ; loss 0.65 ; sentence/s 208 ; words/s 11728 ; accuracy train : 62.62\n",
      "<class 'torch.Tensor'> tensor(16444) 16444\n",
      "25472 ; loss 0.62 ; sentence/s 207 ; words/s 11425 ; accuracy train : 64.23\n",
      "<class 'torch.Tensor'> tensor(25141) 25141\n",
      "38272 ; loss 0.6 ; sentence/s 206 ; words/s 11476 ; accuracy train : 65.47\n",
      "<class 'torch.Tensor'> tensor(33878) 33878\n",
      "51072 ; loss 0.6 ; sentence/s 206 ; words/s 11586 ; accuracy train : 66.17\n",
      "<class 'torch.Tensor'> tensor(42638) 42638\n",
      "63872 ; loss 0.59 ; sentence/s 206 ; words/s 11291 ; accuracy train : 66.62\n",
      "<class 'torch.Tensor'> tensor(51407) 51407\n",
      "76672 ; loss 0.58 ; sentence/s 206 ; words/s 11404 ; accuracy train : 66.94\n",
      "<class 'torch.Tensor'> tensor(60215) 60215\n",
      "89472 ; loss 0.58 ; sentence/s 207 ; words/s 11598 ; accuracy train : 67.2\n",
      "<class 'torch.Tensor'> tensor(69071) 69071\n",
      "102272 ; loss 0.57 ; sentence/s 203 ; words/s 11618 ; accuracy train : 67.45\n",
      "<class 'torch.Tensor'> tensor(78030) 78030\n",
      "115072 ; loss 0.57 ; sentence/s 204 ; words/s 11610 ; accuracy train : 67.73\n",
      "<class 'torch.Tensor'> tensor(87082) 87082\n",
      "127872 ; loss 0.56 ; sentence/s 206 ; words/s 11469 ; accuracy train : 68.03\n",
      "<class 'torch.Tensor'> tensor(96133) 96133\n",
      "140672 ; loss 0.56 ; sentence/s 205 ; words/s 11556 ; accuracy train : 68.28\n",
      "<class 'torch.Tensor'> tensor(105228) 105228\n",
      "153472 ; loss 0.56 ; sentence/s 206 ; words/s 11448 ; accuracy train : 68.51\n",
      "<class 'torch.Tensor'> tensor(114359) 114359\n",
      "166272 ; loss 0.55 ; sentence/s 204 ; words/s 11734 ; accuracy train : 68.73\n",
      "<class 'torch.Tensor'> tensor(123566) 123566\n",
      "179072 ; loss 0.54 ; sentence/s 206 ; words/s 11493 ; accuracy train : 68.95\n",
      "<class 'torch.Tensor'> tensor(132843) 132843\n",
      "191872 ; loss 0.54 ; sentence/s 207 ; words/s 11349 ; accuracy train : 69.19\n",
      "<class 'torch.Tensor'> tensor(142006) 142006\n",
      "204672 ; loss 0.54 ; sentence/s 207 ; words/s 11523 ; accuracy train : 69.34\n",
      "<class 'torch.Tensor'> tensor(151414) 151414\n",
      "217472 ; loss 0.53 ; sentence/s 204 ; words/s 11535 ; accuracy train : 69.58\n",
      "results : epoch 1 ; mean accuracy train : 69.6\n",
      "\n",
      "VALIDATION : Epoch 1\n",
      "list saved to file!\n",
      "list saved to file!\n",
      "togrep : results : epoch 1 ; mean accuracy valid :              70.89\n",
      "saving model at epoch 1\n",
      "\n",
      "TRAINING : Epoch 2\n",
      "Learning rate : 0.001\n",
      "<class 'torch.Tensor'> tensor(9205) 9205\n",
      "12672 ; loss 0.54 ; sentence/s 205 ; words/s 11616 ; accuracy train : 71.91\n",
      "<class 'torch.Tensor'> tensor(18588) 18588\n",
      "25472 ; loss 0.52 ; sentence/s 203 ; words/s 11875 ; accuracy train : 72.61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-431fcb7fee2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m     \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-431fcb7fee2a>\u001b[0m in \u001b[0;36mtrainepoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# prepare batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n\u001b[0;32m--> 536\u001b[0;31m                                      word_vec)\n\u001b[0m\u001b[1;32m    537\u001b[0m         s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n\u001b[1;32m    538\u001b[0m                                      word_vec)\n",
      "\u001b[0;32m<ipython-input-1-431fcb7fee2a>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(batch, word_vec)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0membed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import inspect\n",
    "import time \n",
    "import torch\n",
    "from torchqrnn import QRNN\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from MakeData import MakeData\n",
    "\n",
    "#from data import get_nli, get_batch, build_vocab\n",
    "#from mutils import get_optimizer\n",
    "#from models import NLINet\n",
    "\n",
    "start_time = time.time()\n",
    "W2V_PATH = \"/home/dc/cs230_project/dataset/GloVe/glove.840B.300d.txt\"\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training')\n",
    "# paths\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='3layernonlinear_2fearurescleanadam.pickle')\n",
    "parser.add_argument(\"--modeldir\", type=str, default='rocandmodel/', help=\"roc and model directory\")\n",
    "parser.add_argument(\"--rocdir\", type=str, default='rocandmodel/', help=\"roc and model directory\")\n",
    "\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=25)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "#this only works if num_layers>1\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=.2, help=\"encoder dropout\")\n",
    "#this is only for the dropout after batchnorm in nonlinear\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0.2, help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=1, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"adam\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4, help=\"weight decay for sgd\")\n",
    "\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSent', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=256, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=2, help=\"duplicate/not duplicate\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "parser.add_argument(\"--word_emb_dim\", type=int, default='300', help=\"embedding dim\")\n",
    "parser.add_argument(\"--LSTM_num_layers\", type=int, default='2', help=\"LSTM num layers\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default='/home/dc/cs230_project/dataset', help=\"store duplicate questions\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=4, help=\"seed\")\n",
    "\n",
    "\n",
    "params, _ = parser.parse_known_args()\n",
    "\n",
    "# set gpu device\n",
    "torch.cuda.set_device(params.gpu_id)\n",
    "\n",
    "# print parameters passed, and all parameters\n",
    "print('\\ntogrep : {0}\\n'.format(sys.argv[1:]))\n",
    "print(params)\n",
    "\n",
    "\"\"\"\n",
    "SEED\n",
    "\"\"\"\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "\n",
    "make_data = MakeData()\n",
    "train, valid, test,word_vec = make_data.quora(big=False,small=False,clean=True)\n",
    "\n",
    "print(f\"checkpoint after formatting: len(train[s1]):{len(train['s1'])} ,len(train[s2]):{len(train['s2'])} \\\n",
    "      ,len(train[label]):{len(train['label'])}, len(valid[s2]):{len(valid['s1'])} ,len(valid[s2]):{len(valid['s2'])}, \\\n",
    "      len(valid[label]):{len(valid['label'])},len(test[s2]):{len(test['s1'])}, len(test[s2]):{len(test['s2'])} \\\n",
    "      ,len(valid[label]):{len(valid['label'])},len(word_vec):{len(word_vec)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MODEL\n",
    "\"\"\"\n",
    "# model config\n",
    "config_nli_model = {\n",
    "    'n_words'        :  300          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, params.LSTM_num_layers,\n",
    "                                bidirectional=True, dropout=self.dpout_model)\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # either all weights are on cpu or they are on gpu\n",
    "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len: [max_len, ..., min_len] (bsize)\n",
    "        # sent: Variable(seqlen x bsize x worddim)\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # Sort by length (keep idx) https://github.com/pytorch/pytorch/issues/3584\n",
    "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() else torch.from_numpy(idx_sort)\n",
    "        sent = sent.index_select(1, Variable(idx_sort))\n",
    "\n",
    "        # Padding perf increase\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
    "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
    "\n",
    "        # Un-sort by length\n",
    "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_unsort)\n",
    "        sent_output = sent_output.index_select(1, Variable(idx_unsort))\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            sent_len = Variable(torch.FloatTensor(sent_len.copy())).unsqueeze(1).cuda()\n",
    "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
    "            emb = emb / sent_len.expand_as(emb)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb = torch.max(sent_output, 0)[0]\n",
    "            if emb.ndimension() == 3:\n",
    "                emb = emb.squeeze(0)\n",
    "                assert emb.ndimension() == 2\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with w2v vectors\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with k first w2v vectors\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path) as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if k <= K:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "                    k += 1\n",
    "                if k > K:\n",
    "                    if word in [self.bos, self.eos]:\n",
    "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    # build w2v vocab with k most frequent words\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (K))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # keep only new words\n",
    "        for word in self.word_vec:\n",
    "            if word in word_dict:\n",
    "                del word_dict[word]\n",
    "\n",
    "        # udpate vocabulary\n",
    "        if word_dict:\n",
    "            new_word_vec = self.get_w2v(word_dict)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "        else:\n",
    "            new_word_vec = []\n",
    "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # sent in batch in decreasing order of lengths\n",
    "        # batch: (bsize, max_len, word_dim)\n",
    "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "        return torch.FloatTensor(embed)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        if self.moses_tok:\n",
    "            s = ' '.join(word_tokenize(s))\n",
    "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
    "            return s.split()\n",
    "        else:\n",
    "            return word_tokenize(s)\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
    "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # filters words without w2v vectors\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
    "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
    "                s_f = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
    "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
    "\n",
    "        # sort by decreasing length\n",
    "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "        sentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "        return sentences, lengths, idx_sort\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "        sentences, lengths, idx_sort = self.prepare_samples(\n",
    "                        sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences), bsize):\n",
    "            batch = Variable(self.get_batch(\n",
    "                        sentences[stidx:stidx + bsize]), volatile=True)\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            batch = self.forward(\n",
    "                (batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "            embeddings.append(batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # unsort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
    "\n",
    "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
    "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        batch = Variable(self.get_batch(sent), volatile=True)\n",
    "\n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.enc_lstm(batch)[0]\n",
    "        output, idxs = torch.max(output, 0)\n",
    "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
    "        idxs = idxs.data.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "        # visualize model\n",
    "        import matplotlib.pyplot as plt\n",
    "        x = range(len(sent[0]))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent[0], rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs\n",
    "\n",
    "\n",
    "class NLINet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NLINet, self).__init__()\n",
    "\n",
    "        # classifier\n",
    "        self.nonlinear_fc = config['nonlinear_fc']\n",
    "        self.fc_dim = config['fc_dim']\n",
    "        self.n_classes = config['n_classes']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.encoder_type = config['encoder_type']\n",
    "        self.dpout_fc = config['dpout_fc']\n",
    "\n",
    "        self.encoder = eval(self.encoder_type)(config)\n",
    "        self.inputdim = 2*2*self.enc_lstm_dim\n",
    "        self.inputdim = 4*self.inputdim if self.encoder_type in \\\n",
    "                        [\"ConvNetEncoder\", \"InnerAttentionMILAEncoder\"] else self.inputdim\n",
    "        self.inputdim = ((int)(self.inputdim/2)) if self.encoder_type == \"LSTMEncoder\" \\\n",
    "                                        else self.inputdim\n",
    "        if self.nonlinear_fc:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.BatchNorm1d(self.fc_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dpout_fc),\n",
    "                \n",
    "                nn.Linear(self.fc_dim, self.n_classes),\n",
    "                \n",
    "                )\n",
    "        else:\n",
    "            print(f\"self.inputdim:{self.inputdim}, self.fc_dim:{self.fc_dim}\")\n",
    "            print(type(self.inputdim),type(self.fc_dim))\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(self.inputdim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.fc_dim),\n",
    "                nn.Linear(self.fc_dim, self.n_classes)\n",
    "                )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        # s1 : (s1, s1_len)\n",
    "        u = self.encoder(s1)\n",
    "        v = self.encoder(s2)\n",
    "\n",
    "        features = torch.cat((u, v), 1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def encode(self, s1):\n",
    "        emb = self.encoder(s1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "def get_optimizer(s):\n",
    "    \"\"\"\n",
    "    Parse optimizer parameters.\n",
    "    Input should be of the form:\n",
    "        - \"sgd,lr=0.01\"\n",
    "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
    "    \"\"\"\n",
    "    if \",\" in s:\n",
    "        method = s[:s.find(',')]\n",
    "        optim_params = {}\n",
    "        for x in s[s.find(',') + 1:].split(','):\n",
    "            split = x.split('=')\n",
    "            assert len(split) == 2\n",
    "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
    "            optim_params[split[0]] = float(split[1])\n",
    "    else:\n",
    "        method = s\n",
    "        optim_params = {}\n",
    "\n",
    "    if method == 'adadelta':\n",
    "        optim_fn = optim.Adadelta\n",
    "    elif method == 'adagrad':\n",
    "        optim_fn = optim.Adagrad\n",
    "    elif method == 'adam':\n",
    "        optim_fn = optim.Adam\n",
    "    elif method == 'adamax':\n",
    "        optim_fn = optim.Adamax\n",
    "    elif method == 'asgd':\n",
    "        optim_fn = optim.ASGD\n",
    "    elif method == 'rmsprop':\n",
    "        optim_fn = optim.RMSprop\n",
    "    elif method == 'rprop':\n",
    "        optim_fn = optim.Rprop\n",
    "    elif method == 'sgd':\n",
    "        optim_fn = optim.SGD\n",
    "        assert 'lr' in optim_params\n",
    "    else:\n",
    "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
    "\n",
    "    # check that we give good parameters to the optimizer\n",
    "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
    "    assert expected_args[:2] == ['self', 'params']\n",
    "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
    "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
    "            str(expected_args[2:]), str(optim_params.keys())))\n",
    "\n",
    "    return optim_fn, optim_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "encoder_types = ['InferSent', 'BLSTMprojEncoder', 'BGRUlastEncoder',\n",
    "                 'InnerAttentionMILAEncoder', 'InnerAttentionYANGEncoder',\n",
    "                 'InnerAttentionNAACLEncoder', 'ConvNetEncoder', 'LSTMEncoder']\n",
    "assert params.encoder_type in encoder_types, \"encoder_type must be in \" + \\\n",
    "                                             str(encoder_types)\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)\n",
    "\n",
    "\n",
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "#BCE next w2 categories\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = 10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None\n",
    "\n",
    "\n",
    "def get_batch(batch, word_vec):\n",
    "    # sent in batch in decreasing order of lengths (bsize, max_len, word_dim)\n",
    "    lengths = np.array([len(x) for x in batch])\n",
    "    max_len = np.max(lengths)\n",
    "    embed = np.zeros((max_len, len(batch), 300))\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        for j in range(len(batch[i])):\n",
    "            embed[j, i, :] = word_vec[batch[i][j]]\n",
    "\n",
    "    return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "\n",
    "\n",
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['s1']))\n",
    "    #print(f\"type(permutation):{type(permutation)}\")\n",
    "    #print(f\"type(train['s1']):{type(train['s1'])}\")\n",
    "    \n",
    "    s1 = train['s1'][permutation]\n",
    "    s2 = train['s2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "    \n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "                                     word_vec)\n",
    "        #print(type(s1_batch),type(s2_batch)) #should be list\n",
    "        #print(f\"s1_len:{s1_len},s2_len:{s2_len}\")\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        target_batch=target[stidx:stidx + params.batch_size]\n",
    "        #print(f\"target_batch.shape:{target_batch.shape}\")\n",
    "        #print(f\"target_batch:{target_batch}\")\n",
    "        #print(f\"target shape:{target.shape}\")\n",
    "        #print(f\"target:{target[stidx:stidx + params.batch_size]}\")\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "        #print(f\"tgt_batch:{tgt_batch}\")\n",
    "        #print(f\"k:{k}\")\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        #print(f\"type(tgt_batch):{type(tgt_batch)}\")\n",
    "        #print(f\"type(output):{type(output)}\")\n",
    "        #print(f\"output size:{output.size()}\")\n",
    "        \n",
    "        #print(f\"output:{output}\")\n",
    "        #\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        all_costs.append(loss.item())\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "        \n",
    "        if len(all_costs) == 100:\n",
    "            print(type(correct),correct,correct.item())\n",
    "            #logs.append('{0} ; loss {1} accuracy:{2} ;'.format(stidx,round(np.mean(all_costs), 2),round(100.*correct.item()/(stidx+k), 2)))\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)), \n",
    "                            round(100.*correct.item()/(stidx+k), 2)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = round(100 * correct.item()/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "\n",
    "def save_list(my_list,filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(my_list,f)\n",
    "        f.close()\n",
    "    print(\"list saved to file!\")\n",
    "\n",
    "def read_list(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        my_list = pickle.load(f)\n",
    "    return my_list\n",
    "\n",
    "def evaluate(epoch, eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['s1'] if eval_type == 'valid' else test['s1']\n",
    "    s2 = valid['s2'] if eval_type == 'valid' else test['s2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "    \n",
    "    predictions=[]\n",
    "    targets=[]\n",
    "    \n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        predictions.append(pred.cpu().data.numpy().tolist())\n",
    "        targets.append(tgt_batch.cpu().data.numpy().tolist())\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        \n",
    "    # save model, note from above targets is set to valid and test labels before training\n",
    "    if eval_type == 'valid':\n",
    "        save_list(predictions,\"valid_\"+params.outputmodelname+\"_predict.pkl\")\n",
    "        save_list(targets,\"valid_\"+params.outputmodelname+\"_targets.pkl\")\n",
    "    else:\n",
    "        save_list(predictions,\"test_\"+params.outputmodelname+\"_predict.pkl\")\n",
    "        save_list(targets,\"test_\"+params.outputmodelname+\"_targets.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    eval_acc = round(100 * correct.item() / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train model on Natural Language Inference task\n",
    "\"\"\"\n",
    "epoch = 1\n",
    "print(f\"total num epochs:{params.n_epochs}\")\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1\n",
    "    \n",
    "#nli_net.save_state_dict(os.path.join(params.outputdir, params.outputmodelname))\n",
    "# Run best model on test set.\n",
    "#nli_net.load_state_dict(os.path.join(params.outputdir, params.outputmodelname))\n",
    "print(\"saving state dict\")\n",
    "torch.save(nli_net.state_dict,os.path.join(params.outputdir, params.outputmodelname + \"_statedict.pt\"))\n",
    "print(\"done saving state dict\")\n",
    "\n",
    "\n",
    "print('\\nTEST : Epoch {0}'.format(epoch))\n",
    "print('calculating validation error')\n",
    "evaluate(1e6, 'valid', True)\n",
    "print('calculating test error')\n",
    "evaluate(0, 'test', True)\n",
    "\n",
    "# Save full model\n",
    "torch.save(nli_net, os.path.join(params.outputdir, params.outputmodelname + '_fullmodel.pt'))\n",
    "#save encoder, use this to run another model after!!!\n",
    "torch.save(nli_net.encoder.state_dict(), os.path.join(params.outputdir, params.outputmodelname + '.encoder.pt'))\n",
    "\n",
    "#save entire model...\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"fin\",elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVED TO V100 wont run on titanx, up to 11.9GB\n",
    "Namespace(data_dir='/home/dc/cs230_project/dataset')\n",
    "\n",
    "togrep : ['-f', '/run/user/1000/jupyter/kernel-c6ea98b6-4492-4d7a-9c9d-f67f0fc5ca58.json']\n",
    "\n",
    "Namespace(LSTM_num_layers=2, batch_size=128, data_dir='/home/dc/cs230_project/dataset', decay=0.99, dpout_fc=0.2, dpout_model=0.2, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=256, gpu_id=0, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=2, n_enc_layers=1, n_epochs=25, nonlinear_fc=1, optimizer='adam', outputdir='savedir/', outputmodelname='3layernonlinear_2fearurescleanadam.pickle', pool_type='max', seed=4, weight_decay=0.0005, word_emb_dim=300)\n",
    "loading clean\n",
    "quora checkpoint len(train[s1]):219666,len(train[s2]):219666,          len(train[label]):219666\n",
    "============\n",
    "len(valid['s1']):73223, len(valid[s2]):73223,           len(valid['label']):73223\n",
    "============\n",
    "len(test['s1']):73222,len(test['s2']):73222,           len(test['label']):73222\n",
    "Found 79019(/198506) words with glove vectors\n",
    "Vocab size : 79019\n",
    "checkpoint after formatting: len(train[s1]):219666 ,len(train[s2]):219666       ,len(train[label]):219666, len(valid[s2]):73223 ,len(valid[s2]):73223,       len(valid[label]):73223,len(test[s2]):73222, len(test[s2]):73222       ,len(valid[label]):73223,len(word_vec):79019\n",
    "NLINet(\n",
    "  (encoder): InferSent(\n",
    "    (enc_lstm): LSTM(300, 2048, num_layers=2, dropout=0.2, bidirectional=True)\n",
    "  )\n",
    "  (classifier): Sequential(\n",
    "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
    "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (2): ReLU()\n",
    "    (3): Dropout(p=0.2)\n",
    "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
    "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (6): ReLU()\n",
    "    (7): Dropout(p=0.2)\n",
    "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
    "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (10): ReLU()\n",
    "    (11): Dropout(p=0.2)\n",
    "    (12): Linear(in_features=256, out_features=2, bias=True)\n",
    "  )\n",
    ")\n",
    "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/ipykernel_launcher.py:451: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
    "total num epochs:25\n",
    "\n",
    "TRAINING : Epoch 1\n",
    "Learning rate : 0.001\n",
    "<class 'torch.Tensor'> tensor(8015) 8015\n",
    "12672 ; loss 0.65 ; sentence/s 208 ; words/s 11728 ; accuracy train : 62.62\n",
    "<class 'torch.Tensor'> tensor(16444) 16444\n",
    "25472 ; loss 0.62 ; sentence/s 207 ; words/s 11425 ; accuracy train : 64.23\n",
    "<class 'torch.Tensor'> tensor(25141) 25141\n",
    "38272 ; loss 0.6 ; sentence/s 206 ; words/s 11476 ; accuracy train : 65.47\n",
    "<class 'torch.Tensor'> tensor(33878) 33878\n",
    "51072 ; loss 0.6 ; sentence/s 206 ; words/s 11586 ; accuracy train : 66.17\n",
    "<class 'torch.Tensor'> tensor(42638) 42638\n",
    "63872 ; loss 0.59 ; sentence/s 206 ; words/s 11291 ; accuracy train : 66.62\n",
    "<class 'torch.Tensor'> tensor(51407) 51407\n",
    "76672 ; loss 0.58 ; sentence/s 206 ; words/s 11404 ; accuracy train : 66.94\n",
    "<class 'torch.Tensor'> tensor(60215) 60215\n",
    "89472 ; loss 0.58 ; sentence/s 207 ; words/s 11598 ; accuracy train : 67.2\n",
    "<class 'torch.Tensor'> tensor(69071) 69071\n",
    "102272 ; loss 0.57 ; sentence/s 203 ; words/s 11618 ; accuracy train : 67.45\n",
    "<class 'torch.Tensor'> tensor(78030) 78030\n",
    "115072 ; loss 0.57 ; sentence/s 204 ; words/s 11610 ; accuracy train : 67.73\n",
    "<class 'torch.Tensor'> tensor(87082) 87082\n",
    "127872 ; loss 0.56 ; sentence/s 206 ; words/s 11469 ; accuracy train : 68.03\n",
    "<class 'torch.Tensor'> tensor(96133) 96133\n",
    "140672 ; loss 0.56 ; sentence/s 205 ; words/s 11556 ; accuracy train : 68.28\n",
    "<class 'torch.Tensor'> tensor(105228) 105228\n",
    "153472 ; loss 0.56 ; sentence/s 206 ; words/s 11448 ; accuracy train : 68.51\n",
    "<class 'torch.Tensor'> tensor(114359) 114359\n",
    "166272 ; loss 0.55 ; sentence/s 204 ; words/s 11734 ; accuracy train : 68.73\n",
    "<class 'torch.Tensor'> tensor(123566) 123566\n",
    "179072 ; loss 0.54 ; sentence/s 206 ; words/s 11493 ; accuracy train : 68.95\n",
    "<class 'torch.Tensor'> tensor(132843) 132843\n",
    "191872 ; loss 0.54 ; sentence/s 207 ; words/s 11349 ; accuracy train : 69.19\n",
    "<class 'torch.Tensor'> tensor(142006) 142006\n",
    "204672 ; loss 0.54 ; sentence/s 207 ; words/s 11523 ; accuracy train : 69.34\n",
    "<class 'torch.Tensor'> tensor(151414) 151414\n",
    "217472 ; loss 0.53 ; sentence/s 204 ; words/s 11535 ; accuracy train : 69.58\n",
    "results : epoch 1 ; mean accuracy train : 69.6\n",
    "\n",
    "VALIDATION : Epoch 1\n",
    "list saved to file!\n",
    "list saved to file!\n",
    "togrep : results : epoch 1 ; mean accuracy valid :              70.89\n",
    "saving model at epoch 1\n",
    "\n",
    "TRAINING : Epoch 2\n",
    "Learning rate : 0.001\n",
    "<class 'torch.Tensor'> tensor(9205) 9205\n",
    "12672 ; loss 0.54 ; sentence/s 205 ; words/s 11616 ; accuracy train : 71.91\n",
    "<class 'torch.Tensor'> tensor(18588) 18588\n",
    "25472 ; loss 0.52 ; sentence/s 203 ; words/s 11875 ; accuracy train : 72.61\n",
    "---------------------------------------------------------------------------\n",
    "KeyboardInterrupt                         Traceback (most recent call last)\n",
    "<ipython-input-1-431fcb7fee2a> in <module>()\n",
    "    699 \n",
    "    700 while not stop_training and epoch <= params.n_epochs:\n",
    "--> 701     train_acc = trainepoch(epoch)\n",
    "    702     eval_acc = evaluate(epoch, 'valid')\n",
    "    703     epoch += 1\n",
    "\n",
    "<ipython-input-1-431fcb7fee2a> in trainepoch(epoch)\n",
    "    534         # prepare batch\n",
    "    535         s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size],\n",
    "--> 536                                      word_vec)\n",
    "    537         s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size],\n",
    "    538                                      word_vec)\n",
    "\n",
    "<ipython-input-1-431fcb7fee2a> in get_batch(batch, word_vec)\n",
    "    503     for i in range(len(batch)):\n",
    "    504         for j in range(len(batch[i])):\n",
    "--> 505             embed[j, i, :] = word_vec[batch[i][j]]\n",
    "    506 \n",
    "    507     return torch.from_numpy(embed).float(), lengths\n",
    "\n",
    "KeyboardInterrupt: \n",
    "\n",
    "\n",
    "1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> 573 573\n",
      "<class 'list'> <class 'list'> 73223 73223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "predict_valid = 'valid_3layernonlinear_2fearurescleanadam.pickle_predict.pkl'\n",
    "target_valid = 'valid_3layernonlinear_2fearurescleanadam.pickle_targets.pkl' #573\n",
    "\n",
    "#predict_valid = 'valid_3layernonlinear.pickle_predict.pkl' #158\n",
    "#target_valid = 'valid_3layernonlinear.pickle_targets.pkl'\n",
    "\n",
    "\n",
    "#predict_valid = 'valid_3layernonlinear_small.pickle_predict.pkl'\n",
    "#target_valid = 'valid_3layernonlinear_small.pickle_targets.pkl' #128\n",
    "\n",
    "\n",
    "\n",
    "def read_list(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        my_list = pickle.load(f)\n",
    "    return my_list\n",
    "\n",
    "\n",
    "predict = read_list(predict_valid)\n",
    "target = read_list(target_valid)\n",
    "predict_flatten = [item for sublist in predict for item in sublist]\n",
    "target_flatten = [item for sublist in target for item in sublist]\n",
    "print(type(predict),type(target),len(predict),len(target))\n",
    "print(type(predict_flatten),type(target_flatten),len(predict_flatten),len(target_flatten))\n",
    "\n",
    "#generate ROC curve and auc for clean. This is small or full? looks like small\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
