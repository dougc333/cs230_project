{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow2vec(txt,nlp):\n",
    "    \"\"\"\n",
    "    Converts txt to a list of word vectors,\"bag of words\" encoding,\n",
    "    and then averages the vectors to produce a single vector encoding\n",
    "    \"\"\"\n",
    "    txt = txt.strip()\n",
    "    words = nlp(txt)\n",
    "    \n",
    "    #already proved words.vector is average of words\n",
    "    #vecs = [word.vector for word in words ]\n",
    "\n",
    "    return words.vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> (1000, 2) (1000, 1)\n",
      "(800, 2) (200, 2) (800, 1) (200, 1)\n",
      "(1000, 600)\n",
      "(800, 600) (200, 600) (800, 1) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"quora_duplicate_questions.tsv\", delimiter=\"\\t\",nrows=1000)\n",
    "#list(df)\n",
    "#['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
    "df = df[['question1','question2','is_duplicate']]\n",
    "X =df[['question1','question2']].values\n",
    "Y = df[['is_duplicate']].values\n",
    "print(type(X),type(Y),X.shape,Y.shape)\n",
    "X_train,X_dev,Y_train,Y_dev = train_test_split(X,Y,test_size=.20,random_state=4)\n",
    "print(X_train.shape, X_dev.shape,Y_train.shape,Y_dev.shape)\n",
    "nlp = spacy.load('en_core_web_lg',disable=['parser', 'tagger', 'ner'])\n",
    "q1_vec = [bow2vec(x,nlp) for x in df['question1'].values]\n",
    "q2_vec = [bow2vec(x,nlp) for x in df['question2'].values]\n",
    "X=np.hstack((q1_vec,q2_vec))\n",
    "print(X.shape)\n",
    "X_train,X_dev,Y_train,Y_dev = train_test_split(X,Y,test_size=.20,random_state=4)\n",
    "print(X_train.shape, X_dev.shape,Y_train.shape,Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (800, 902)\n",
      "Test shape: (200, 902)\n",
      "Training model...\n",
      "\n",
      "Train Accuracy : 87.75%\n",
      "Test Accuracy  : 68.00%\n",
      "Test logloss   : 0.62\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not duplicate       0.71      0.80      0.75       121\n",
      " is duplicate       0.62      0.49      0.55        79\n",
      "\n",
      "    micro avg       0.68      0.68      0.68       200\n",
      "    macro avg       0.66      0.65      0.65       200\n",
      " weighted avg       0.67      0.68      0.67       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import *\n",
    "df = load_df(\"quora_duplicate_questions.tsv\")  \n",
    "df = df[0:1000]\n",
    "    \n",
    "# build data features\n",
    "X = make_features2(df,method='spacy')\n",
    "y = df[\"is_duplicate\"]\n",
    "    \n",
    "# split data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state = 4)\n",
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\",X_test.shape)     \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, \n",
    "                                                        random_state = 4)\n",
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\",X_test.shape)     \n",
    "    \n",
    "# train model\n",
    "print(\"Training model...\")\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)   \n",
    "\n",
    "# make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# print results\n",
    "print(\"\\nTrain Accuracy : %0.2f%%\"%(100*accuracy_score(y_train, model.predict(X_train))))\n",
    "print(\"Test Accuracy  : %0.2f%%\"%(100*accuracy_score(y_test,y_pred)))\n",
    "print(\"Test logloss   : %0.2f\"%log_loss(y_test,model.predict_proba(X_test)[:,1]))\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=['not duplicate','is duplicate']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def load_df(filename):\n",
    "    \"\"\"\n",
    "    Load quora duplicate question data from tsv file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep='\\t',  encoding = 'utf8',keep_default_na=False)\n",
    "    print(\"length of orignal data set: \", len(df))\n",
    "    \n",
    "    df = df.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "\n",
    "    # drop rows wo questions\n",
    "    df = df[df['question1']!='']\n",
    "    df = df[df['question2']!='']\n",
    "\n",
    "    print(\"length of processed data set: \",len(df))\n",
    "    return df\n",
    "\n",
    "def make_features(df,nlp=nlp,method='spacy'):\n",
    "    \"\"\"\n",
    "    Build similarity features from a pandas dataframe of question pairs.\n",
    "    \"\"\"\n",
    "    fasttext = None\n",
    "    \n",
    "    if method=='spacy':\n",
    "        print(\"Loading spacy 'en_core_web_lg'...\")\n",
    "        nlp = spacy.load('en_core_web_lg',disable=['parser', 'tagger', 'ner'])\n",
    "        \n",
    "    elif method=='fasttext':\n",
    "        import fastText as ft\n",
    "        print(\"Loading fasttext embeddings...\")        \n",
    "        fasttext = ft.load_model('/data/demo_quora_data/crawl-300d-2M-subword.bin')   \n",
    "         \n",
    "    print(\"Vectorizing question1...\")\n",
    "    q1_vec = [vec(q,nlp=nlp,fasttext=fasttext,method=method) for q in  df['question1'].values]\n",
    "    print(\"...Finished vectorizing question 1\")\n",
    "\n",
    "    print(\"Vectorizing question2...\")\n",
    "    q2_vec = [vec(q,nlp=nlp,fasttext=fasttext,method=method) for q in  df['question2'].values]\n",
    "    print(\"...Finished vectorizing question2\")\n",
    "    \n",
    "    # BoW difference vector\n",
    "    #bow_diff=np.array([abs(q2 - q1) for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "    \n",
    "    # BoW cosine feature\n",
    "    #print(\"Building BoW cosine similarity...\")\n",
    "    cos_sim = np.array([cosine_similarity(q1,q2) for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "\n",
    "    # BoW distance feature\n",
    "    #print(\"Building BoW euclidean similarity...\")    \n",
    "    #euclidean_sim = np.array([np.linalg.norm(q2 - q1) for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "\n",
    "    # BoW sum vector\n",
    "    #bow_sum=np.array([ q1 + q2 for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "    #df['cos_sim'] = cos_sim\n",
    "    #df['euclidean_sim'] = euclidean_sim\n",
    "    \n",
    "    #X = np.hstack((q1_vec, \n",
    "    #               q2_vec, \n",
    "    #               bow_diff, \n",
    "    #               cos_sim.reshape(-1,1),\n",
    "    #               euclidean_sim.reshape(-1,1)))\n",
    "    X = np.hstack((q1_vec, q2_vec))\n",
    "    \n",
    "    return X\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    return 1. - cosine(v1,v2)\n",
    "\n",
    "def vec(txt,nlp=nlp,fasttext=None,method='spacy'):\n",
    "    \n",
    "    txt = txt.strip()\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    words = nlp(txt)\n",
    "    words = [w for w in words]\n",
    "    \n",
    "    if len(words)==0:\n",
    "        words=nlp(u\"empty\")\n",
    "    \n",
    "    if method == 'spacy':\n",
    "        vecs = [word.vector  for word in words ]\n",
    "\n",
    "    elif method=='fasttext':\n",
    "        vecs= [fasttext.get_word_vector(word.text) for word in words]\n",
    "  \n",
    "    else:\n",
    "        print(\"Error: unknown method!\")\n",
    "    #dont need this for spacy\n",
    "    return np.mean(vecs,0)\n",
    "    #return [word.vector for word in words ]\n",
    "    \n",
    "    \n",
    "def make_features2(df,nlp=nlp,method='spacy',stopwords=True,add_to_df=True):\n",
    "    \"\"\"\n",
    "    Build similarity features from a pandas dataframe of question pairs.\n",
    "    \"\"\"\n",
    "    fasttext = None\n",
    "    \n",
    "    if method=='spacy':\n",
    "        print(\"Loading spacy 'en_core_web_lg'...\")\n",
    "        nlp = spacy.load('en_core_web_lg',disable=['parser', 'tagger', 'ner'])\n",
    "        \n",
    "    elif method=='fasttext':\n",
    "        import fastText as ft\n",
    "        print(\"Loading fasttext embeddings...\")        \n",
    "        fasttext = ft.load_model('/data/demo_quora_data/crawl-300d-2M-subword.bin')   \n",
    "         \n",
    "    print(\"Vectorizing question1...\")\n",
    "    q1_vec = [vec(q,nlp=nlp,fasttext=fasttext,method=method) \n",
    "                for q in  df['question1'].values]\n",
    "    print(\"...Finished vectorizing question 1\")\n",
    "\n",
    "    print(\"Vectorizing question2...\")\n",
    "    q2_vec = [vec(q,nlp=nlp,fasttext=fasttext,method=method) \n",
    "                for q in  df['question2'].values]\n",
    "    print(\"...Finished vectorizing question2\")\n",
    "    \n",
    "    # BoW difference vector\n",
    "    bow_diff=np.array([abs(q2 - q1) for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "    \n",
    "    # BoW cosine feature\n",
    "    print(\"Building BoW cosine similarity...\")\n",
    "    cos_sim = np.array([cosine_similarity(q1,q2) for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "\n",
    "    # BoW distance feature\n",
    "    print(\"Building BoW euclidean similarity...\")    \n",
    "    euclidean_sim = np.array([np.linalg.norm(q2 - q1) for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "\n",
    "    # BoW sum vector\n",
    "    #bow_sum=np.array([ q1 + q2 for (q1,q2) in zip(q1_vec,q2_vec)])\n",
    "    \n",
    "    X = np.hstack((q1_vec, \n",
    "                   q2_vec, \n",
    "                   bow_diff, \n",
    "                   cos_sim.reshape(-1,1),\n",
    "                   euclidean_sim.reshape(-1,1)))\n",
    "\n",
    "    if add_to_df:\n",
    "        df['cos_sim'] = cos_sim\n",
    "        df['euclidean_sim'] = euclidean_sim\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of orignal data set:  404290\n",
      "length of processed data set:  404288\n",
      "Loading spacy 'en_core_web_lg'...\n",
      "Vectorizing question1...\n",
      "...Finished vectorizing question 1\n",
      "Vectorizing question2...\n",
      "...Finished vectorizing question2\n",
      "Building BoW cosine similarity...\n",
      "Building BoW euclidean similarity...\n",
      "Train shape: (800, 600)\n",
      "Test shape: (200, 600)\n"
     ]
    }
   ],
   "source": [
    "df = load_df(\"quora_duplicate_questions.tsv\")  \n",
    "df = df[0:1000]\n",
    "    \n",
    "# build data features\n",
    "X = make_features(df,method='spacy')\n",
    "y = df[\"is_duplicate\"]\n",
    "    \n",
    "# split data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state = 4)\n",
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\",X_test.shape)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (800, 600)\n",
      "Test shape: (200, 600)\n",
      "Training model...\n",
      "\n",
      "Train Accuracy : 82.00%\n",
      "Test Accuracy  : 64.00%\n",
      "Test logloss   : 0.64\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not duplicate       0.66      0.83      0.74       121\n",
      " is duplicate       0.57      0.35      0.44        79\n",
      "\n",
      "    micro avg       0.64      0.64      0.64       200\n",
      "    macro avg       0.62      0.59      0.59       200\n",
      " weighted avg       0.63      0.64      0.62       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, \n",
    "                                                        random_state = 4)\n",
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\",X_test.shape)     \n",
    "    \n",
    "# train model\n",
    "print(\"Training model...\")\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)   \n",
    "\n",
    "# make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# print results\n",
    "print(\"\\nTrain Accuracy : %0.2f%%\"%(100*accuracy_score(y_train, model.predict(X_train))))\n",
    "print(\"Test Accuracy  : %0.2f%%\"%(100*accuracy_score(y_test,y_pred)))\n",
    "print(\"Test logloss   : %0.2f\"%log_loss(y_test,model.predict_proba(X_test)[:,1]))\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=['not duplicate','is duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--quora_filename QUORA_FILENAME]\n",
      "                             [--nrows NROWS] [--model_filename MODEL_FILENAME]\n",
      "                             [--use_cuda USE_CUDA] [--test_size TEST_SIZE]\n",
      "                             [--random_state RANDOM_STATE]\n",
      "                             [--infersent_encoder_path INFERSENT_ENCODER_PATH]\n",
      "                             [--word_vec_path WORD_VEC_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-e991e4d7-c78b-4492-bffe-b2cb81222786.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/anaconda3/envs/cs230/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "\n",
    "#from utils_infersent import load_infersent, build_features\n",
    "\n",
    "def load_quora_df(filename):\n",
    "    \"\"\"Load quora duplicate question data from tsv file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(quora_filename, sep='\\t',  encoding = 'utf8',keep_default_na=False)\n",
    "    print(\"length of orignal data set: \", len(df))\n",
    "    \n",
    "    df = df.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "\n",
    "    # drop rows wo questions\n",
    "    df = df[df['question1']!='']\n",
    "    df = df[df['question2']!='']\n",
    "\n",
    "    print(\"length of processed data set: \",len(df))\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--quora_filename', default='./quora_duplicate_questions.tsv',type=str,\n",
    "                        help='file containing quora questions in tsv format')  \n",
    "    parser.add_argument('--nrows', default=None, type=int,\n",
    "                        help='number of rows of quora data to use')                        \n",
    "    parser.add_argument('--model_filename', default='./model_infersent/saved_model_infersent.pkl',type=str,\n",
    "                        help='pickle file containing stored model')\n",
    "                          \n",
    "    parser.add_argument('--use_cuda',  default=True, type=lambda x: (str(x).lower() == 'true'),\n",
    "                        help='Use cuda on GPU')    \n",
    "    parser.add_argument('--test_size', default=0.25,type=float,\n",
    "                        help='test size fraction')     \n",
    "                        \n",
    "    parser.add_argument('--random_state', default=42,type=int,\n",
    "                        help='random state seed')                          \n",
    "\n",
    "    parser.add_argument('--infersent_encoder_path', \n",
    "                        default='./model_infersent/infersent2.pkl',type=str,\n",
    "                        help='path to infersent enocoder model')                          \n",
    "\n",
    "    parser.add_argument('--word_vec_path', \n",
    "                        default='./model_infersent/crawl-300d-2M.vec',type=str,\n",
    "                        help='path to word vector embedding')        \n",
    "                                       \n",
    "    args = parser.parse_args()\n",
    "    quora_filename = args.quora_filename\n",
    "    nrows = args.nrows     \n",
    "    model_filename = args.model_filename\n",
    "    use_cuda = args.use_cuda     \n",
    "    test_size = args.test_size\n",
    "    random_state = args.random_state\n",
    "    encoder_path = args.infersent_encoder_path\n",
    "    word_vec_path = args.word_vec_path   \n",
    "            \n",
    "    # read quora tsv\n",
    "    print(\"Loading quora tsv file...\") \n",
    "    df = load_quora_df(quora_filename)\n",
    "    if nrows: df = df[0:nrows]\n",
    "    \n",
    "    # get infersent encoder\n",
    "    encoder = load_infersent(encoder_path,word_vec_path,use_cuda)\n",
    "\n",
    "    # build data features\n",
    "    X = build_features(df,encoder)\n",
    "    y = df[\"is_duplicate\"]\n",
    "        \n",
    "    # split data into training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state = random_state)\n",
    "                                                        \n",
    "    del X, encoder, df                                          \n",
    "                                                        \n",
    "                                                        \n",
    "    print(\"Train shape:\",X_train.shape)\n",
    "    print(\"Test shape:\",X_test.shape)     \n",
    "    \n",
    "    # train model\n",
    "    print(\"Training model...\")\n",
    "    #model = LogisticRegression()\n",
    "    model = SGDClassifier(loss='modified_huber',tol=1e-4,verbose=1,random_state=random_state)\n",
    "    model.fit(X_train, y_train)   \n",
    "\n",
    "    # make predictions on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # print results\n",
    "    print(\"\\nTrain Accuracy : %0.2f%%\"%(100*accuracy_score(y_train, model.predict(X_train))))\n",
    "    print(\"Test Accuracy  : %0.2f%%\"%(100*accuracy_score(y_test,y_pred)))\n",
    "    print(\"Test logloss   : %0.2f\"%log_loss(y_test,model.predict_proba(X_test)[:,1]))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, target_names=['not duplicate','is duplicate']))\n",
    "\n",
    "\n",
    "    #save model\n",
    "    with open(model_filename, 'wb') as file:  \n",
    "        pickle.dump(model, file)\n",
    "        \n",
    "    print(\"Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
