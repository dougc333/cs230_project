{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "tf.executing_eagerly() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can implement an RNN: \n",
    "\n",
    "**Steps**:\n",
    "1. Implement the calculations needed for one time-step of the RNN.\n",
    "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. \n",
    "\n",
    "Let's go!\n",
    "\n",
    "## 1.1 - RNN cell\n",
    "\n",
    "A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "**Exercise**: Implement the RNN-cell described in Figure (2).\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. We provided you a function: `softmax`.\n",
    "3. Store $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in cache\n",
    "4. Return $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ and cache\n",
    "\n",
    "We will vectorize over $m$ examples. Thus, $x^{\\langle t \\rangle}$ will have dimension $(n_x,m)$, and $a^{\\langle t \\rangle}$ will have dimension $(n_a,m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387\n",
      "   1.74481176 -0.7612069   0.3190391  -0.24937038]\n",
      " [ 1.46210794 -2.06014071 -0.3224172  -0.38405435  1.13376944 -1.09989127\n",
      "  -0.17242821 -0.87785842  0.04221375  0.58281521]\n",
      " [-1.10061918  1.14472371  0.90159072  0.50249434  0.90085595 -0.68372786\n",
      "  -0.12289023 -0.93576943 -0.26788808  0.53035547]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rnn_cell_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c5095660bd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Waa\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wax\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wya\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ba\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"by\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0ma_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_cell_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a_next[4] = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a_next.shape = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn_cell_forward' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \n",
    "\n",
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "print(xt)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Pytorch Internals:https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html</h6>\n",
    "$h_t = tanh(W_{ih}x_t + b_{ih} + W_{hh}h_{t-1}+b_{hh})$ \n",
    "<p>the PyTorch code puts the tanh equation into forward which is called w/parameter match. __call__ directs\n",
    "to forward()</p>\n",
    "#rnn(params) goes to __call__, https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L485\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20 1\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(nonlinearity='tanh',input_size=10,hidden_size=20,num_layers = 1) # input_size=10,hidden_size=20,num_layers=1 \n",
    "print(rnn.input_size,rnn.hidden_size,rnn.num_layers)\n",
    "input= torch.randn(5,3,10) # \n",
    "h0 = torch.randn(1,3,20)\n",
    "output,hn=rnn(input,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1, 3)\n",
      "0 27566712.0\n",
      "1 20002552.0\n",
      "2 16637350.0\n",
      "3 14413199.0\n",
      "4 12254453.0\n",
      "5 9924016.0\n",
      "6 7619562.5\n",
      "7 5577152.5\n",
      "8 3957862.5\n",
      "9 2767106.5\n",
      "10 1939058.75\n",
      "11 1378717.25\n",
      "12 1003990.6875\n",
      "13 752204.125\n",
      "14 580510.875\n",
      "15 460580.125\n",
      "16 374434.5\n",
      "17 310672.65625\n",
      "18 262117.265625\n",
      "19 224134.78125\n",
      "20 193661.703125\n",
      "21 168791.015625\n",
      "22 148077.6875\n",
      "23 130605.03125\n",
      "24 115693.7890625\n",
      "25 102862.3984375\n",
      "26 91743.2578125\n",
      "27 82053.3984375\n",
      "28 73575.7109375\n",
      "29 66124.4375\n",
      "30 59550.08984375\n",
      "31 53728.765625\n",
      "32 48562.58984375\n",
      "33 43993.13671875\n",
      "34 39921.33984375\n",
      "35 36280.9375\n",
      "36 33018.4921875\n",
      "37 30089.337890625\n",
      "38 27453.955078125\n",
      "39 25087.958984375\n",
      "40 22959.927734375\n",
      "41 21037.779296875\n",
      "42 19297.603515625\n",
      "43 17719.751953125\n",
      "44 16287.3056640625\n",
      "45 14984.5517578125\n",
      "46 13798.66015625\n",
      "47 12718.4169921875\n",
      "48 11734.0068359375\n",
      "49 10834.8017578125\n",
      "50 10012.287109375\n",
      "51 9259.029296875\n",
      "52 8568.662109375\n",
      "53 7935.3046875\n",
      "54 7353.9169921875\n",
      "55 6819.3828125\n",
      "56 6327.76513671875\n",
      "57 5875.34521484375\n",
      "58 5458.39794921875\n",
      "59 5073.9248046875\n",
      "60 4719.1142578125\n",
      "61 4391.35498046875\n",
      "62 4088.496826171875\n",
      "63 3808.447998046875\n",
      "64 3549.391845703125\n",
      "65 3309.39111328125\n",
      "66 3087.349609375\n",
      "67 2881.4638671875\n",
      "68 2690.48486328125\n",
      "69 2513.262939453125\n",
      "70 2348.622802734375\n",
      "71 2195.60107421875\n",
      "72 2053.356201171875\n",
      "73 1921.05908203125\n",
      "74 1797.92236328125\n",
      "75 1683.2459716796875\n",
      "76 1576.400634765625\n",
      "77 1476.8082275390625\n",
      "78 1383.9388427734375\n",
      "79 1297.3427734375\n",
      "80 1216.51513671875\n",
      "81 1141.063232421875\n",
      "82 1070.562255859375\n",
      "83 1004.708984375\n",
      "84 943.167236328125\n",
      "85 885.6121826171875\n",
      "86 831.77685546875\n",
      "87 781.4058837890625\n",
      "88 734.2643432617188\n",
      "89 690.1270141601562\n",
      "90 648.779296875\n",
      "91 610.052734375\n",
      "92 573.7471313476562\n",
      "93 539.7195434570312\n",
      "94 507.80487060546875\n",
      "95 477.8758544921875\n",
      "96 449.7952575683594\n",
      "97 423.4496765136719\n",
      "98 398.71893310546875\n",
      "99 375.4926452636719\n",
      "100 353.6897277832031\n",
      "101 333.20526123046875\n",
      "102 313.9576416015625\n",
      "103 295.8674011230469\n",
      "104 278.8611755371094\n",
      "105 262.8721008300781\n",
      "106 247.83352661132812\n",
      "107 233.69192504882812\n",
      "108 220.38372802734375\n",
      "109 207.8628692626953\n",
      "110 196.07998657226562\n",
      "111 184.98992919921875\n",
      "112 174.5531768798828\n",
      "113 164.7239227294922\n",
      "114 155.46604919433594\n",
      "115 146.74478149414062\n",
      "116 138.53419494628906\n",
      "117 130.7947540283203\n",
      "118 123.49951934814453\n",
      "119 116.62262725830078\n",
      "120 110.14139556884766\n",
      "121 104.03067016601562\n",
      "122 98.26673889160156\n",
      "123 92.83436584472656\n",
      "124 87.71038818359375\n",
      "125 82.87539672851562\n",
      "126 78.31438446044922\n",
      "127 74.01224517822266\n",
      "128 69.95103454589844\n",
      "129 66.11832427978516\n",
      "130 62.50005340576172\n",
      "131 59.08491897583008\n",
      "132 55.86181640625\n",
      "133 52.81745529174805\n",
      "134 49.94399642944336\n",
      "135 47.229496002197266\n",
      "136 44.6649055480957\n",
      "137 42.243736267089844\n",
      "138 39.955753326416016\n",
      "139 37.795047760009766\n",
      "140 35.75262451171875\n",
      "141 33.823516845703125\n",
      "142 31.999929428100586\n",
      "143 30.276884078979492\n",
      "144 28.648765563964844\n",
      "145 27.109371185302734\n",
      "146 25.65419578552246\n",
      "147 24.2784423828125\n",
      "148 22.977981567382812\n",
      "149 21.74825668334961\n",
      "150 20.58545684814453\n",
      "151 19.48590087890625\n",
      "152 18.44597816467285\n",
      "153 17.462018966674805\n",
      "154 16.531936645507812\n",
      "155 15.65168571472168\n",
      "156 14.820028305053711\n",
      "157 14.032224655151367\n",
      "158 13.286712646484375\n",
      "159 12.581694602966309\n",
      "160 11.915109634399414\n",
      "161 11.283876419067383\n",
      "162 10.686176300048828\n",
      "163 10.121330261230469\n",
      "164 9.586456298828125\n",
      "165 9.080350875854492\n",
      "166 8.600898742675781\n",
      "167 8.147461891174316\n",
      "168 7.717777252197266\n",
      "169 7.311426162719727\n",
      "170 6.926689624786377\n",
      "171 6.562365531921387\n",
      "172 6.217127323150635\n",
      "173 5.890729904174805\n",
      "174 5.581292152404785\n",
      "175 5.288665771484375\n",
      "176 5.0114336013793945\n",
      "177 4.748713970184326\n",
      "178 4.499979019165039\n",
      "179 4.264637470245361\n",
      "180 4.041536331176758\n",
      "181 3.8302669525146484\n",
      "182 3.6301538944244385\n",
      "183 3.440312623977661\n",
      "184 3.2608234882354736\n",
      "185 3.090766668319702\n",
      "186 2.9296085834503174\n",
      "187 2.777085065841675\n",
      "188 2.632517099380493\n",
      "189 2.4954185485839844\n",
      "190 2.365530490875244\n",
      "191 2.242539405822754\n",
      "192 2.1259820461273193\n",
      "193 2.0155975818634033\n",
      "194 1.9107983112335205\n",
      "195 1.8116919994354248\n",
      "196 1.7175530195236206\n",
      "197 1.6285055875778198\n",
      "198 1.5441256761550903\n",
      "199 1.4641661643981934\n",
      "200 1.3883380889892578\n",
      "201 1.3163546323776245\n",
      "202 1.2482457160949707\n",
      "203 1.1837979555130005\n",
      "204 1.1225522756576538\n",
      "205 1.0645158290863037\n",
      "206 1.0095897912979126\n",
      "207 0.9574540853500366\n",
      "208 0.9080316424369812\n",
      "209 0.861160397529602\n",
      "210 0.8166243433952332\n",
      "211 0.7744957804679871\n",
      "212 0.7347288131713867\n",
      "213 0.696792721748352\n",
      "214 0.6608476638793945\n",
      "215 0.6268077492713928\n",
      "216 0.5946304202079773\n",
      "217 0.5640456676483154\n",
      "218 0.5349825620651245\n",
      "219 0.5075148344039917\n",
      "220 0.48141661286354065\n",
      "221 0.4567481577396393\n",
      "222 0.43323850631713867\n",
      "223 0.41103577613830566\n",
      "224 0.389896422624588\n",
      "225 0.3699267506599426\n",
      "226 0.35099151730537415\n",
      "227 0.33292123675346375\n",
      "228 0.31585294008255005\n",
      "229 0.2996388375759125\n",
      "230 0.28428366780281067\n",
      "231 0.26979559659957886\n",
      "232 0.25595757365226746\n",
      "233 0.2428334802389145\n",
      "234 0.2304503321647644\n",
      "235 0.21867066621780396\n",
      "236 0.20751845836639404\n",
      "237 0.19694262742996216\n",
      "238 0.18684400618076324\n",
      "239 0.17728987336158752\n",
      "240 0.16821177303791046\n",
      "241 0.15968215465545654\n",
      "242 0.15150965750217438\n",
      "243 0.14379414916038513\n",
      "244 0.13644398748874664\n",
      "245 0.12949159741401672\n",
      "246 0.12289220839738846\n",
      "247 0.1166134849190712\n",
      "248 0.11067724227905273\n",
      "249 0.10503126680850983\n",
      "250 0.09970083832740784\n",
      "251 0.09458667784929276\n",
      "252 0.089789018034935\n",
      "253 0.08524327725172043\n",
      "254 0.08088453114032745\n",
      "255 0.07678914815187454\n",
      "256 0.07290006428956985\n",
      "257 0.0691969245672226\n",
      "258 0.06567297875881195\n",
      "259 0.06235453486442566\n",
      "260 0.05919165164232254\n",
      "261 0.05616641789674759\n",
      "262 0.05329113453626633\n",
      "263 0.05061006546020508\n",
      "264 0.048047177493572235\n",
      "265 0.045619383454322815\n",
      "266 0.04330957308411598\n",
      "267 0.041123248636722565\n",
      "268 0.03902209550142288\n",
      "269 0.037062741816043854\n",
      "270 0.03517794981598854\n",
      "271 0.03342045843601227\n",
      "272 0.031707584857940674\n",
      "273 0.030108485370874405\n",
      "274 0.02858659066259861\n",
      "275 0.02715270034968853\n",
      "276 0.02578699216246605\n",
      "277 0.024504149332642555\n",
      "278 0.02326524630188942\n",
      "279 0.02208622731268406\n",
      "280 0.020984405651688576\n",
      "281 0.019938241690397263\n",
      "282 0.0189357977360487\n",
      "283 0.017975443974137306\n",
      "284 0.017079947516322136\n",
      "285 0.016225572675466537\n",
      "286 0.015415607020258904\n",
      "287 0.014644771814346313\n",
      "288 0.013915919698774815\n",
      "289 0.013223922811448574\n",
      "290 0.01256888173520565\n",
      "291 0.011952058412134647\n",
      "292 0.011356395669281483\n",
      "293 0.010794727131724358\n",
      "294 0.010259105823934078\n",
      "295 0.009755606763064861\n",
      "296 0.009274128824472427\n",
      "297 0.008815506473183632\n",
      "298 0.00838262028992176\n",
      "299 0.007972609251737595\n",
      "300 0.007579341996461153\n",
      "301 0.0072134858928620815\n",
      "302 0.006857261992990971\n",
      "303 0.006527972407639027\n",
      "304 0.0062088132835924625\n",
      "305 0.005911978427320719\n",
      "306 0.0056249825283885\n",
      "307 0.0053568375296890736\n",
      "308 0.005101699382066727\n",
      "309 0.00485906982794404\n",
      "310 0.004624570719897747\n",
      "311 0.0044089434668421745\n",
      "312 0.004198106937110424\n",
      "313 0.004002304282039404\n",
      "314 0.0038117156364023685\n",
      "315 0.0036309403367340565\n",
      "316 0.0034653409384191036\n",
      "317 0.0033016796223819256\n",
      "318 0.003149039577692747\n",
      "319 0.0030056529212743044\n",
      "320 0.0028716097585856915\n",
      "321 0.002740398980677128\n",
      "322 0.0026163174770772457\n",
      "323 0.0024990024976432323\n",
      "324 0.0023836547043174505\n",
      "325 0.0022793097887188196\n",
      "326 0.002177371410652995\n",
      "327 0.002083208179101348\n",
      "328 0.0019900277256965637\n",
      "329 0.0019048115937039256\n",
      "330 0.0018207686953246593\n",
      "331 0.0017424506368115544\n",
      "332 0.0016681344714015722\n",
      "333 0.0015975539572536945\n",
      "334 0.0015332334442064166\n",
      "335 0.0014679961604997516\n",
      "336 0.001409544376656413\n",
      "337 0.001349056139588356\n",
      "338 0.001294732908718288\n",
      "339 0.0012404137523844838\n",
      "340 0.0011907480657100677\n",
      "341 0.0011433756444603205\n",
      "342 0.0010968375718221068\n",
      "343 0.0010537642519921064\n",
      "344 0.0010114179458469152\n",
      "345 0.0009728107834234834\n",
      "346 0.0009351203916594386\n",
      "347 0.0008978821570053697\n",
      "348 0.0008640330052003264\n",
      "349 0.0008307508542202413\n",
      "350 0.0008008121512830257\n",
      "351 0.0007705846801400185\n",
      "352 0.0007428985554724932\n",
      "353 0.0007136391359381378\n",
      "354 0.0006888742209412158\n",
      "355 0.0006640216452069581\n",
      "356 0.0006413087830878794\n",
      "357 0.0006174758891575038\n",
      "358 0.0005960077396593988\n",
      "359 0.0005767173133790493\n",
      "360 0.00055502267787233\n",
      "361 0.0005362529191188514\n",
      "362 0.000518570770509541\n",
      "363 0.0005004492122679949\n",
      "364 0.00048479312681593\n",
      "365 0.0004686984757427126\n",
      "366 0.0004527001001406461\n",
      "367 0.0004382647748570889\n",
      "368 0.0004260119458194822\n",
      "369 0.0004112787137273699\n",
      "370 0.0003984231734648347\n",
      "371 0.00038494644104503095\n",
      "372 0.0003727622388396412\n",
      "373 0.00036179335438646376\n",
      "374 0.0003505097411107272\n",
      "375 0.00034008652437478304\n",
      "376 0.00033005161094479263\n",
      "377 0.00031990036950446665\n",
      "378 0.0003105513460468501\n",
      "379 0.0003010250220540911\n",
      "380 0.0002929484180640429\n",
      "381 0.00028368091443553567\n",
      "382 0.0002760665083769709\n",
      "383 0.00026722942129708827\n",
      "384 0.0002606397611089051\n",
      "385 0.00025314107188023627\n",
      "386 0.0002465102879796177\n",
      "387 0.0002397585049038753\n",
      "388 0.0002334542223252356\n",
      "389 0.00022704173170495778\n",
      "390 0.00022097294277045876\n",
      "391 0.00021436807583086193\n",
      "392 0.0002090412308461964\n",
      "393 0.00020318396855145693\n",
      "394 0.00019821726891677827\n",
      "395 0.00019320406136102974\n",
      "396 0.00018789601745083928\n",
      "397 0.00018330253078602254\n",
      "398 0.00017796851170714945\n",
      "399 0.00017368028056807816\n",
      "400 0.00016934431914705783\n",
      "401 0.0001652842911425978\n",
      "402 0.00016116943152155727\n",
      "403 0.00015725371486041695\n",
      "404 0.0001544803672004491\n",
      "405 0.00015030665963422507\n",
      "406 0.0001464475499233231\n",
      "407 0.00014293508138507605\n",
      "408 0.00013931677676737309\n",
      "409 0.00013654778013005853\n",
      "410 0.0001330255763605237\n",
      "411 0.0001303781318711117\n",
      "412 0.0001269440836040303\n",
      "413 0.00012401360436342657\n",
      "414 0.00012152413546573371\n",
      "415 0.00011874907067976892\n",
      "416 0.00011669148807413876\n",
      "417 0.00011379041825421154\n",
      "418 0.00011154004459967837\n",
      "419 0.00010926799586741254\n",
      "420 0.0001074310130206868\n",
      "421 0.00010491204011486843\n",
      "422 0.00010296942491549999\n",
      "423 0.00010065087553812191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 9.859283454716206e-05\n",
      "425 9.643431258155033e-05\n",
      "426 9.477358253207058e-05\n",
      "427 9.260043589165434e-05\n",
      "428 9.112207044381648e-05\n",
      "429 8.866759890224785e-05\n",
      "430 8.732217247597873e-05\n",
      "431 8.544359297957271e-05\n",
      "432 8.340102067450061e-05\n",
      "433 8.171754598151892e-05\n",
      "434 8.06076277513057e-05\n",
      "435 7.886979437898844e-05\n",
      "436 7.769790681777522e-05\n",
      "437 7.593689952045679e-05\n",
      "438 7.422159978887066e-05\n",
      "439 7.331671804422513e-05\n",
      "440 7.205112342489883e-05\n",
      "441 7.085664401529357e-05\n",
      "442 6.912554817972705e-05\n",
      "443 6.816781387897208e-05\n",
      "444 6.690061127301306e-05\n",
      "445 6.604007649002597e-05\n",
      "446 6.474024121416733e-05\n",
      "447 6.371351628331468e-05\n",
      "448 6.255101470742375e-05\n",
      "449 6.150281114969403e-05\n",
      "450 6.063249020371586e-05\n",
      "451 5.96462850808166e-05\n",
      "452 5.895300273550674e-05\n",
      "453 5.765011519542895e-05\n",
      "454 5.6483087973902e-05\n",
      "455 5.5639415222685784e-05\n",
      "456 5.4982552683213726e-05\n",
      "457 5.4012165492167696e-05\n",
      "458 5.334473098628223e-05\n",
      "459 5.236173092271201e-05\n",
      "460 5.1572671509347856e-05\n",
      "461 5.0954316975548863e-05\n",
      "462 4.9968868552241474e-05\n",
      "463 4.919378989143297e-05\n",
      "464 4.822511982638389e-05\n",
      "465 4.752055974677205e-05\n",
      "466 4.695052848546766e-05\n",
      "467 4.62503703602124e-05\n",
      "468 4.5164546463638544e-05\n",
      "469 4.479446215555072e-05\n",
      "470 4.423121572472155e-05\n",
      "471 4.3558840843616053e-05\n",
      "472 4.288105628802441e-05\n",
      "473 4.2328465497121215e-05\n",
      "474 4.181585609330796e-05\n",
      "475 4.10505999752786e-05\n",
      "476 4.04074125981424e-05\n",
      "477 3.990741970483214e-05\n",
      "478 3.93745576729998e-05\n",
      "479 3.878434654325247e-05\n",
      "480 3.821293648798019e-05\n",
      "481 3.775793084059842e-05\n",
      "482 3.723708869074471e-05\n",
      "483 3.665643816930242e-05\n",
      "484 3.6416771763470024e-05\n",
      "485 3.572111018002033e-05\n",
      "486 3.548336826497689e-05\n",
      "487 3.5093267797492445e-05\n",
      "488 3.452743840171024e-05\n",
      "489 3.418085543671623e-05\n",
      "490 3.361533163115382e-05\n",
      "491 3.3247088140342385e-05\n",
      "492 3.282539910287596e-05\n",
      "493 3.2483450922882184e-05\n",
      "494 3.2008108973968774e-05\n",
      "495 3.160117194056511e-05\n",
      "496 3.13243581331335e-05\n",
      "497 3.0864433938404545e-05\n",
      "498 3.030253719771281e-05\n",
      "499 3.0002845960552804e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = torch.randn(1, 3)  \n",
    "print(type(inputs.data.numpy()),inputs.data.numpy().shape)\n",
    "#what is a difference bewteen a Tensor and Variable? A variable is a wrapper for tensor which allows\n",
    "#back propagation and forward\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "loss_graph=[]\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    loss_graph.append(loss.item())\n",
    "    print(t, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(loss_graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 28\n",
    "timesteps = 28\n",
    "num_classes = 10\n",
    "num_hidden=128\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "rnn = RNN(X,weights,biases)\n",
    "print(type(rnn))\n",
    "#print out the parameters, this just returns wx,bx\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run( tf.global_variables_initializer())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "k = keras.layers.LSTMCell(units=2, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                      recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, \n",
    "                      kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "                      kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, \n",
    "                          dropout=0.0, recurrent_dropout=0.0, implementation=1)\n",
    "\n",
    "print(\"asdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb_lstm.py\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addition.py\n",
    "# -*- coding: utf-8 -*-\n",
    "'''An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "\n",
    "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "\n",
    "Two digits reversed:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "\n",
    "Three digits reversed:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "\n",
    "Four digits reversed:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "\n",
    "Five digits reversed:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''  # noqa\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + 'â˜‘' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + 'â˜’' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a memory network on the bAbI dataset.\n",
    "\n",
    "References:\n",
    "\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895\n",
    "\n",
    "Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
    "Time per epoch: 3s on CPU (core i7).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_'\n",
    "                                  'single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_'\n",
    "                                'two-supporting-facts_{}.txt',\n",
    "}\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "with tarfile.open(path) as tar:\n",
    "    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=120,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains two recurrent neural networks based upon a story and a question.\n",
    "\n",
    "The resulting merged vector is then queried to answer a range of bAbI tasks.\n",
    "\n",
    "The results are comparable to those for an LSTM model provided in Weston et al.:\n",
    "\"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"\n",
    "http://arxiv.org/abs/1502.05698\n",
    "\n",
    "Task Number                  | FB LSTM Baseline | Keras QA\n",
    "---                          | ---              | ---\n",
    "QA1 - Single Supporting Fact | 50               | 52.1\n",
    "QA2 - Two Supporting Facts   | 20               | 37.0\n",
    "QA3 - Three Supporting Facts | 20               | 20.5\n",
    "QA4 - Two Arg. Relations     | 61               | 62.9\n",
    "QA5 - Three Arg. Relations   | 70               | 61.9\n",
    "QA6 - yes/No Questions       | 48               | 50.7\n",
    "QA7 - Counting               | 49               | 78.9\n",
    "QA8 - Lists/Sets             | 45               | 77.2\n",
    "QA9 - Simple Negation        | 64               | 64.0\n",
    "QA10 - Indefinite Knowledge  | 44               | 47.7\n",
    "QA11 - Basic Coreference     | 72               | 74.9\n",
    "QA12 - Conjunction           | 74               | 76.4\n",
    "QA13 - Compound Coreference  | 94               | 94.4\n",
    "QA14 - Time Reasoning        | 27               | 34.8\n",
    "QA15 - Basic Deduction       | 21               | 32.4\n",
    "QA16 - Basic Induction       | 23               | 50.6\n",
    "QA17 - Positional Reasoning  | 51               | 49.1\n",
    "QA18 - Size Reasoning        | 52               | 90.8\n",
    "QA19 - Path Finding          | 8                | 9.0\n",
    "QA20 - Agent's Motivations   | 91               | 90.7\n",
    "\n",
    "For the resources related to the bAbI project, refer to:\n",
    "https://research.facebook.com/researchers/1543934539189348\n",
    "\n",
    "# Notes\n",
    "\n",
    "- With default word, sentence, and query vector sizes, the GRU model achieves:\n",
    "  - 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n",
    "  - 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n",
    "In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n",
    "\n",
    "- The task does not traditionally parse the question separately. This likely\n",
    "improves accuracy and is a good example of merging two RNNs.\n",
    "\n",
    "- The word vector embeddings are not shared between the story and question RNNs.\n",
    "\n",
    "- See how the accuracy changes given 10,000 training samples (en-10k) instead\n",
    "of only 1000. 1000 was used in order to be comparable to the original paper.\n",
    "\n",
    "- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n",
    "\n",
    "- The length and noise (i.e. 'useless' story components) impact the ability of\n",
    "LSTMs / GRUs to provide the correct answer. Given only the supporting facts,\n",
    "these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\n",
    "networks that use attentional processes can efficiently search through this\n",
    "noise to find the relevant statements, improving performance substantially.\n",
    "This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "\n",
    "    If only_supporting is true,\n",
    "    only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return (pad_sequences(xs, maxlen=story_maxlen),\n",
    "            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/'\n",
    "                           'babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n",
    "          '.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\n",
    "# Default QA1 with 1000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
    "# QA1 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "# QA2 with 1000 samples\n",
    "challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
    "# QA2 with 10,000 samples\n",
    "# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "with tarfile.open(path) as tar:\n",
    "    train = get_stories(tar.extractfile(challenge.format('train')))\n",
    "    test = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xq.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n",
    "encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question)\n",
    "\n",
    "merged = layers.concatenate([encoded_sentence, encoded_question])\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "\n",
    "print('Evaluation')\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_lstm for next frame movie prediction\n",
    "\"\"\" This script demonstrates the use of a convolutional LSTM network.\n",
    "\n",
    "This network is used to predict the next frame of an artificially\n",
    "generated movie which contains moving squares.\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "# We create a layer which take as input movies of shape\n",
    "# (n_frames, width, height, channels) and returns a movie\n",
    "# of identical shape.\n",
    "\n",
    "seq = Sequential()\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   input_shape=(None, 40, 40, 1),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "\n",
    "\n",
    "# Artificial data generation:\n",
    "# Generate movies with 3 to 7 moving squares inside.\n",
    "# The squares are of shape 1x1 or 2x2 pixels,\n",
    "# which move linearly over time.\n",
    "# For convenience we first create movies with bigger width and height (80x80)\n",
    "# and at the end we select a 40x40 window.\n",
    "\n",
    "def generate_movies(n_samples=1200, n_frames=15):\n",
    "    row = 80\n",
    "    col = 80\n",
    "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
    "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n",
    "                              dtype=np.float)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Add 3 to 7 moving squares\n",
    "        n = np.random.randint(3, 8)\n",
    "\n",
    "        for j in range(n):\n",
    "            # Initial position\n",
    "            xstart = np.random.randint(20, 60)\n",
    "            ystart = np.random.randint(20, 60)\n",
    "            # Direction of motion\n",
    "            directionx = np.random.randint(0, 3) - 1\n",
    "            directiony = np.random.randint(0, 3) - 1\n",
    "\n",
    "            # Size of the square\n",
    "            w = np.random.randint(2, 4)\n",
    "\n",
    "            for t in range(n_frames):\n",
    "                x_shift = xstart + directionx * t\n",
    "                y_shift = ystart + directiony * t\n",
    "                noisy_movies[i, t, x_shift - w: x_shift + w,\n",
    "                             y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "                # Make it more robust by adding noise.\n",
    "                # The idea is that if during inference,\n",
    "                # the value of the pixel is not exactly one,\n",
    "                # we need to train the network to be robust and still\n",
    "                # consider it as a pixel belonging to a square.\n",
    "                if np.random.randint(0, 2):\n",
    "                    noise_f = (-1)**np.random.randint(0, 2)\n",
    "                    noisy_movies[i, t,\n",
    "                                 x_shift - w - 1: x_shift + w + 1,\n",
    "                                 y_shift - w - 1: y_shift + w + 1,\n",
    "                                 0] += noise_f * 0.1\n",
    "\n",
    "                # Shift the ground truth by 1\n",
    "                x_shift = xstart + directionx * (t + 1)\n",
    "                y_shift = ystart + directiony * (t + 1)\n",
    "                shifted_movies[i, t, x_shift - w: x_shift + w,\n",
    "                               y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "    # Cut to a 40x40 window\n",
    "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
    "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
    "    noisy_movies[noisy_movies >= 1] = 1\n",
    "    shifted_movies[shifted_movies >= 1] = 1\n",
    "    return noisy_movies, shifted_movies\n",
    "\n",
    "# Train the network\n",
    "noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n",
    "seq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n",
    "        epochs=300, validation_split=0.05)\n",
    "\n",
    "# Testing the network on one movie\n",
    "# feed it with the first 7 positions and then\n",
    "# predict the new positions\n",
    "which = 1004\n",
    "track = noisy_movies[which][:7, ::, ::, ::]\n",
    "\n",
    "for j in range(16):\n",
    "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
    "    new = new_pos[::, -1, ::, ::, ::]\n",
    "    track = np.concatenate((track, new), axis=0)\n",
    "\n",
    "\n",
    "# And then compare the predictions\n",
    "# to the ground truth\n",
    "track2 = noisy_movies[which][::, ::, ::, ::]\n",
    "for i in range(15):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "\n",
    "    if i >= 7:\n",
    "        ax.text(1, 3, 'Predictions !', fontsize=20, color='w')\n",
    "    else:\n",
    "        ax.text(1, 3, 'Initial trajectory', fontsize=20)\n",
    "\n",
    "    toplot = track[i, ::, ::, 0]\n",
    "\n",
    "    plt.imshow(toplot)\n",
    "    ax = fig.add_subplot(122)\n",
    "    plt.text(1, 3, 'Ground truth', fontsize=20)\n",
    "\n",
    "    toplot = track2[i, ::, ::, 0]\n",
    "    if i >= 2:\n",
    "        toplot = shifted_movies[which][i - 1, ::, ::, 0]\n",
    "\n",
    "    plt.imshow(toplot)\n",
    "    plt.savefig('%i_animate.png' % (i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-lstm imdb sentiment\n",
    "'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train a recurrent convolutional network on the IMDB sentiment\n",
    "classification task.\n",
    "\n",
    "Gets to 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "\n",
    "Gets to 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 7s 265us/step - loss: 0.6123 - acc: 0.7436 - val_loss: 0.5048 - val_acc: 0.8227\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 6s 240us/step - loss: 0.4057 - acc: 0.8634 - val_loss: 0.3738 - val_acc: 0.8646\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 6s 257us/step - loss: 0.3061 - acc: 0.8934 - val_loss: 0.3219 - val_acc: 0.8783\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 7s 264us/step - loss: 0.2550 - acc: 0.9110 - val_loss: 0.2970 - val_acc: 0.8853\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 7s 270us/step - loss: 0.2206 - acc: 0.9248 - val_loss: 0.2843 - val_acc: 0.8879\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 6s 246us/step - loss: 0.1949 - acc: 0.9337 - val_loss: 0.2790 - val_acc: 0.8904\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 6s 256us/step - loss: 0.1727 - acc: 0.9422 - val_loss: 0.2763 - val_acc: 0.8916\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 6s 245us/step - loss: 0.1546 - acc: 0.9486 - val_loss: 0.2778 - val_acc: 0.8906\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 6s 248us/step - loss: 0.1387 - acc: 0.9552 - val_loss: 0.2874 - val_acc: 0.8876\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 7s 269us/step - loss: 0.1251 - acc: 0.9608 - val_loss: 0.2875 - val_acc: 0.8891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2a5b57f60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This example demonstrates the use of fasttext for text classification\n",
    "\n",
    "Based on Joulin et al's paper:\n",
    "\n",
    "Bags of Tricks for Efficient Text Classification\n",
    "https://arxiv.org/abs/1607.01759\n",
    "\n",
    "Results on IMDB datasets with uni and bi-gram embeddings:\n",
    "    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n",
    "    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 1\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.3028 - acc: 0.8769 - val_loss: 0.3848 - val_acc: 0.8302\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.2150 - acc: 0.9176 - val_loss: 0.4080 - val_acc: 0.8331\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 206s 8ms/step - loss: 0.1531 - acc: 0.9425 - val_loss: 0.4663 - val_acc: 0.8280\n",
      "Epoch 5/15\n",
      "23552/25000 [===========================>..] - ETA: 9s - loss: 0.1105 - acc: 0.9586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 202s 8ms/step - loss: 0.0209 - acc: 0.9932 - val_loss: 0.8753 - val_acc: 0.8108\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 202s 8ms/step - loss: 0.0170 - acc: 0.9951 - val_loss: 0.9454 - val_acc: 0.8122\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.0148 - acc: 0.9949 - val_loss: 1.0514 - val_acc: 0.8079\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.0106 - acc: 0.9969 - val_loss: 1.0791 - val_acc: 0.8108\n",
      "Epoch 15/15\n",
      "12288/25000 [=============>................] - ETA: 1:22 - loss: 0.0077 - acc: 0.9976"
     ]
    }
   ],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
