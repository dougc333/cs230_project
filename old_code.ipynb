{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data formatting\n",
    "QUORA_PATH=\"/home/dc/cs230_project/dataset\"\n",
    "\n",
    "def clean_quora(quora_path):\n",
    "    '''\n",
    "    input: path of quora tsv file downloaded from kaggle\n",
    "    output: df with questions <10 chars removed\n",
    "    \n",
    "    '''\n",
    "    df = pd.read_csv(os.path.join(quora_path,\"quora_duplicate_questions.tsv\"),sep=\"\\t\")\n",
    "    print(df.head())\n",
    "    df = df.drop([\"id\",\"qid1\",\"qid2\"],axis=1)\n",
    "    print(df.count())\n",
    "    df=df.dropna()\n",
    "    print(df.count())\n",
    "    df['q1_len'] = df['question1'].apply(len)\n",
    "    df['q2_len'] = df['question2'].apply(len)\n",
    "    print(df.head())\n",
    "    #print(df.loc[df['q1_len'] < 10])\n",
    "    #print(df.loc[df['q2_len'] < 10])\n",
    "    df = df.loc[ (df['q1_len'] > 10) & (df['q2_len'] > 10)]\n",
    "    print(df.count())\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_single_file(filename):\n",
    "    fh = open(os.path.join(params.data_dir,filename+'.pkl'),'rb')\n",
    "    data = pickle.load(fh)\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "def load_data():\n",
    "    '''\n",
    "    remove <10 char length question rows. this isnt enough cleaning.\n",
    "    '''\n",
    "    X_train = load_single_file(\"X_train\")\n",
    "    X_valid = load_single_file(\"X_valid\")\n",
    "    X_test = load_single_file(\"X_test\")\n",
    "    y_train = load_single_file(\"y_train\")\n",
    "    y_valid = load_single_file(\"y_valid\")\n",
    "    y_test = load_single_file(\"y_test\")\n",
    "    return X_train, X_valid, X_test, y_train,y_valid, y_test\n",
    "\"\"\"    \n",
    "   \n",
    "    \n",
    "def format_data(X_train, X_valid,X_test, y_train,y_valid,y_test):\n",
    "    s1 = {}\n",
    "    s2 = {}\n",
    "    target = {}\n",
    "    s1['train'],s1['dev'],s1['test'],s2['train'],s2['dev'],s2['test'] = {},{},{},{},{},{}\n",
    "    target['train'],target['dev'],target['test']={},{},{}\n",
    "\n",
    "    s1['train']['sent'] = [x for x in X_train[:,0]]\n",
    "    s2['train']['sent'] = [x for x in X_train[:,1]]\n",
    "    s1['dev']['sent'] = [x for x in X_valid[:,0]]\n",
    "    s2['dev']['sent'] = [x for x in X_valid[:,1]]\n",
    "    s1['test']['sent'] = [x for x in X_test[:,0]]\n",
    "    s2['test']['sent'] = [x for x in X_test[:,1]]\n",
    "    target['train']['data'] = np.array([x[0] for x in y_train])\n",
    "    target['dev']['data'] = np.array([x[0] for x in y_valid])\n",
    "    target['test']['data'] = np.array([x[0] for x in y_test.tolist()])\n",
    "\n",
    "    train = {'s1': s1['train']['sent'], 's2': s2['train']['sent'],\n",
    "             'label': target['train']['data']}\n",
    "    dev = {'s1': s1['dev']['sent'], 's2': s2['dev']['sent'],\n",
    "           'label': target['dev']['data']}\n",
    "    test = {'s1': s1['test']['sent'], 's2': s2['test']['sent'],\n",
    "            'label': target['test']['data']}\n",
    "    return train,dev,test\n",
    "\n",
    "\n",
    "\n",
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def quora(big=False,small=False,clean=False):\n",
    "    \n",
    "    \n",
    "    X_train,X_valid,X_test,y_train,y_valid,y_test = load_data(big=big,small=small,clean=clean)\n",
    "    \n",
    "    train,valid,test = format_data(X_train, X_valid,X_test, y_train,y_valid,y_test)\n",
    "    print(f\"quora checkpoint len(train[s1]):{len(train['s1'])},len(train[s2]):{len(train['s2'])},\\\n",
    "          len(train[label]):{len(train['label'])}\")\n",
    "    print('============')\n",
    "    print(f\"len(valid['s1']):{len(valid['s1'])}, len(valid[s2]):{len(valid['s2'])}, \\\n",
    "          len(valid['label']):{len(valid['label'])}\")\n",
    "    print('============')\n",
    "    print(f\"len(test['s1']):{len(test['s1'])},len(test['s2']):{len(test['s2'])}, \\\n",
    "          len(test['label']):{len(test['label'])}\")\n",
    "          \n",
    "    word_vec = build_vocab(train['s1'] + train['s2'] +\n",
    "                       valid['s1'] + valid['s2'] +\n",
    "                       test['s1'] + test['s2'], W2V_PATH)\n",
    "    for split in ['s1', 's2']:\n",
    "        for data_type in ['train', 'valid', 'test']:\n",
    "            eval(data_type)[split] = np.array([['<s>'] +\n",
    "            [word for word in sent.split() if word in word_vec] +\n",
    "            ['</s>'] for sent in eval(data_type)[split]])\n",
    "    \n",
    "    return train,valid,test,word_vec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
